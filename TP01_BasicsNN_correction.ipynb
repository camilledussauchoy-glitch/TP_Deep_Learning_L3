{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e44a123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchviz\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d130573",
   "metadata": {},
   "source": [
    "# Manipulate the device and the precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb149b",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "In PyTorch, each tensor has a `device` and a `dtype`. The `device` refers to the device on which the tensor is currently loaded. The `dtype` is the type of data stored in the tensor. By default, `device = torch.device(\"cpu\")` and `dtype = torch.float32`.\n",
    "\n",
    "One can build a tensor directly on a given device and with a given data type (or precision) by using the optional arguments `device` and `dtype`, or we can build them first with by-default values, and send copy them on the desired device with the desired precision with the `to` method.\n",
    "\n",
    "Use both methods with devices `torch.device(\"cuda\")` and `torch.device(\"cpu\")` and with data types other than `torch.float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "382fba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_cuda = torch.cuda.is_available()\n",
    "if with_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62a06dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5792,  0.5092,  1.7268, -2.9908, -0.5224], device='cuda:0')\n",
      "tensor([ 0.4727, -1.6924, -0.0090,  1.3975,  1.8789], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "tensor([-1.6825, -0.5781, -1.1377,  0.3520, -1.4793], device='cuda:0')\n",
      "tensor([-0.9854,  0.6533,  1.1514, -0.4280,  0.1829], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# Intialize a tensor directly on a device\n",
    "x = torch.randn(5, device = device)\n",
    "print(x)\n",
    "\n",
    "# Intialize a tensor directly on a device with given precision\n",
    "x = torch.randn(5, device = device, dtype = torch.float16)\n",
    "print(x)\n",
    "\n",
    "# Send an object to a device\n",
    "x = torch.randn(5, device = device)\n",
    "x = x.to(device = device)\n",
    "print(x)\n",
    "# Note: the object is first created on the CPU, then copied on the GPU\n",
    "#    if it is not used on the CPU, this operation is suboptimal\n",
    "\n",
    "# Send an object to a device with given precision\n",
    "x = torch.randn(5)\n",
    "x = x.to(device = device, dtype = torch.float16)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401555f6",
   "metadata": {},
   "source": [
    "# Backpropagation of the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1b0bc",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Code libraries such as PyTorch, TensorFlow, JAX implement **automatic differentation (AutoDiff)** methods, which are at the core of ML methods trainable by gradient-based optimization techniques. These ML method include naturally neural networks.\n",
    "\n",
    "AutoDiff is based on the construction of a computational graph, which is a *Directed Acyclic Graph* (DAG). This graph represents the different stages of computation of a function $f$ at a point $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_S)$, and we want to compute: \n",
    "$$\n",
    "\\frac{\\partial f}{\\partial \\theta_1}, \\frac{\\partial f}{\\partial \\theta_2}, \\cdots, \n",
    "\\frac{\\partial f}{\\partial \\theta_S} .\n",
    "$$\n",
    "The DAG is then made of:\n",
    " * *directed* edges: represent the flows of data;\n",
    " * the root of the graph (a node which does not has any child): contains the (scalar) result of the computation of $f(\\boldsymbol{\\theta})$;\n",
    " * the leaf nodes (nodes which do not have any parent): contain the variables $(\\theta_1, \\theta_2, \\cdots, \\theta_S)$ with repect to which we want to differentiate $f$;\n",
    " * the non-leaf nodes: contain an operation to perform on their inputs.\n",
    "\n",
    "In PyTorch, the leaf nodes are built either automatically when initializing a `torch.nn.Module` (such as a linear layer, in which weights and biases are by-default leaf nodes), or manually trough the class `torch.nn.Parameter`. Tensors that we use during a computation, but that are not variables of our function are just instances of the class `torch.Tensor` (with `requires_grad = False`).\n",
    "\n",
    "The DAG is built on-the-fly during the successive operations, and the gradients are computed only when calling the `backward` method on the root node, or the function `torch.autograd.grad` on the root node and the leaf nodes with respect to which we want to differentiate. Using `backward` is very straightforward, while `torch.autograd.grad` is slightly more complex, but it supports advanced custom operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c31df7",
   "metadata": {},
   "source": [
    "## Parameters and tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81f1a65",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "Build some `torch.nn.Parameter`, some `torch.Tensor` and some `torch.Tensor` with `requires_grad = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5066745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-1.4674,  0.1926,  0.3745,  0.0024, -2.1369], requires_grad=True) requires_grad: True\n",
      "Parameter containing:\n",
      "tensor(0.4646, requires_grad=True) requires_grad: True\n",
      "tensor([ 3.9066e-01,  7.6140e-01, -1.1397e+00, -5.9417e-04, -1.1050e-02]) requires_grad: False\n",
      "tensor(1.1291)\n",
      "tensor([[ 0.7868, -0.0717,  0.3272, -0.1342],\n",
      "        [-0.9332,  0.1874, -1.2130,  0.3403],\n",
      "        [-1.4030,  2.3964,  1.5526, -0.3514]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "c = torch.randn(3, 4, requires_grad = True)\n",
    "\n",
    "print(x1, \"requires_grad:\", x1.requires_grad)\n",
    "print(x2, \"requires_grad:\", x2.requires_grad)\n",
    "print(a, \"requires_grad:\", a.requires_grad)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b2bd38",
   "metadata": {},
   "source": [
    "Note: The main difference between a `Parameter` and a `Tensor` with `requires_grad = True` is how they are managed inside a PyTorch `Module`. To avoid any unexpected behavior, one should prefer to use `Parameter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8108cb",
   "metadata": {},
   "source": [
    "## Computing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9332678",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Let $f$ be the function defined below. Compute its derivative with respect to `x1` and `x2` by using `backward`, and then by using `torch.autograd.grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651a0ffc",
   "metadata": {},
   "source": [
    "### backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e563f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "y = x2 * torch.sin((a * x1).sum() + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a259c9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "tensor([-0.0915, -0.0635, -0.1047, -0.1179,  0.0016]) tensor(-0.9572)\n"
     ]
    }
   ],
   "source": [
    "print(x1.grad, x2.grad)\n",
    "y.backward()\n",
    "print(x1.grad, x2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a95a34",
   "metadata": {},
   "source": [
    "### torch.autograd.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e7cbc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "y = x2 * torch.sin((a * x1).sum() + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6831205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad of x1: tensor([ 0.0011, -0.0061, -0.0009,  0.0965, -0.0141])\n",
      "grad of x2: tensor(-0.7921)\n"
     ]
    }
   ],
   "source": [
    "grads = torch.autograd.grad(y, (x1, x2))\n",
    "print(\"grad of x1:\", grads[0])\n",
    "print(\"grad of x2:\", grads[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e6e67",
   "metadata": {},
   "source": [
    "### Higher-order derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e546633",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Compute $\\frac{\\partial f}{\\partial x_1 \\partial x_2}$ by using `torch.autograd.grad` twice. One check how to use the additional parameters `create_graph` and `allow_unused`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f459cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "y = x2 * torch.sin((a * x1).sum() + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98e11e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads: tensor([ 1.6467,  1.2498,  0.8305, -1.7406, -1.9315], grad_fn=<MulBackward0>) tensor(0.0854, grad_fn=<MulBackward0>)\n",
      "hessian_x2_x1: (tensor([-1.1082, -0.8411, -0.5589,  1.1714,  1.2999]),)\n"
     ]
    }
   ],
   "source": [
    "grad_x1, grad_x2 = torch.autograd.grad(y, (x1, x2), create_graph = True)\n",
    "print(\"grads:\", grad_x1, grad_x2)\n",
    "hessian_x2_x1 = torch.autograd.grad(grad_x2, (x1,), retain_graph = True, allow_unused = True)\n",
    "print(\"hessian_x2_x1:\", hessian_x2_x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee8100",
   "metadata": {},
   "source": [
    "## Access the computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6daae9",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "The root node contains all the required information to perform the backpropagation and compute the derivatives. The graph and the nodes can be accessed with the methods `grad_fn` and `next_functions`. The leaf nodes can be accessed with the `variable` method. Before the backward, the intermediady results can be accessed with `_saved_self`.\n",
    "\n",
    "Access the various nodes of the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81cdf684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2927, grad_fn=<MulBackward0>)\n",
      "<MulBackward0 object at 0x715266a08af0>\n",
      "((<AccumulateGrad object at 0x7153912ebc40>, 0), (<SinBackward0 object at 0x7153912ea860>, 0))\n"
     ]
    }
   ],
   "source": [
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "y = x2 * torch.sin((a * x1).sum() + b)\n",
    "y.backward()\n",
    "\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "print(y.grad_fn.next_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dce79bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7987, grad_fn=<AddBackward0>)\n",
      "tensor(-0.2904, grad_fn=<MulBackward0>)\n",
      "<MulBackward0 object at 0x71526698f2b0>\n",
      "((<AccumulateGrad object at 0x715266c44a00>, 0), (<SinBackward0 object at 0x715266859ab0>, 0))\n",
      "Parameter containing:\n",
      "tensor(0.4754, requires_grad=True)\n",
      "((<AddBackward0 object at 0x715266c44a00>, 0),)\n",
      "((<SumBackward0 object at 0x715266c44a00>, 0), (None, 0))\n",
      "((<MulBackward0 object at 0x71526698f2b0>, 0),)\n"
     ]
    }
   ],
   "source": [
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "y = x2 * torch.sin((a * x1).sum() + b)\n",
    "\n",
    "print(y.grad_fn.next_functions[1][0]._saved_self)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "print(y.grad_fn.next_functions)\n",
    "\n",
    "print(y.grad_fn.next_functions[0][0].variable)\n",
    "print(y.grad_fn.next_functions[1][0].next_functions)\n",
    "print(y.grad_fn.next_functions[1][0].next_functions[0][0].next_functions)\n",
    "print(y.grad_fn.next_functions[1][0].next_functions[0][0].next_functions[0][0].next_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65685068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other example\n",
    "\n",
    "x = nn.Parameter(torch.randn(6))\n",
    "y = torch.split(x, 2)\n",
    "z = sum(y).sum()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff732117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<AddBackward0 object at 0x715266859b10>, 0),)\n",
      "((<AddBackward0 object at 0x715266859b10>, 0), (<SplitBackward0 object at 0x71526685b880>, 2))\n",
      "((<AddBackward0 object at 0x71526685b2e0>, 0), (<SplitBackward0 object at 0x71526685b880>, 1))\n"
     ]
    }
   ],
   "source": [
    "print(z.grad_fn.next_functions)\n",
    "print(z.grad_fn.next_functions[0][0].next_functions)\n",
    "print(z.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1200da26",
   "metadata": {},
   "source": [
    "## Visualize the computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440dd88e",
   "metadata": {},
   "source": [
    "**Question 5 (optional)**\n",
    "\n",
    "Visualize the computational graph with the package `torchviz` (function `make_dot`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "363f7256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"222pt\" height=\"446pt\"\n",
       " viewBox=\"0.00 0.00 222.00 446.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 442)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-442 218,-442 218,4 -4,4\"/>\n",
       "<!-- 124598721503520 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>124598721503520</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"133.5,-31 79.5,-31 79.5,0 133.5,0 133.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 124598720767280 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>124598720767280</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-86 62,-86 62,-67 151,-67 151,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 124598720767280&#45;&gt;124598721503520 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>124598720767280&#45;&gt;124598721503520</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.5,-66.79C106.5,-60.07 106.5,-50.4 106.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110,-41.19 106.5,-31.19 103,-41.19 110,-41.19\"/>\n",
       "</g>\n",
       "<!-- 124598721295792 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>124598721295792</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-141 0,-141 0,-122 101,-122 101,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 124598721295792&#45;&gt;124598720767280 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>124598721295792&#45;&gt;124598720767280</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.5,-121.98C67.69,-114.23 80.01,-102.58 89.97,-93.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.48,-95.59 97.34,-86.17 87.67,-90.5 92.48,-95.59\"/>\n",
       "</g>\n",
       "<!-- 124598721506560 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>124598721506560</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-207 23.5,-207 23.5,-177 77.5,-177 77.5,-207\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-195\" font-family=\"monospace\" font-size=\"10.00\">x2</text>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 124598721506560&#45;&gt;124598721295792 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>124598721506560&#45;&gt;124598721295792</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-176.84C50.5,-169.21 50.5,-159.7 50.5,-151.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-151.27 50.5,-141.27 47,-151.27 54,-151.27\"/>\n",
       "</g>\n",
       "<!-- 124598721294352 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>124598721294352</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-141 119,-141 119,-122 208,-122 208,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">SinBackward0</text>\n",
       "</g>\n",
       "<!-- 124598721294352&#45;&gt;124598720767280 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>124598721294352&#45;&gt;124598720767280</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M154.34,-121.98C146,-114.23 133.47,-102.58 123.32,-93.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"125.53,-90.42 115.82,-86.17 120.76,-95.54 125.53,-90.42\"/>\n",
       "</g>\n",
       "<!-- 124598721294160 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>124598721294160</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-201.5 119,-201.5 119,-182.5 208,-182.5 208,-201.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 124598721294160&#45;&gt;124598721294352 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>124598721294160&#45;&gt;124598721294352</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-182.37C163.5,-174.25 163.5,-161.81 163.5,-151.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-151.17 163.5,-141.17 160,-151.17 167,-151.17\"/>\n",
       "</g>\n",
       "<!-- 124598721297232 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>124598721297232</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-262 119,-262 119,-243 208,-243 208,-262\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\">SumBackward0</text>\n",
       "</g>\n",
       "<!-- 124598721297232&#45;&gt;124598721294160 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>124598721297232&#45;&gt;124598721294160</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-242.87C163.5,-234.75 163.5,-222.31 163.5,-211.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-211.67 163.5,-201.67 160,-211.67 167,-211.67\"/>\n",
       "</g>\n",
       "<!-- 124598721293440 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>124598721293440</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-317 119,-317 119,-298 208,-298 208,-317\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 124598721293440&#45;&gt;124598721297232 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>124598721293440&#45;&gt;124598721297232</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-297.75C163.5,-290.8 163.5,-280.85 163.5,-272.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-272.09 163.5,-262.09 160,-272.09 167,-272.09\"/>\n",
       "</g>\n",
       "<!-- 124598721294256 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>124598721294256</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-372 113,-372 113,-353 214,-353 214,-372\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-360\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 124598721294256&#45;&gt;124598721293440 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>124598721294256&#45;&gt;124598721293440</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-352.75C163.5,-345.8 163.5,-335.85 163.5,-327.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-327.09 163.5,-317.09 160,-327.09 167,-327.09\"/>\n",
       "</g>\n",
       "<!-- 124598721505680 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>124598721505680</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"190.5,-438 136.5,-438 136.5,-408 190.5,-408 190.5,-438\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-426\" font-family=\"monospace\" font-size=\"10.00\">x1</text>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-415\" font-family=\"monospace\" font-size=\"10.00\"> (5)</text>\n",
       "</g>\n",
       "<!-- 124598721505680&#45;&gt;124598721294256 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>124598721505680&#45;&gt;124598721294256</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-407.84C163.5,-400.21 163.5,-390.7 163.5,-382.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-382.27 163.5,-372.27 160,-382.27 167,-382.27\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x715266859d60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "y = x2 * torch.sin((a * x1).sum() + b)\n",
    "\n",
    "make_dot(y, {\"x1\": x1, \"x2\": x2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a20d692",
   "metadata": {},
   "source": [
    "## Forward-only mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09dadbf",
   "metadata": {},
   "source": [
    "**Question 6**\n",
    "\n",
    "Sometimes, for instance when testing a model, we are just interested in the result, and not the gradients. Such computations are more efficient if we are in a \"forward-only\" mode (in that situation, the computational graph is not created and the intermediary results are not stored).\n",
    "\n",
    "The easiest way is to use `with torch.no_grad()`. One can also use the method `detach` on the parameters to use their content without building the computational graph.\n",
    "\n",
    "Compute the function $f$ without building the computational graph using both methods and check that no computational graph is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "084f6de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.1100)\n"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad()\n",
    "\n",
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "with torch.no_grad(): # Faster computation when no gradient is needed\n",
    "    y = x2 * torch.sin((a * x1).sum() + b)\n",
    "    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97ff3221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0872)\n"
     ]
    }
   ],
   "source": [
    "# detach()\n",
    "\n",
    "x1 = nn.Parameter(torch.randn(5))\n",
    "x2 = nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "y = x2.detach() * torch.sin((a * x1.detach()).sum() + b)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90917ce1",
   "metadata": {},
   "source": [
    "# Managing a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19cb4f6",
   "metadata": {},
   "source": [
    "Before training a model on a dataset, we have to make some basic checks on the dataset and preprocess it correctly in order to obtain the best possible results (on a validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b0e6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = \"/home/pwolinski/datasets\" # A MODIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5946f3ce",
   "metadata": {},
   "source": [
    "## Checking a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6d630",
   "metadata": {},
   "source": [
    "First, we load MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d1829ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# build transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ]) \n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(datasets_path, train = True,\n",
    "                              download = False, transform = transform)\n",
    "test_data = datasets.MNIST(datasets_path, train=False,\n",
    "                             download = False, transform = transform)\n",
    "\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "# build the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# specify the image classes\n",
    "classes = [f\"{i}\" for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567f47f7",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "By using `numpy.random.choice`, subplots and the function `imshow` of matplotlib, print 10 (or more) random data points of the training set (image + label).\n",
    "\n",
    "Is the task feasible for a human? Do the data look clean ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3edad397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x350 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lst_idx = np.random.choice(train_size, 10, replace = False)\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize = (7.5, 3.5))\n",
    "\n",
    "for i, idx in enumerate(lst_idx):\n",
    "    img, target = train_data[idx]\n",
    "    ax[i//5, i%5].imshow(img.numpy().transpose(1, 2, 0), cmap=\"Greys\")\n",
    "    ax[i//5, i%5].set_title(classes[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce31f09",
   "metadata": {},
   "source": [
    "## Check for class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f8913",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "In classification tasks, it is common that the different classes are inequally represented in the dataset. If this \"class imbalance\" is too severe, the model is likely to fail to learn well the least represented classes.\n",
    "\n",
    "Compute the number of data points in each class (in the training dataset). What do we observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33fe12f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])\n"
     ]
    }
   ],
   "source": [
    "ndata_per_class = 0\n",
    "\n",
    "for data, target in train_loader:\n",
    "    ndata_per_class += torch.bincount(target)\n",
    "    \n",
    "print(ndata_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0442c4",
   "metadata": {},
   "source": [
    "Theoretical number of data points per class if the dataset was perfectly balanced: 6000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e481c4",
   "metadata": {},
   "source": [
    "# Managing a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "366ee497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        self.act_function = torch.tanh\n",
    "        layers = [1, 6, 16, 120, 84, 10]\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(layers[0], layers[1], 5, padding = 2)\n",
    "        self.conv2 = torch.nn.Conv2d(layers[1], layers[2], 5)\n",
    "        self.fc1 = torch.nn.Linear(5 * 5 * layers[2], layers[3])\n",
    "        self.fc2 = torch.nn.Linear(layers[3], layers[4])\n",
    "        self.fc3 = torch.nn.Linear(layers[4], layers[5])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act_function(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act_function(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_function(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.act_function(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = torch.nn.functional.log_softmax(x, dim = 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd5f803",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "When a `torch.nn.Module` is created, its parameters and its submodules are automatically registered, which is necessary to optimize them with an `Optimizer`. One can access them with the methods `named_parameters`, `parameters`, `named_modules`, `modules`.\n",
    "\n",
    "Access the various elements on an instance of LeNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8184ad91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('conv1.weight', Parameter containing:\n",
      "tensor([[[[ 1.5421e-01, -1.2760e-02,  1.7148e-01, -1.0920e-05, -1.1357e-01],\n",
      "          [-1.0583e-01, -8.2900e-03,  1.7941e-01,  6.6240e-02, -1.5408e-01],\n",
      "          [ 1.8438e-01, -1.7201e-01, -2.8445e-02, -2.1170e-02, -7.6009e-02],\n",
      "          [ 1.5098e-01,  6.1614e-02, -3.9796e-02,  1.6809e-01,  1.9390e-01],\n",
      "          [-1.4025e-01,  5.9152e-02,  9.3119e-02, -1.2812e-01,  1.5073e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.9574e-02,  1.7498e-01, -1.1324e-01, -1.9779e-01,  1.7495e-01],\n",
      "          [ 9.4930e-02, -1.4430e-01,  1.4850e-01, -1.9941e-01,  1.6103e-01],\n",
      "          [-1.6562e-01, -9.4148e-02, -5.0372e-02, -7.7963e-02,  1.7822e-01],\n",
      "          [ 1.0875e-01, -1.9102e-01, -1.2377e-01, -1.8269e-01,  1.9430e-01],\n",
      "          [ 1.7640e-01, -6.6218e-02, -1.9421e-01,  1.6191e-01,  1.2358e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.6979e-02, -1.5262e-01, -8.7070e-02,  6.9412e-02, -1.1991e-01],\n",
      "          [ 7.4968e-02, -1.6417e-01,  1.8857e-02, -2.1248e-02,  2.7089e-02],\n",
      "          [ 1.7792e-01,  2.3576e-03, -1.5850e-01,  1.3479e-01,  2.1980e-02],\n",
      "          [-1.4167e-01,  1.9049e-01,  1.6152e-01, -1.8377e-01,  1.8078e-01],\n",
      "          [-9.1209e-02,  1.0307e-01,  1.4772e-01,  1.2343e-01, -1.5440e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0732e-03,  5.1267e-02,  1.1738e-02, -3.4220e-03, -5.1624e-02],\n",
      "          [-4.6804e-03, -4.8332e-02,  5.8460e-02,  1.6075e-01, -1.2379e-01],\n",
      "          [-2.5536e-02,  1.1524e-01, -2.9624e-02,  3.0960e-03,  1.1039e-01],\n",
      "          [ 4.2373e-02,  1.9950e-01,  7.5429e-02, -1.5179e-01,  5.6623e-02],\n",
      "          [ 8.8069e-02, -1.5151e-01, -4.3720e-02, -1.7002e-01,  1.7791e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8668e-01, -9.5943e-02,  3.1235e-02, -1.8983e-01, -1.0103e-01],\n",
      "          [ 1.0881e-01, -6.2329e-02, -5.7947e-02,  1.2529e-01, -2.2828e-02],\n",
      "          [-3.3154e-02,  8.6161e-02, -1.3503e-01,  1.6009e-01,  9.4467e-02],\n",
      "          [ 3.1969e-02,  3.1655e-02,  1.5406e-01,  2.2576e-02,  8.8456e-02],\n",
      "          [-1.8176e-01, -1.1693e-01,  1.7511e-01,  1.9407e-01,  3.8362e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.7344e-02,  1.6616e-01,  2.2568e-02,  1.7927e-02, -7.6326e-02],\n",
      "          [-8.4304e-02, -1.3810e-01,  1.2302e-01, -1.5034e-01, -1.7635e-01],\n",
      "          [ 2.2991e-02, -1.8643e-01, -1.7082e-01, -2.3778e-02, -1.9087e-01],\n",
      "          [ 1.0838e-01, -5.9832e-02,  1.5071e-01,  1.2121e-01,  6.2252e-02],\n",
      "          [ 1.9416e-01,  9.1070e-02, -1.5781e-01, -9.3969e-02,  1.0056e-01]]]],\n",
      "       requires_grad=True)), ('conv1.bias', Parameter containing:\n",
      "tensor([-0.0539, -0.0175, -0.1239, -0.1323,  0.1621,  0.0125],\n",
      "       requires_grad=True)), ('conv2.weight', Parameter containing:\n",
      "tensor([[[[ 0.0737,  0.0801,  0.0090,  0.0081, -0.0359],\n",
      "          [-0.0376,  0.0450, -0.0006, -0.0352,  0.0164],\n",
      "          [ 0.0480, -0.0764,  0.0250,  0.0349, -0.0683],\n",
      "          [ 0.0547,  0.0346, -0.0497, -0.0685,  0.0240],\n",
      "          [-0.0448, -0.0748,  0.0730,  0.0524,  0.0781]],\n",
      "\n",
      "         [[-0.0593,  0.0057,  0.0167,  0.0026,  0.0031],\n",
      "          [ 0.0115,  0.0524, -0.0686, -0.0657, -0.0220],\n",
      "          [-0.0710,  0.0296,  0.0646, -0.0420,  0.0656],\n",
      "          [-0.0244,  0.0327, -0.0197,  0.0807,  0.0637],\n",
      "          [ 0.0013,  0.0495, -0.0214,  0.0499,  0.0423]],\n",
      "\n",
      "         [[ 0.0032, -0.0088,  0.0021, -0.0216,  0.0724],\n",
      "          [-0.0610, -0.0719, -0.0596, -0.0355, -0.0737],\n",
      "          [-0.0767, -0.0456,  0.0723, -0.0412, -0.0216],\n",
      "          [ 0.0217,  0.0184,  0.0265, -0.0719, -0.0370],\n",
      "          [-0.0156, -0.0537,  0.0757,  0.0083,  0.0095]],\n",
      "\n",
      "         [[ 0.0542,  0.0180, -0.0118,  0.0560, -0.0795],\n",
      "          [ 0.0305, -0.0612,  0.0352,  0.0223, -0.0086],\n",
      "          [ 0.0418, -0.0512,  0.0281,  0.0150,  0.0404],\n",
      "          [ 0.0238, -0.0026,  0.0620, -0.0228, -0.0528],\n",
      "          [-0.0801, -0.0072,  0.0093,  0.0246,  0.0698]],\n",
      "\n",
      "         [[ 0.0050,  0.0730,  0.0539, -0.0553, -0.0675],\n",
      "          [-0.0080,  0.0641, -0.0262,  0.0166,  0.0417],\n",
      "          [ 0.0058,  0.0408,  0.0504,  0.0359,  0.0521],\n",
      "          [ 0.0105,  0.0256, -0.0176, -0.0452, -0.0322],\n",
      "          [-0.0511,  0.0404,  0.0244,  0.0737,  0.0689]],\n",
      "\n",
      "         [[-0.0250,  0.0749,  0.0187, -0.0773, -0.0691],\n",
      "          [-0.0040, -0.0160, -0.0546, -0.0107, -0.0660],\n",
      "          [-0.0353, -0.0407,  0.0211,  0.0086, -0.0023],\n",
      "          [ 0.0503,  0.0446, -0.0224, -0.0514,  0.0566],\n",
      "          [-0.0381,  0.0273,  0.0685,  0.0365,  0.0290]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0590,  0.0184,  0.0286, -0.0084, -0.0488],\n",
      "          [ 0.0346, -0.0752,  0.0020,  0.0689,  0.0755],\n",
      "          [ 0.0121,  0.0746,  0.0302, -0.0443,  0.0319],\n",
      "          [ 0.0410, -0.0200, -0.0320,  0.0422, -0.0326],\n",
      "          [-0.0156, -0.0411, -0.0185,  0.0468, -0.0098]],\n",
      "\n",
      "         [[-0.0699,  0.0165, -0.0414, -0.0683,  0.0783],\n",
      "          [-0.0552, -0.0297, -0.0490,  0.0008,  0.0453],\n",
      "          [-0.0710,  0.0070, -0.0215, -0.0189,  0.0586],\n",
      "          [ 0.0594,  0.0048,  0.0498,  0.0039, -0.0141],\n",
      "          [ 0.0769, -0.0567,  0.0519,  0.0427,  0.0614]],\n",
      "\n",
      "         [[-0.0289,  0.0071,  0.0126,  0.0274, -0.0183],\n",
      "          [-0.0266,  0.0594, -0.0254,  0.0397, -0.0012],\n",
      "          [ 0.0196, -0.0170,  0.0170,  0.0595, -0.0174],\n",
      "          [-0.0042,  0.0165,  0.0388,  0.0303,  0.0481],\n",
      "          [ 0.0734, -0.0007,  0.0215, -0.0611, -0.0119]],\n",
      "\n",
      "         [[ 0.0054, -0.0002,  0.0285,  0.0028, -0.0421],\n",
      "          [ 0.0122, -0.0575,  0.0691, -0.0190, -0.0786],\n",
      "          [ 0.0054, -0.0768,  0.0446, -0.0791,  0.0129],\n",
      "          [-0.0134,  0.0637,  0.0390,  0.0627,  0.0763],\n",
      "          [ 0.0143,  0.0465,  0.0085,  0.0230,  0.0310]],\n",
      "\n",
      "         [[ 0.0309,  0.0253, -0.0765, -0.0438, -0.0757],\n",
      "          [ 0.0629,  0.0399, -0.0259,  0.0232,  0.0334],\n",
      "          [ 0.0008,  0.0375, -0.0477,  0.0466, -0.0079],\n",
      "          [-0.0635,  0.0513, -0.0103, -0.0692,  0.0063],\n",
      "          [ 0.0769,  0.0331, -0.0004,  0.0659, -0.0816]],\n",
      "\n",
      "         [[-0.0420, -0.0410,  0.0606,  0.0196, -0.0103],\n",
      "          [-0.0723,  0.0166, -0.0143, -0.0228,  0.0748],\n",
      "          [-0.0502, -0.0331, -0.0015, -0.0580, -0.0125],\n",
      "          [ 0.0094,  0.0651, -0.0242, -0.0691, -0.0100],\n",
      "          [-0.0109,  0.0602, -0.0242,  0.0798, -0.0256]]],\n",
      "\n",
      "\n",
      "        [[[-0.0461, -0.0676, -0.0731, -0.0680, -0.0254],\n",
      "          [-0.0246,  0.0322,  0.0343,  0.0749,  0.0760],\n",
      "          [-0.0258, -0.0290,  0.0434, -0.0137,  0.0263],\n",
      "          [ 0.0109, -0.0489,  0.0480, -0.0732,  0.0386],\n",
      "          [ 0.0562,  0.0601, -0.0058,  0.0334, -0.0428]],\n",
      "\n",
      "         [[-0.0009,  0.0609, -0.0481,  0.0122, -0.0289],\n",
      "          [-0.0025, -0.0335,  0.0333,  0.0811,  0.0265],\n",
      "          [ 0.0332, -0.0017, -0.0390, -0.0790,  0.0307],\n",
      "          [-0.0522, -0.0811,  0.0455, -0.0031, -0.0278],\n",
      "          [ 0.0134,  0.0649, -0.0511,  0.0606,  0.0435]],\n",
      "\n",
      "         [[ 0.0672,  0.0346,  0.0501, -0.0515, -0.0403],\n",
      "          [-0.0430,  0.0373,  0.0400, -0.0038,  0.0717],\n",
      "          [ 0.0417, -0.0435,  0.0754,  0.0630,  0.0466],\n",
      "          [ 0.0606,  0.0357, -0.0189,  0.0567, -0.0105],\n",
      "          [ 0.0816,  0.0480, -0.0541, -0.0379, -0.0276]],\n",
      "\n",
      "         [[-0.0141,  0.0327, -0.0246, -0.0758, -0.0468],\n",
      "          [ 0.0356, -0.0061, -0.0104,  0.0689, -0.0696],\n",
      "          [-0.0085, -0.0667, -0.0789, -0.0452,  0.0283],\n",
      "          [-0.0455,  0.0122,  0.0200, -0.0400, -0.0241],\n",
      "          [ 0.0426, -0.0718, -0.0109, -0.0029, -0.0424]],\n",
      "\n",
      "         [[ 0.0259, -0.0726, -0.0604, -0.0201, -0.0231],\n",
      "          [ 0.0801, -0.0353, -0.0550, -0.0397, -0.0722],\n",
      "          [ 0.0218, -0.0669, -0.0738,  0.0745, -0.0360],\n",
      "          [ 0.0691, -0.0147, -0.0418,  0.0024, -0.0143],\n",
      "          [-0.0388, -0.0055,  0.0395, -0.0732,  0.0777]],\n",
      "\n",
      "         [[ 0.0607,  0.0237,  0.0123, -0.0518, -0.0204],\n",
      "          [ 0.0442,  0.0275, -0.0708,  0.0425,  0.0234],\n",
      "          [-0.0163, -0.0463, -0.0361,  0.0009,  0.0239],\n",
      "          [ 0.0558, -0.0479,  0.0194, -0.0756, -0.0688],\n",
      "          [-0.0716, -0.0728, -0.0447,  0.0262,  0.0787]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0473,  0.0088, -0.0249, -0.0573, -0.0788],\n",
      "          [ 0.0442, -0.0050, -0.0581,  0.0382, -0.0358],\n",
      "          [ 0.0313, -0.0242,  0.0013,  0.0738, -0.0025],\n",
      "          [-0.0534,  0.0535,  0.0348, -0.0035, -0.0167],\n",
      "          [ 0.0348,  0.0468, -0.0615,  0.0381,  0.0198]],\n",
      "\n",
      "         [[ 0.0327,  0.0807,  0.0628,  0.0073,  0.0684],\n",
      "          [ 0.0221,  0.0299, -0.0315, -0.0123,  0.0600],\n",
      "          [-0.0671,  0.0799,  0.0229, -0.0374, -0.0223],\n",
      "          [-0.0159, -0.0575, -0.0242, -0.0087,  0.0043],\n",
      "          [-0.0327,  0.0386, -0.0340, -0.0659, -0.0369]],\n",
      "\n",
      "         [[ 0.0380,  0.0550, -0.0727, -0.0154,  0.0085],\n",
      "          [ 0.0211,  0.0328, -0.0267,  0.0654, -0.0142],\n",
      "          [ 0.0451, -0.0422,  0.0551, -0.0608, -0.0604],\n",
      "          [ 0.0736,  0.0256, -0.0755, -0.0656, -0.0605],\n",
      "          [-0.0687,  0.0078, -0.0815,  0.0487, -0.0111]],\n",
      "\n",
      "         [[-0.0010,  0.0662, -0.0274, -0.0749,  0.0029],\n",
      "          [-0.0468, -0.0268,  0.0312,  0.0459, -0.0571],\n",
      "          [-0.0706, -0.0797, -0.0429, -0.0764, -0.0520],\n",
      "          [ 0.0001, -0.0439,  0.0146, -0.0529, -0.0618],\n",
      "          [-0.0446, -0.0656,  0.0399,  0.0724,  0.0153]],\n",
      "\n",
      "         [[-0.0186,  0.0057,  0.0729, -0.0694,  0.0614],\n",
      "          [ 0.0385, -0.0416, -0.0261,  0.0171, -0.0130],\n",
      "          [-0.0349,  0.0726,  0.0092, -0.0549,  0.0210],\n",
      "          [-0.0569,  0.0081,  0.0222,  0.0804, -0.0011],\n",
      "          [-0.0202, -0.0228, -0.0062, -0.0583,  0.0176]],\n",
      "\n",
      "         [[ 0.0733, -0.0165,  0.0794,  0.0351,  0.0739],\n",
      "          [-0.0179,  0.0036,  0.0341,  0.0755,  0.0083],\n",
      "          [ 0.0446, -0.0146, -0.0738,  0.0479,  0.0182],\n",
      "          [ 0.0026,  0.0738, -0.0560, -0.0516,  0.0261],\n",
      "          [ 0.0798,  0.0289, -0.0096, -0.0314, -0.0055]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0761, -0.0494,  0.0083, -0.0760,  0.0657],\n",
      "          [ 0.0009, -0.0274,  0.0785,  0.0429, -0.0347],\n",
      "          [ 0.0435, -0.0160, -0.0246,  0.0343,  0.0073],\n",
      "          [-0.0096,  0.0303, -0.0508, -0.0729,  0.0517],\n",
      "          [-0.0093,  0.0421,  0.0716, -0.0269,  0.0367]],\n",
      "\n",
      "         [[-0.0055, -0.0053,  0.0188, -0.0712,  0.0553],\n",
      "          [-0.0458, -0.0228,  0.0609, -0.0451,  0.0632],\n",
      "          [-0.0545,  0.0313, -0.0469, -0.0322, -0.0668],\n",
      "          [-0.0102,  0.0314, -0.0367, -0.0649,  0.0034],\n",
      "          [ 0.0443,  0.0029, -0.0287, -0.0405,  0.0122]],\n",
      "\n",
      "         [[ 0.0023, -0.0640, -0.0351,  0.0684, -0.0150],\n",
      "          [ 0.0250, -0.0105,  0.0736,  0.0052,  0.0026],\n",
      "          [-0.0092, -0.0085,  0.0383,  0.0393, -0.0400],\n",
      "          [-0.0006, -0.0024, -0.0626,  0.0328,  0.0374],\n",
      "          [ 0.0583, -0.0163,  0.0225, -0.0571,  0.0731]],\n",
      "\n",
      "         [[ 0.0092, -0.0101, -0.0501, -0.0025, -0.0313],\n",
      "          [-0.0238, -0.0287,  0.0423, -0.0301, -0.0039],\n",
      "          [-0.0185,  0.0487,  0.0246, -0.0443, -0.0456],\n",
      "          [ 0.0743, -0.0598,  0.0474,  0.0342, -0.0269],\n",
      "          [ 0.0512, -0.0140,  0.0037, -0.0753,  0.0687]],\n",
      "\n",
      "         [[ 0.0183, -0.0651, -0.0775,  0.0361, -0.0733],\n",
      "          [-0.0531, -0.0543,  0.0195,  0.0485, -0.0250],\n",
      "          [ 0.0084,  0.0650, -0.0007,  0.0361, -0.0734],\n",
      "          [ 0.0120,  0.0339,  0.0706, -0.0540,  0.0113],\n",
      "          [ 0.0477,  0.0062,  0.0652, -0.0690, -0.0430]],\n",
      "\n",
      "         [[-0.0695, -0.0154,  0.0162, -0.0318, -0.0308],\n",
      "          [-0.0328,  0.0527,  0.0083, -0.0070, -0.0029],\n",
      "          [ 0.0630, -0.0122, -0.0766,  0.0059,  0.0749],\n",
      "          [-0.0491,  0.0572,  0.0468, -0.0354,  0.0650],\n",
      "          [ 0.0097, -0.0169,  0.0763, -0.0787, -0.0217]]],\n",
      "\n",
      "\n",
      "        [[[-0.0155, -0.0105, -0.0378,  0.0285,  0.0630],\n",
      "          [ 0.0553,  0.0738, -0.0667,  0.0269, -0.0083],\n",
      "          [-0.0638,  0.0078, -0.0604, -0.0058, -0.0371],\n",
      "          [-0.0671,  0.0162,  0.0307,  0.0115, -0.0665],\n",
      "          [ 0.0050, -0.0533,  0.0304, -0.0159,  0.0375]],\n",
      "\n",
      "         [[-0.0152, -0.0425, -0.0457,  0.0439,  0.0207],\n",
      "          [-0.0777, -0.0289, -0.0738, -0.0333,  0.0697],\n",
      "          [-0.0534, -0.0096, -0.0672,  0.0157, -0.0692],\n",
      "          [-0.0037, -0.0005,  0.0431,  0.0718, -0.0778],\n",
      "          [-0.0727,  0.0430, -0.0419,  0.0756,  0.0128]],\n",
      "\n",
      "         [[-0.0387,  0.0253, -0.0259, -0.0781,  0.0289],\n",
      "          [ 0.0612, -0.0022, -0.0665,  0.0364,  0.0048],\n",
      "          [ 0.0302, -0.0157, -0.0515,  0.0417, -0.0143],\n",
      "          [-0.0466,  0.0157, -0.0198,  0.0359, -0.0018],\n",
      "          [ 0.0351,  0.0768,  0.0510, -0.0669,  0.0808]],\n",
      "\n",
      "         [[ 0.0039, -0.0663,  0.0393,  0.0306,  0.0126],\n",
      "          [ 0.0461,  0.0696, -0.0606,  0.0559, -0.0720],\n",
      "          [-0.0627,  0.0298,  0.0447, -0.0418, -0.0110],\n",
      "          [-0.0133,  0.0177,  0.0444, -0.0793,  0.0101],\n",
      "          [ 0.0659,  0.0142,  0.0060, -0.0724,  0.0151]],\n",
      "\n",
      "         [[-0.0476, -0.0677, -0.0119,  0.0358,  0.0488],\n",
      "          [-0.0147,  0.0555, -0.0789,  0.0075, -0.0724],\n",
      "          [-0.0686, -0.0564, -0.0225, -0.0560, -0.0804],\n",
      "          [-0.0455,  0.0779,  0.0413,  0.0654, -0.0634],\n",
      "          [-0.0719,  0.0627,  0.0765,  0.0399, -0.0656]],\n",
      "\n",
      "         [[-0.0025, -0.0693,  0.0753, -0.0747, -0.0788],\n",
      "          [ 0.0596,  0.0364, -0.0220,  0.0028, -0.0072],\n",
      "          [ 0.0744,  0.0475,  0.0091,  0.0224,  0.0184],\n",
      "          [ 0.0796, -0.0798,  0.0764,  0.0760,  0.0157],\n",
      "          [-0.0233,  0.0107, -0.0121,  0.0770,  0.0344]]]], requires_grad=True)), ('conv2.bias', Parameter containing:\n",
      "tensor([ 0.0431,  0.0592, -0.0377, -0.0025, -0.0416, -0.0160,  0.0681, -0.0060,\n",
      "        -0.0100,  0.0042, -0.0280, -0.0762,  0.0007,  0.0756, -0.0039,  0.0358],\n",
      "       requires_grad=True)), ('fc1.weight', Parameter containing:\n",
      "tensor([[-0.0036, -0.0217, -0.0047,  ...,  0.0058, -0.0377, -0.0229],\n",
      "        [-0.0110,  0.0497, -0.0177,  ...,  0.0426,  0.0180,  0.0032],\n",
      "        [-0.0132,  0.0498, -0.0263,  ..., -0.0248,  0.0037,  0.0332],\n",
      "        ...,\n",
      "        [-0.0390,  0.0143,  0.0418,  ..., -0.0256,  0.0040,  0.0281],\n",
      "        [-0.0154,  0.0230, -0.0264,  ..., -0.0491,  0.0110,  0.0104],\n",
      "        [ 0.0202, -0.0054,  0.0363,  ...,  0.0190, -0.0250, -0.0212]],\n",
      "       requires_grad=True)), ('fc1.bias', Parameter containing:\n",
      "tensor([-0.0486, -0.0414,  0.0435,  0.0118,  0.0111, -0.0180, -0.0457, -0.0339,\n",
      "         0.0363,  0.0035, -0.0334, -0.0098,  0.0309, -0.0369,  0.0021,  0.0359,\n",
      "         0.0398,  0.0448, -0.0178, -0.0072, -0.0321, -0.0031, -0.0399, -0.0482,\n",
      "         0.0496, -0.0185,  0.0096,  0.0305,  0.0039, -0.0498,  0.0157,  0.0086,\n",
      "        -0.0269, -0.0035, -0.0306, -0.0146, -0.0499,  0.0381, -0.0349, -0.0220,\n",
      "         0.0413,  0.0281,  0.0073, -0.0238, -0.0298,  0.0407,  0.0142, -0.0455,\n",
      "        -0.0037, -0.0384,  0.0499,  0.0006,  0.0132,  0.0499, -0.0308, -0.0063,\n",
      "         0.0298,  0.0055, -0.0105, -0.0385, -0.0062, -0.0307, -0.0363,  0.0420,\n",
      "         0.0383,  0.0170,  0.0368,  0.0004, -0.0060,  0.0105, -0.0363,  0.0047,\n",
      "         0.0220,  0.0460,  0.0111,  0.0202,  0.0108, -0.0158, -0.0154,  0.0026,\n",
      "         0.0338,  0.0200,  0.0318,  0.0088,  0.0417,  0.0353, -0.0263, -0.0476,\n",
      "        -0.0251, -0.0475, -0.0287, -0.0450, -0.0105,  0.0496,  0.0328,  0.0078,\n",
      "         0.0142, -0.0174, -0.0388, -0.0303, -0.0206, -0.0454,  0.0078,  0.0237,\n",
      "         0.0080,  0.0230,  0.0398,  0.0494,  0.0350,  0.0060,  0.0022,  0.0366,\n",
      "        -0.0179,  0.0256, -0.0037,  0.0270,  0.0387, -0.0117, -0.0404,  0.0420],\n",
      "       requires_grad=True)), ('fc2.weight', Parameter containing:\n",
      "tensor([[ 0.0543,  0.0422,  0.0299,  ...,  0.0291, -0.0908,  0.0039],\n",
      "        [ 0.0149, -0.0364,  0.0476,  ..., -0.0431,  0.0426,  0.0354],\n",
      "        [ 0.0594, -0.0622, -0.0195,  ...,  0.0819,  0.0023, -0.0368],\n",
      "        ...,\n",
      "        [ 0.0588,  0.0435,  0.0282,  ..., -0.0768, -0.0258, -0.0793],\n",
      "        [-0.0445, -0.0795,  0.0496,  ...,  0.0382, -0.0788, -0.0212],\n",
      "        [-0.0790, -0.0695,  0.0638,  ..., -0.0676, -0.0274,  0.0767]],\n",
      "       requires_grad=True)), ('fc2.bias', Parameter containing:\n",
      "tensor([ 0.0054,  0.0781,  0.0870, -0.0162,  0.0813,  0.0306, -0.0181, -0.0716,\n",
      "        -0.0259, -0.0269,  0.0543,  0.0807,  0.0353, -0.0624, -0.0571,  0.0690,\n",
      "         0.0059,  0.0607,  0.0228, -0.0558, -0.0675, -0.0834,  0.0258, -0.0641,\n",
      "         0.0608, -0.0011,  0.0809,  0.0456, -0.0874,  0.0195,  0.0668, -0.0832,\n",
      "         0.0443,  0.0739,  0.0354,  0.0202, -0.0679, -0.0607,  0.0871, -0.0413,\n",
      "         0.0755, -0.0246, -0.0469,  0.0773, -0.0494, -0.0237,  0.0460, -0.0565,\n",
      "        -0.0490, -0.0301,  0.0541,  0.0010, -0.0459, -0.0748, -0.0182, -0.0269,\n",
      "        -0.0650,  0.0083, -0.0909,  0.0762,  0.0544,  0.0097,  0.0249, -0.0609,\n",
      "        -0.0870,  0.0126,  0.0221, -0.0230, -0.0753,  0.0251,  0.0439, -0.0080,\n",
      "         0.0012, -0.0661,  0.0407, -0.0309, -0.0047, -0.0572,  0.0292,  0.0456,\n",
      "         0.0169,  0.0520, -0.0646,  0.0542], requires_grad=True)), ('fc3.weight', Parameter containing:\n",
      "tensor([[-0.0990,  0.0130, -0.0432, -0.0482, -0.0141, -0.0670, -0.0272,  0.0143,\n",
      "         -0.0466, -0.0521,  0.0934,  0.0590,  0.0140,  0.0783,  0.0677,  0.0016,\n",
      "          0.0797, -0.1090,  0.0668, -0.0565,  0.0749,  0.0083,  0.0163,  0.0299,\n",
      "          0.0916, -0.0003, -0.0732,  0.0645,  0.0661,  0.0058,  0.0907,  0.0004,\n",
      "          0.0731,  0.0042,  0.0991, -0.1048,  0.0156, -0.0752, -0.0715,  0.0856,\n",
      "         -0.0947, -0.0817, -0.0430,  0.0649,  0.0129, -0.0543, -0.0943,  0.0142,\n",
      "         -0.0679, -0.0042, -0.0217, -0.0036,  0.0208,  0.0453, -0.0502,  0.0646,\n",
      "         -0.0901,  0.0157, -0.1028,  0.0575,  0.0996, -0.0824, -0.0175, -0.0462,\n",
      "          0.1078,  0.0827, -0.0477, -0.0907, -0.0764, -0.1042,  0.1040, -0.0121,\n",
      "          0.0294, -0.0693,  0.0631, -0.0888,  0.0036, -0.0509,  0.0014, -0.0724,\n",
      "          0.0325,  0.0050, -0.0873, -0.0369],\n",
      "        [-0.0526, -0.1028,  0.0796,  0.0637, -0.0497, -0.0708,  0.0293, -0.0946,\n",
      "          0.0471,  0.0349,  0.1054, -0.0836,  0.0275, -0.0552,  0.0779,  0.0154,\n",
      "         -0.0870, -0.0676, -0.0613, -0.0260, -0.0598,  0.0488,  0.0377,  0.0794,\n",
      "         -0.0102,  0.0764, -0.0579, -0.0674, -0.1088,  0.0358, -0.0624, -0.0111,\n",
      "         -0.0154, -0.0102, -0.0185, -0.0833, -0.1003, -0.0265,  0.0435, -0.0615,\n",
      "         -0.0006,  0.0009, -0.0741, -0.1003,  0.0901,  0.0670,  0.0375,  0.1035,\n",
      "          0.0483,  0.0971,  0.0226,  0.0135,  0.0499,  0.0903, -0.0934, -0.0919,\n",
      "         -0.0702,  0.1010,  0.0965,  0.0832, -0.0184, -0.1071,  0.0986, -0.0945,\n",
      "          0.0717,  0.0627, -0.0923, -0.0497, -0.0312, -0.0102,  0.1034,  0.0456,\n",
      "          0.0285, -0.0083,  0.0364,  0.1085,  0.0785,  0.0826,  0.0565,  0.0396,\n",
      "          0.0610,  0.0320,  0.0611, -0.0437],\n",
      "        [ 0.0865, -0.0158, -0.0731,  0.0248, -0.0452, -0.0547, -0.1075,  0.0907,\n",
      "          0.0681,  0.0295,  0.0645, -0.0004,  0.0020, -0.0919,  0.0169, -0.0485,\n",
      "          0.0735, -0.0712,  0.0107, -0.0036, -0.0241, -0.0381,  0.1015, -0.0660,\n",
      "         -0.0513,  0.0156,  0.0822, -0.0468,  0.0144, -0.0857,  0.0221,  0.0944,\n",
      "          0.0531, -0.0511, -0.0958,  0.0175,  0.0369,  0.0926, -0.0057, -0.0358,\n",
      "         -0.0067,  0.0101,  0.0498,  0.0461,  0.0484, -0.0344,  0.0779,  0.0112,\n",
      "          0.1077, -0.0997,  0.0802, -0.0094, -0.0884, -0.0933,  0.0299,  0.0283,\n",
      "         -0.0157,  0.1042, -0.0827,  0.0738,  0.0547,  0.0860,  0.0124,  0.0127,\n",
      "         -0.0113,  0.0124,  0.0117,  0.0701, -0.0578,  0.0319, -0.0991, -0.0133,\n",
      "         -0.0714,  0.0869,  0.0055,  0.0817,  0.0640, -0.0123,  0.0861, -0.0737,\n",
      "          0.0648,  0.1086,  0.0665, -0.0836],\n",
      "        [-0.0796, -0.0102,  0.0114, -0.0537,  0.0954,  0.0871,  0.0095, -0.0853,\n",
      "          0.0802, -0.1084, -0.0180,  0.0522,  0.0282, -0.0962, -0.0788, -0.0398,\n",
      "          0.0135,  0.0253,  0.0416, -0.0909, -0.0625,  0.0016, -0.0819,  0.0003,\n",
      "         -0.0822, -0.0415,  0.0574,  0.0849, -0.0389, -0.0309, -0.0754,  0.0702,\n",
      "         -0.0951, -0.0951,  0.0381,  0.0136, -0.1068,  0.0865, -0.0099, -0.0946,\n",
      "         -0.0825, -0.0258,  0.0324,  0.0146, -0.0223, -0.0408,  0.0881,  0.0531,\n",
      "          0.0711,  0.0742, -0.0371, -0.0386, -0.0648, -0.0494,  0.0745, -0.0552,\n",
      "          0.0054,  0.0276, -0.0949, -0.1030,  0.0037, -0.0955, -0.1049, -0.0027,\n",
      "          0.0156,  0.0172, -0.0833, -0.0817,  0.0665, -0.0026, -0.0383, -0.0856,\n",
      "          0.1027, -0.0226,  0.0711, -0.0122,  0.0892, -0.0670, -0.1064, -0.0668,\n",
      "         -0.0669,  0.0640,  0.0412, -0.0696],\n",
      "        [-0.0705, -0.0256,  0.0048,  0.0744, -0.0385, -0.0531,  0.0698,  0.0859,\n",
      "         -0.0714, -0.0297,  0.0115, -0.0424,  0.0354, -0.0275,  0.0086,  0.0986,\n",
      "          0.0409,  0.0215, -0.0743,  0.0517, -0.0770,  0.0390, -0.1040,  0.0766,\n",
      "         -0.0330,  0.0179,  0.0009, -0.0740,  0.1048, -0.1087, -0.0317, -0.0949,\n",
      "          0.0656,  0.0602,  0.0937,  0.0963, -0.0558,  0.0688,  0.0430, -0.0143,\n",
      "         -0.0899, -0.0311, -0.0466,  0.0374,  0.0923, -0.0895, -0.0774,  0.0559,\n",
      "         -0.0739,  0.0215,  0.0514,  0.0544,  0.0252, -0.0015, -0.0609, -0.0820,\n",
      "         -0.0063, -0.0026,  0.1033, -0.0480,  0.0109,  0.0288,  0.0269, -0.0867,\n",
      "         -0.0860,  0.1043, -0.0658,  0.0956,  0.0457,  0.0150,  0.0524,  0.0897,\n",
      "          0.0293, -0.0161, -0.0281,  0.0984, -0.0889,  0.1083, -0.0905, -0.0672,\n",
      "         -0.0909,  0.0541, -0.0247, -0.0489],\n",
      "        [ 0.0806,  0.0643,  0.0344, -0.0181, -0.0663, -0.0166, -0.0418, -0.0544,\n",
      "         -0.0466, -0.0326,  0.0601, -0.0276, -0.0193,  0.0228, -0.0910, -0.0593,\n",
      "         -0.0649,  0.0372, -0.0226, -0.1060,  0.0154,  0.0922, -0.0250, -0.0294,\n",
      "          0.0238, -0.0054,  0.0824,  0.0880, -0.0209, -0.0408, -0.0910,  0.0718,\n",
      "          0.0796,  0.0356, -0.0984,  0.0673, -0.0280,  0.0837,  0.0176,  0.0489,\n",
      "          0.0290,  0.0187, -0.0846, -0.0045,  0.1070,  0.0141,  0.0765, -0.0646,\n",
      "         -0.1064,  0.0608, -0.0489,  0.0103,  0.0606, -0.0288,  0.0740, -0.0136,\n",
      "          0.0821,  0.0243,  0.0251,  0.0430, -0.0300,  0.0797,  0.0832,  0.0898,\n",
      "          0.1084,  0.0497, -0.0029, -0.0399, -0.0522, -0.0686, -0.0508, -0.0442,\n",
      "         -0.0934,  0.0132, -0.0716, -0.0398,  0.0959, -0.1082,  0.0790, -0.0799,\n",
      "         -0.0834,  0.0021,  0.0454,  0.0419],\n",
      "        [-0.0316, -0.0982, -0.0488, -0.0067,  0.0641, -0.1020, -0.0922,  0.0317,\n",
      "          0.0326, -0.0905, -0.0586, -0.0869,  0.0817, -0.0890, -0.0141,  0.0689,\n",
      "         -0.0450,  0.0359,  0.0393,  0.0990, -0.0859,  0.0806, -0.0343,  0.0502,\n",
      "          0.0841,  0.0145,  0.0843,  0.0224, -0.0673, -0.1089,  0.0232, -0.0351,\n",
      "          0.0288,  0.0244,  0.0895, -0.1053,  0.0505, -0.1006,  0.0223, -0.0949,\n",
      "         -0.0090,  0.0601, -0.0003,  0.0062, -0.0686, -0.0760,  0.0266,  0.0549,\n",
      "         -0.0510, -0.0881, -0.0684, -0.0894, -0.0192, -0.0110,  0.0128, -0.0966,\n",
      "          0.0735,  0.0173,  0.0760,  0.0709, -0.0022, -0.0755,  0.0976,  0.0882,\n",
      "         -0.0482, -0.0571, -0.0681,  0.0864, -0.0844, -0.0328, -0.0244, -0.0778,\n",
      "         -0.0828,  0.0979,  0.0748,  0.0934, -0.0215,  0.0816,  0.0635, -0.0818,\n",
      "         -0.0054, -0.0027, -0.0111, -0.0237],\n",
      "        [-0.0749, -0.0592, -0.0130,  0.0038,  0.0637, -0.0804,  0.0559,  0.0604,\n",
      "         -0.0246,  0.0119, -0.0177,  0.0031,  0.0165,  0.1072,  0.0581, -0.0081,\n",
      "          0.0664, -0.0262, -0.0951,  0.0330,  0.0024,  0.0904, -0.0010,  0.0047,\n",
      "          0.0078, -0.0317, -0.0954, -0.0201, -0.1022, -0.0566,  0.0347, -0.0203,\n",
      "         -0.0559, -0.0166,  0.0674,  0.0593,  0.0874,  0.0231,  0.0549,  0.0935,\n",
      "          0.0979,  0.0989, -0.0251, -0.0570,  0.1036,  0.0630, -0.0068, -0.0941,\n",
      "         -0.0493, -0.1059, -0.0868, -0.0885, -0.0201,  0.0559, -0.0934, -0.0530,\n",
      "         -0.0954,  0.0679, -0.0669,  0.0654,  0.0741,  0.0356,  0.0189,  0.0193,\n",
      "          0.0119,  0.0213,  0.0706,  0.0395, -0.1069, -0.0548, -0.0921, -0.0971,\n",
      "          0.0201,  0.1048,  0.0283, -0.0368, -0.0651,  0.0481,  0.0141, -0.0042,\n",
      "          0.0890,  0.0363, -0.0225, -0.0127],\n",
      "        [-0.0498,  0.0437,  0.0529, -0.0668,  0.0199, -0.0389, -0.0204, -0.0234,\n",
      "          0.0032,  0.0841, -0.0381,  0.0993,  0.0768, -0.0106, -0.0237, -0.0965,\n",
      "          0.0215,  0.0139,  0.0808,  0.0371, -0.0156,  0.0995,  0.0904,  0.0472,\n",
      "         -0.0646,  0.0656, -0.0973,  0.1010,  0.0446,  0.0257, -0.0593, -0.0467,\n",
      "          0.0217, -0.0846,  0.0578,  0.0737,  0.0106, -0.0136,  0.0308, -0.0729,\n",
      "          0.0262,  0.0733, -0.0250, -0.0009, -0.0824,  0.1022,  0.1057, -0.0540,\n",
      "          0.0852, -0.0924,  0.0584, -0.0775, -0.0829,  0.0512, -0.1028, -0.0429,\n",
      "          0.0121, -0.0023, -0.0236,  0.0462,  0.0119, -0.0818, -0.0408,  0.0430,\n",
      "         -0.0310, -0.0760, -0.0332,  0.0094, -0.0087,  0.0960,  0.0615,  0.0245,\n",
      "         -0.0782,  0.0818, -0.0371,  0.0111, -0.0048, -0.0669, -0.0934, -0.1037,\n",
      "         -0.0038, -0.0503,  0.0837,  0.0809],\n",
      "        [-0.1072, -0.0061, -0.0473,  0.0847, -0.0108,  0.0192, -0.0609,  0.0471,\n",
      "         -0.0349, -0.0257,  0.0195, -0.1028,  0.0491,  0.0391,  0.0893, -0.1084,\n",
      "          0.0901, -0.0223, -0.0357, -0.0902, -0.0022, -0.0383,  0.0187, -0.0031,\n",
      "         -0.0070, -0.0539, -0.0660, -0.0248,  0.0522,  0.0784, -0.0959, -0.0863,\n",
      "          0.0533,  0.0763,  0.0253,  0.0789,  0.0885,  0.0060,  0.0952, -0.0459,\n",
      "         -0.0198,  0.0683,  0.0076,  0.0235, -0.0199, -0.0328, -0.0546, -0.0368,\n",
      "         -0.0053,  0.0192, -0.0075,  0.0052, -0.0170,  0.1033,  0.1031, -0.0034,\n",
      "         -0.0615,  0.0398, -0.0414, -0.0743, -0.0693, -0.0245, -0.0130,  0.0094,\n",
      "         -0.0364,  0.1014,  0.0063,  0.0843,  0.0299,  0.0349,  0.1043,  0.0553,\n",
      "          0.0444, -0.0808, -0.0846, -0.0571,  0.0198, -0.0733, -0.0027,  0.0747,\n",
      "         -0.0047,  0.0579,  0.0524,  0.1003]], requires_grad=True)), ('fc3.bias', Parameter containing:\n",
      "tensor([-0.0740,  0.0635,  0.1077, -0.0462, -0.0459, -0.0768,  0.0636,  0.0623,\n",
      "        -0.0041, -0.0742], requires_grad=True))]\n",
      "[Parameter containing:\n",
      "tensor([[[[ 1.5421e-01, -1.2760e-02,  1.7148e-01, -1.0920e-05, -1.1357e-01],\n",
      "          [-1.0583e-01, -8.2900e-03,  1.7941e-01,  6.6240e-02, -1.5408e-01],\n",
      "          [ 1.8438e-01, -1.7201e-01, -2.8445e-02, -2.1170e-02, -7.6009e-02],\n",
      "          [ 1.5098e-01,  6.1614e-02, -3.9796e-02,  1.6809e-01,  1.9390e-01],\n",
      "          [-1.4025e-01,  5.9152e-02,  9.3119e-02, -1.2812e-01,  1.5073e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.9574e-02,  1.7498e-01, -1.1324e-01, -1.9779e-01,  1.7495e-01],\n",
      "          [ 9.4930e-02, -1.4430e-01,  1.4850e-01, -1.9941e-01,  1.6103e-01],\n",
      "          [-1.6562e-01, -9.4148e-02, -5.0372e-02, -7.7963e-02,  1.7822e-01],\n",
      "          [ 1.0875e-01, -1.9102e-01, -1.2377e-01, -1.8269e-01,  1.9430e-01],\n",
      "          [ 1.7640e-01, -6.6218e-02, -1.9421e-01,  1.6191e-01,  1.2358e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.6979e-02, -1.5262e-01, -8.7070e-02,  6.9412e-02, -1.1991e-01],\n",
      "          [ 7.4968e-02, -1.6417e-01,  1.8857e-02, -2.1248e-02,  2.7089e-02],\n",
      "          [ 1.7792e-01,  2.3576e-03, -1.5850e-01,  1.3479e-01,  2.1980e-02],\n",
      "          [-1.4167e-01,  1.9049e-01,  1.6152e-01, -1.8377e-01,  1.8078e-01],\n",
      "          [-9.1209e-02,  1.0307e-01,  1.4772e-01,  1.2343e-01, -1.5440e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0732e-03,  5.1267e-02,  1.1738e-02, -3.4220e-03, -5.1624e-02],\n",
      "          [-4.6804e-03, -4.8332e-02,  5.8460e-02,  1.6075e-01, -1.2379e-01],\n",
      "          [-2.5536e-02,  1.1524e-01, -2.9624e-02,  3.0960e-03,  1.1039e-01],\n",
      "          [ 4.2373e-02,  1.9950e-01,  7.5429e-02, -1.5179e-01,  5.6623e-02],\n",
      "          [ 8.8069e-02, -1.5151e-01, -4.3720e-02, -1.7002e-01,  1.7791e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8668e-01, -9.5943e-02,  3.1235e-02, -1.8983e-01, -1.0103e-01],\n",
      "          [ 1.0881e-01, -6.2329e-02, -5.7947e-02,  1.2529e-01, -2.2828e-02],\n",
      "          [-3.3154e-02,  8.6161e-02, -1.3503e-01,  1.6009e-01,  9.4467e-02],\n",
      "          [ 3.1969e-02,  3.1655e-02,  1.5406e-01,  2.2576e-02,  8.8456e-02],\n",
      "          [-1.8176e-01, -1.1693e-01,  1.7511e-01,  1.9407e-01,  3.8362e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.7344e-02,  1.6616e-01,  2.2568e-02,  1.7927e-02, -7.6326e-02],\n",
      "          [-8.4304e-02, -1.3810e-01,  1.2302e-01, -1.5034e-01, -1.7635e-01],\n",
      "          [ 2.2991e-02, -1.8643e-01, -1.7082e-01, -2.3778e-02, -1.9087e-01],\n",
      "          [ 1.0838e-01, -5.9832e-02,  1.5071e-01,  1.2121e-01,  6.2252e-02],\n",
      "          [ 1.9416e-01,  9.1070e-02, -1.5781e-01, -9.3969e-02,  1.0056e-01]]]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0539, -0.0175, -0.1239, -0.1323,  0.1621,  0.0125],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0737,  0.0801,  0.0090,  0.0081, -0.0359],\n",
      "          [-0.0376,  0.0450, -0.0006, -0.0352,  0.0164],\n",
      "          [ 0.0480, -0.0764,  0.0250,  0.0349, -0.0683],\n",
      "          [ 0.0547,  0.0346, -0.0497, -0.0685,  0.0240],\n",
      "          [-0.0448, -0.0748,  0.0730,  0.0524,  0.0781]],\n",
      "\n",
      "         [[-0.0593,  0.0057,  0.0167,  0.0026,  0.0031],\n",
      "          [ 0.0115,  0.0524, -0.0686, -0.0657, -0.0220],\n",
      "          [-0.0710,  0.0296,  0.0646, -0.0420,  0.0656],\n",
      "          [-0.0244,  0.0327, -0.0197,  0.0807,  0.0637],\n",
      "          [ 0.0013,  0.0495, -0.0214,  0.0499,  0.0423]],\n",
      "\n",
      "         [[ 0.0032, -0.0088,  0.0021, -0.0216,  0.0724],\n",
      "          [-0.0610, -0.0719, -0.0596, -0.0355, -0.0737],\n",
      "          [-0.0767, -0.0456,  0.0723, -0.0412, -0.0216],\n",
      "          [ 0.0217,  0.0184,  0.0265, -0.0719, -0.0370],\n",
      "          [-0.0156, -0.0537,  0.0757,  0.0083,  0.0095]],\n",
      "\n",
      "         [[ 0.0542,  0.0180, -0.0118,  0.0560, -0.0795],\n",
      "          [ 0.0305, -0.0612,  0.0352,  0.0223, -0.0086],\n",
      "          [ 0.0418, -0.0512,  0.0281,  0.0150,  0.0404],\n",
      "          [ 0.0238, -0.0026,  0.0620, -0.0228, -0.0528],\n",
      "          [-0.0801, -0.0072,  0.0093,  0.0246,  0.0698]],\n",
      "\n",
      "         [[ 0.0050,  0.0730,  0.0539, -0.0553, -0.0675],\n",
      "          [-0.0080,  0.0641, -0.0262,  0.0166,  0.0417],\n",
      "          [ 0.0058,  0.0408,  0.0504,  0.0359,  0.0521],\n",
      "          [ 0.0105,  0.0256, -0.0176, -0.0452, -0.0322],\n",
      "          [-0.0511,  0.0404,  0.0244,  0.0737,  0.0689]],\n",
      "\n",
      "         [[-0.0250,  0.0749,  0.0187, -0.0773, -0.0691],\n",
      "          [-0.0040, -0.0160, -0.0546, -0.0107, -0.0660],\n",
      "          [-0.0353, -0.0407,  0.0211,  0.0086, -0.0023],\n",
      "          [ 0.0503,  0.0446, -0.0224, -0.0514,  0.0566],\n",
      "          [-0.0381,  0.0273,  0.0685,  0.0365,  0.0290]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0590,  0.0184,  0.0286, -0.0084, -0.0488],\n",
      "          [ 0.0346, -0.0752,  0.0020,  0.0689,  0.0755],\n",
      "          [ 0.0121,  0.0746,  0.0302, -0.0443,  0.0319],\n",
      "          [ 0.0410, -0.0200, -0.0320,  0.0422, -0.0326],\n",
      "          [-0.0156, -0.0411, -0.0185,  0.0468, -0.0098]],\n",
      "\n",
      "         [[-0.0699,  0.0165, -0.0414, -0.0683,  0.0783],\n",
      "          [-0.0552, -0.0297, -0.0490,  0.0008,  0.0453],\n",
      "          [-0.0710,  0.0070, -0.0215, -0.0189,  0.0586],\n",
      "          [ 0.0594,  0.0048,  0.0498,  0.0039, -0.0141],\n",
      "          [ 0.0769, -0.0567,  0.0519,  0.0427,  0.0614]],\n",
      "\n",
      "         [[-0.0289,  0.0071,  0.0126,  0.0274, -0.0183],\n",
      "          [-0.0266,  0.0594, -0.0254,  0.0397, -0.0012],\n",
      "          [ 0.0196, -0.0170,  0.0170,  0.0595, -0.0174],\n",
      "          [-0.0042,  0.0165,  0.0388,  0.0303,  0.0481],\n",
      "          [ 0.0734, -0.0007,  0.0215, -0.0611, -0.0119]],\n",
      "\n",
      "         [[ 0.0054, -0.0002,  0.0285,  0.0028, -0.0421],\n",
      "          [ 0.0122, -0.0575,  0.0691, -0.0190, -0.0786],\n",
      "          [ 0.0054, -0.0768,  0.0446, -0.0791,  0.0129],\n",
      "          [-0.0134,  0.0637,  0.0390,  0.0627,  0.0763],\n",
      "          [ 0.0143,  0.0465,  0.0085,  0.0230,  0.0310]],\n",
      "\n",
      "         [[ 0.0309,  0.0253, -0.0765, -0.0438, -0.0757],\n",
      "          [ 0.0629,  0.0399, -0.0259,  0.0232,  0.0334],\n",
      "          [ 0.0008,  0.0375, -0.0477,  0.0466, -0.0079],\n",
      "          [-0.0635,  0.0513, -0.0103, -0.0692,  0.0063],\n",
      "          [ 0.0769,  0.0331, -0.0004,  0.0659, -0.0816]],\n",
      "\n",
      "         [[-0.0420, -0.0410,  0.0606,  0.0196, -0.0103],\n",
      "          [-0.0723,  0.0166, -0.0143, -0.0228,  0.0748],\n",
      "          [-0.0502, -0.0331, -0.0015, -0.0580, -0.0125],\n",
      "          [ 0.0094,  0.0651, -0.0242, -0.0691, -0.0100],\n",
      "          [-0.0109,  0.0602, -0.0242,  0.0798, -0.0256]]],\n",
      "\n",
      "\n",
      "        [[[-0.0461, -0.0676, -0.0731, -0.0680, -0.0254],\n",
      "          [-0.0246,  0.0322,  0.0343,  0.0749,  0.0760],\n",
      "          [-0.0258, -0.0290,  0.0434, -0.0137,  0.0263],\n",
      "          [ 0.0109, -0.0489,  0.0480, -0.0732,  0.0386],\n",
      "          [ 0.0562,  0.0601, -0.0058,  0.0334, -0.0428]],\n",
      "\n",
      "         [[-0.0009,  0.0609, -0.0481,  0.0122, -0.0289],\n",
      "          [-0.0025, -0.0335,  0.0333,  0.0811,  0.0265],\n",
      "          [ 0.0332, -0.0017, -0.0390, -0.0790,  0.0307],\n",
      "          [-0.0522, -0.0811,  0.0455, -0.0031, -0.0278],\n",
      "          [ 0.0134,  0.0649, -0.0511,  0.0606,  0.0435]],\n",
      "\n",
      "         [[ 0.0672,  0.0346,  0.0501, -0.0515, -0.0403],\n",
      "          [-0.0430,  0.0373,  0.0400, -0.0038,  0.0717],\n",
      "          [ 0.0417, -0.0435,  0.0754,  0.0630,  0.0466],\n",
      "          [ 0.0606,  0.0357, -0.0189,  0.0567, -0.0105],\n",
      "          [ 0.0816,  0.0480, -0.0541, -0.0379, -0.0276]],\n",
      "\n",
      "         [[-0.0141,  0.0327, -0.0246, -0.0758, -0.0468],\n",
      "          [ 0.0356, -0.0061, -0.0104,  0.0689, -0.0696],\n",
      "          [-0.0085, -0.0667, -0.0789, -0.0452,  0.0283],\n",
      "          [-0.0455,  0.0122,  0.0200, -0.0400, -0.0241],\n",
      "          [ 0.0426, -0.0718, -0.0109, -0.0029, -0.0424]],\n",
      "\n",
      "         [[ 0.0259, -0.0726, -0.0604, -0.0201, -0.0231],\n",
      "          [ 0.0801, -0.0353, -0.0550, -0.0397, -0.0722],\n",
      "          [ 0.0218, -0.0669, -0.0738,  0.0745, -0.0360],\n",
      "          [ 0.0691, -0.0147, -0.0418,  0.0024, -0.0143],\n",
      "          [-0.0388, -0.0055,  0.0395, -0.0732,  0.0777]],\n",
      "\n",
      "         [[ 0.0607,  0.0237,  0.0123, -0.0518, -0.0204],\n",
      "          [ 0.0442,  0.0275, -0.0708,  0.0425,  0.0234],\n",
      "          [-0.0163, -0.0463, -0.0361,  0.0009,  0.0239],\n",
      "          [ 0.0558, -0.0479,  0.0194, -0.0756, -0.0688],\n",
      "          [-0.0716, -0.0728, -0.0447,  0.0262,  0.0787]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0473,  0.0088, -0.0249, -0.0573, -0.0788],\n",
      "          [ 0.0442, -0.0050, -0.0581,  0.0382, -0.0358],\n",
      "          [ 0.0313, -0.0242,  0.0013,  0.0738, -0.0025],\n",
      "          [-0.0534,  0.0535,  0.0348, -0.0035, -0.0167],\n",
      "          [ 0.0348,  0.0468, -0.0615,  0.0381,  0.0198]],\n",
      "\n",
      "         [[ 0.0327,  0.0807,  0.0628,  0.0073,  0.0684],\n",
      "          [ 0.0221,  0.0299, -0.0315, -0.0123,  0.0600],\n",
      "          [-0.0671,  0.0799,  0.0229, -0.0374, -0.0223],\n",
      "          [-0.0159, -0.0575, -0.0242, -0.0087,  0.0043],\n",
      "          [-0.0327,  0.0386, -0.0340, -0.0659, -0.0369]],\n",
      "\n",
      "         [[ 0.0380,  0.0550, -0.0727, -0.0154,  0.0085],\n",
      "          [ 0.0211,  0.0328, -0.0267,  0.0654, -0.0142],\n",
      "          [ 0.0451, -0.0422,  0.0551, -0.0608, -0.0604],\n",
      "          [ 0.0736,  0.0256, -0.0755, -0.0656, -0.0605],\n",
      "          [-0.0687,  0.0078, -0.0815,  0.0487, -0.0111]],\n",
      "\n",
      "         [[-0.0010,  0.0662, -0.0274, -0.0749,  0.0029],\n",
      "          [-0.0468, -0.0268,  0.0312,  0.0459, -0.0571],\n",
      "          [-0.0706, -0.0797, -0.0429, -0.0764, -0.0520],\n",
      "          [ 0.0001, -0.0439,  0.0146, -0.0529, -0.0618],\n",
      "          [-0.0446, -0.0656,  0.0399,  0.0724,  0.0153]],\n",
      "\n",
      "         [[-0.0186,  0.0057,  0.0729, -0.0694,  0.0614],\n",
      "          [ 0.0385, -0.0416, -0.0261,  0.0171, -0.0130],\n",
      "          [-0.0349,  0.0726,  0.0092, -0.0549,  0.0210],\n",
      "          [-0.0569,  0.0081,  0.0222,  0.0804, -0.0011],\n",
      "          [-0.0202, -0.0228, -0.0062, -0.0583,  0.0176]],\n",
      "\n",
      "         [[ 0.0733, -0.0165,  0.0794,  0.0351,  0.0739],\n",
      "          [-0.0179,  0.0036,  0.0341,  0.0755,  0.0083],\n",
      "          [ 0.0446, -0.0146, -0.0738,  0.0479,  0.0182],\n",
      "          [ 0.0026,  0.0738, -0.0560, -0.0516,  0.0261],\n",
      "          [ 0.0798,  0.0289, -0.0096, -0.0314, -0.0055]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0761, -0.0494,  0.0083, -0.0760,  0.0657],\n",
      "          [ 0.0009, -0.0274,  0.0785,  0.0429, -0.0347],\n",
      "          [ 0.0435, -0.0160, -0.0246,  0.0343,  0.0073],\n",
      "          [-0.0096,  0.0303, -0.0508, -0.0729,  0.0517],\n",
      "          [-0.0093,  0.0421,  0.0716, -0.0269,  0.0367]],\n",
      "\n",
      "         [[-0.0055, -0.0053,  0.0188, -0.0712,  0.0553],\n",
      "          [-0.0458, -0.0228,  0.0609, -0.0451,  0.0632],\n",
      "          [-0.0545,  0.0313, -0.0469, -0.0322, -0.0668],\n",
      "          [-0.0102,  0.0314, -0.0367, -0.0649,  0.0034],\n",
      "          [ 0.0443,  0.0029, -0.0287, -0.0405,  0.0122]],\n",
      "\n",
      "         [[ 0.0023, -0.0640, -0.0351,  0.0684, -0.0150],\n",
      "          [ 0.0250, -0.0105,  0.0736,  0.0052,  0.0026],\n",
      "          [-0.0092, -0.0085,  0.0383,  0.0393, -0.0400],\n",
      "          [-0.0006, -0.0024, -0.0626,  0.0328,  0.0374],\n",
      "          [ 0.0583, -0.0163,  0.0225, -0.0571,  0.0731]],\n",
      "\n",
      "         [[ 0.0092, -0.0101, -0.0501, -0.0025, -0.0313],\n",
      "          [-0.0238, -0.0287,  0.0423, -0.0301, -0.0039],\n",
      "          [-0.0185,  0.0487,  0.0246, -0.0443, -0.0456],\n",
      "          [ 0.0743, -0.0598,  0.0474,  0.0342, -0.0269],\n",
      "          [ 0.0512, -0.0140,  0.0037, -0.0753,  0.0687]],\n",
      "\n",
      "         [[ 0.0183, -0.0651, -0.0775,  0.0361, -0.0733],\n",
      "          [-0.0531, -0.0543,  0.0195,  0.0485, -0.0250],\n",
      "          [ 0.0084,  0.0650, -0.0007,  0.0361, -0.0734],\n",
      "          [ 0.0120,  0.0339,  0.0706, -0.0540,  0.0113],\n",
      "          [ 0.0477,  0.0062,  0.0652, -0.0690, -0.0430]],\n",
      "\n",
      "         [[-0.0695, -0.0154,  0.0162, -0.0318, -0.0308],\n",
      "          [-0.0328,  0.0527,  0.0083, -0.0070, -0.0029],\n",
      "          [ 0.0630, -0.0122, -0.0766,  0.0059,  0.0749],\n",
      "          [-0.0491,  0.0572,  0.0468, -0.0354,  0.0650],\n",
      "          [ 0.0097, -0.0169,  0.0763, -0.0787, -0.0217]]],\n",
      "\n",
      "\n",
      "        [[[-0.0155, -0.0105, -0.0378,  0.0285,  0.0630],\n",
      "          [ 0.0553,  0.0738, -0.0667,  0.0269, -0.0083],\n",
      "          [-0.0638,  0.0078, -0.0604, -0.0058, -0.0371],\n",
      "          [-0.0671,  0.0162,  0.0307,  0.0115, -0.0665],\n",
      "          [ 0.0050, -0.0533,  0.0304, -0.0159,  0.0375]],\n",
      "\n",
      "         [[-0.0152, -0.0425, -0.0457,  0.0439,  0.0207],\n",
      "          [-0.0777, -0.0289, -0.0738, -0.0333,  0.0697],\n",
      "          [-0.0534, -0.0096, -0.0672,  0.0157, -0.0692],\n",
      "          [-0.0037, -0.0005,  0.0431,  0.0718, -0.0778],\n",
      "          [-0.0727,  0.0430, -0.0419,  0.0756,  0.0128]],\n",
      "\n",
      "         [[-0.0387,  0.0253, -0.0259, -0.0781,  0.0289],\n",
      "          [ 0.0612, -0.0022, -0.0665,  0.0364,  0.0048],\n",
      "          [ 0.0302, -0.0157, -0.0515,  0.0417, -0.0143],\n",
      "          [-0.0466,  0.0157, -0.0198,  0.0359, -0.0018],\n",
      "          [ 0.0351,  0.0768,  0.0510, -0.0669,  0.0808]],\n",
      "\n",
      "         [[ 0.0039, -0.0663,  0.0393,  0.0306,  0.0126],\n",
      "          [ 0.0461,  0.0696, -0.0606,  0.0559, -0.0720],\n",
      "          [-0.0627,  0.0298,  0.0447, -0.0418, -0.0110],\n",
      "          [-0.0133,  0.0177,  0.0444, -0.0793,  0.0101],\n",
      "          [ 0.0659,  0.0142,  0.0060, -0.0724,  0.0151]],\n",
      "\n",
      "         [[-0.0476, -0.0677, -0.0119,  0.0358,  0.0488],\n",
      "          [-0.0147,  0.0555, -0.0789,  0.0075, -0.0724],\n",
      "          [-0.0686, -0.0564, -0.0225, -0.0560, -0.0804],\n",
      "          [-0.0455,  0.0779,  0.0413,  0.0654, -0.0634],\n",
      "          [-0.0719,  0.0627,  0.0765,  0.0399, -0.0656]],\n",
      "\n",
      "         [[-0.0025, -0.0693,  0.0753, -0.0747, -0.0788],\n",
      "          [ 0.0596,  0.0364, -0.0220,  0.0028, -0.0072],\n",
      "          [ 0.0744,  0.0475,  0.0091,  0.0224,  0.0184],\n",
      "          [ 0.0796, -0.0798,  0.0764,  0.0760,  0.0157],\n",
      "          [-0.0233,  0.0107, -0.0121,  0.0770,  0.0344]]]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0431,  0.0592, -0.0377, -0.0025, -0.0416, -0.0160,  0.0681, -0.0060,\n",
      "        -0.0100,  0.0042, -0.0280, -0.0762,  0.0007,  0.0756, -0.0039,  0.0358],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0036, -0.0217, -0.0047,  ...,  0.0058, -0.0377, -0.0229],\n",
      "        [-0.0110,  0.0497, -0.0177,  ...,  0.0426,  0.0180,  0.0032],\n",
      "        [-0.0132,  0.0498, -0.0263,  ..., -0.0248,  0.0037,  0.0332],\n",
      "        ...,\n",
      "        [-0.0390,  0.0143,  0.0418,  ..., -0.0256,  0.0040,  0.0281],\n",
      "        [-0.0154,  0.0230, -0.0264,  ..., -0.0491,  0.0110,  0.0104],\n",
      "        [ 0.0202, -0.0054,  0.0363,  ...,  0.0190, -0.0250, -0.0212]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0486, -0.0414,  0.0435,  0.0118,  0.0111, -0.0180, -0.0457, -0.0339,\n",
      "         0.0363,  0.0035, -0.0334, -0.0098,  0.0309, -0.0369,  0.0021,  0.0359,\n",
      "         0.0398,  0.0448, -0.0178, -0.0072, -0.0321, -0.0031, -0.0399, -0.0482,\n",
      "         0.0496, -0.0185,  0.0096,  0.0305,  0.0039, -0.0498,  0.0157,  0.0086,\n",
      "        -0.0269, -0.0035, -0.0306, -0.0146, -0.0499,  0.0381, -0.0349, -0.0220,\n",
      "         0.0413,  0.0281,  0.0073, -0.0238, -0.0298,  0.0407,  0.0142, -0.0455,\n",
      "        -0.0037, -0.0384,  0.0499,  0.0006,  0.0132,  0.0499, -0.0308, -0.0063,\n",
      "         0.0298,  0.0055, -0.0105, -0.0385, -0.0062, -0.0307, -0.0363,  0.0420,\n",
      "         0.0383,  0.0170,  0.0368,  0.0004, -0.0060,  0.0105, -0.0363,  0.0047,\n",
      "         0.0220,  0.0460,  0.0111,  0.0202,  0.0108, -0.0158, -0.0154,  0.0026,\n",
      "         0.0338,  0.0200,  0.0318,  0.0088,  0.0417,  0.0353, -0.0263, -0.0476,\n",
      "        -0.0251, -0.0475, -0.0287, -0.0450, -0.0105,  0.0496,  0.0328,  0.0078,\n",
      "         0.0142, -0.0174, -0.0388, -0.0303, -0.0206, -0.0454,  0.0078,  0.0237,\n",
      "         0.0080,  0.0230,  0.0398,  0.0494,  0.0350,  0.0060,  0.0022,  0.0366,\n",
      "        -0.0179,  0.0256, -0.0037,  0.0270,  0.0387, -0.0117, -0.0404,  0.0420],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0543,  0.0422,  0.0299,  ...,  0.0291, -0.0908,  0.0039],\n",
      "        [ 0.0149, -0.0364,  0.0476,  ..., -0.0431,  0.0426,  0.0354],\n",
      "        [ 0.0594, -0.0622, -0.0195,  ...,  0.0819,  0.0023, -0.0368],\n",
      "        ...,\n",
      "        [ 0.0588,  0.0435,  0.0282,  ..., -0.0768, -0.0258, -0.0793],\n",
      "        [-0.0445, -0.0795,  0.0496,  ...,  0.0382, -0.0788, -0.0212],\n",
      "        [-0.0790, -0.0695,  0.0638,  ..., -0.0676, -0.0274,  0.0767]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0054,  0.0781,  0.0870, -0.0162,  0.0813,  0.0306, -0.0181, -0.0716,\n",
      "        -0.0259, -0.0269,  0.0543,  0.0807,  0.0353, -0.0624, -0.0571,  0.0690,\n",
      "         0.0059,  0.0607,  0.0228, -0.0558, -0.0675, -0.0834,  0.0258, -0.0641,\n",
      "         0.0608, -0.0011,  0.0809,  0.0456, -0.0874,  0.0195,  0.0668, -0.0832,\n",
      "         0.0443,  0.0739,  0.0354,  0.0202, -0.0679, -0.0607,  0.0871, -0.0413,\n",
      "         0.0755, -0.0246, -0.0469,  0.0773, -0.0494, -0.0237,  0.0460, -0.0565,\n",
      "        -0.0490, -0.0301,  0.0541,  0.0010, -0.0459, -0.0748, -0.0182, -0.0269,\n",
      "        -0.0650,  0.0083, -0.0909,  0.0762,  0.0544,  0.0097,  0.0249, -0.0609,\n",
      "        -0.0870,  0.0126,  0.0221, -0.0230, -0.0753,  0.0251,  0.0439, -0.0080,\n",
      "         0.0012, -0.0661,  0.0407, -0.0309, -0.0047, -0.0572,  0.0292,  0.0456,\n",
      "         0.0169,  0.0520, -0.0646,  0.0542], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0990,  0.0130, -0.0432, -0.0482, -0.0141, -0.0670, -0.0272,  0.0143,\n",
      "         -0.0466, -0.0521,  0.0934,  0.0590,  0.0140,  0.0783,  0.0677,  0.0016,\n",
      "          0.0797, -0.1090,  0.0668, -0.0565,  0.0749,  0.0083,  0.0163,  0.0299,\n",
      "          0.0916, -0.0003, -0.0732,  0.0645,  0.0661,  0.0058,  0.0907,  0.0004,\n",
      "          0.0731,  0.0042,  0.0991, -0.1048,  0.0156, -0.0752, -0.0715,  0.0856,\n",
      "         -0.0947, -0.0817, -0.0430,  0.0649,  0.0129, -0.0543, -0.0943,  0.0142,\n",
      "         -0.0679, -0.0042, -0.0217, -0.0036,  0.0208,  0.0453, -0.0502,  0.0646,\n",
      "         -0.0901,  0.0157, -0.1028,  0.0575,  0.0996, -0.0824, -0.0175, -0.0462,\n",
      "          0.1078,  0.0827, -0.0477, -0.0907, -0.0764, -0.1042,  0.1040, -0.0121,\n",
      "          0.0294, -0.0693,  0.0631, -0.0888,  0.0036, -0.0509,  0.0014, -0.0724,\n",
      "          0.0325,  0.0050, -0.0873, -0.0369],\n",
      "        [-0.0526, -0.1028,  0.0796,  0.0637, -0.0497, -0.0708,  0.0293, -0.0946,\n",
      "          0.0471,  0.0349,  0.1054, -0.0836,  0.0275, -0.0552,  0.0779,  0.0154,\n",
      "         -0.0870, -0.0676, -0.0613, -0.0260, -0.0598,  0.0488,  0.0377,  0.0794,\n",
      "         -0.0102,  0.0764, -0.0579, -0.0674, -0.1088,  0.0358, -0.0624, -0.0111,\n",
      "         -0.0154, -0.0102, -0.0185, -0.0833, -0.1003, -0.0265,  0.0435, -0.0615,\n",
      "         -0.0006,  0.0009, -0.0741, -0.1003,  0.0901,  0.0670,  0.0375,  0.1035,\n",
      "          0.0483,  0.0971,  0.0226,  0.0135,  0.0499,  0.0903, -0.0934, -0.0919,\n",
      "         -0.0702,  0.1010,  0.0965,  0.0832, -0.0184, -0.1071,  0.0986, -0.0945,\n",
      "          0.0717,  0.0627, -0.0923, -0.0497, -0.0312, -0.0102,  0.1034,  0.0456,\n",
      "          0.0285, -0.0083,  0.0364,  0.1085,  0.0785,  0.0826,  0.0565,  0.0396,\n",
      "          0.0610,  0.0320,  0.0611, -0.0437],\n",
      "        [ 0.0865, -0.0158, -0.0731,  0.0248, -0.0452, -0.0547, -0.1075,  0.0907,\n",
      "          0.0681,  0.0295,  0.0645, -0.0004,  0.0020, -0.0919,  0.0169, -0.0485,\n",
      "          0.0735, -0.0712,  0.0107, -0.0036, -0.0241, -0.0381,  0.1015, -0.0660,\n",
      "         -0.0513,  0.0156,  0.0822, -0.0468,  0.0144, -0.0857,  0.0221,  0.0944,\n",
      "          0.0531, -0.0511, -0.0958,  0.0175,  0.0369,  0.0926, -0.0057, -0.0358,\n",
      "         -0.0067,  0.0101,  0.0498,  0.0461,  0.0484, -0.0344,  0.0779,  0.0112,\n",
      "          0.1077, -0.0997,  0.0802, -0.0094, -0.0884, -0.0933,  0.0299,  0.0283,\n",
      "         -0.0157,  0.1042, -0.0827,  0.0738,  0.0547,  0.0860,  0.0124,  0.0127,\n",
      "         -0.0113,  0.0124,  0.0117,  0.0701, -0.0578,  0.0319, -0.0991, -0.0133,\n",
      "         -0.0714,  0.0869,  0.0055,  0.0817,  0.0640, -0.0123,  0.0861, -0.0737,\n",
      "          0.0648,  0.1086,  0.0665, -0.0836],\n",
      "        [-0.0796, -0.0102,  0.0114, -0.0537,  0.0954,  0.0871,  0.0095, -0.0853,\n",
      "          0.0802, -0.1084, -0.0180,  0.0522,  0.0282, -0.0962, -0.0788, -0.0398,\n",
      "          0.0135,  0.0253,  0.0416, -0.0909, -0.0625,  0.0016, -0.0819,  0.0003,\n",
      "         -0.0822, -0.0415,  0.0574,  0.0849, -0.0389, -0.0309, -0.0754,  0.0702,\n",
      "         -0.0951, -0.0951,  0.0381,  0.0136, -0.1068,  0.0865, -0.0099, -0.0946,\n",
      "         -0.0825, -0.0258,  0.0324,  0.0146, -0.0223, -0.0408,  0.0881,  0.0531,\n",
      "          0.0711,  0.0742, -0.0371, -0.0386, -0.0648, -0.0494,  0.0745, -0.0552,\n",
      "          0.0054,  0.0276, -0.0949, -0.1030,  0.0037, -0.0955, -0.1049, -0.0027,\n",
      "          0.0156,  0.0172, -0.0833, -0.0817,  0.0665, -0.0026, -0.0383, -0.0856,\n",
      "          0.1027, -0.0226,  0.0711, -0.0122,  0.0892, -0.0670, -0.1064, -0.0668,\n",
      "         -0.0669,  0.0640,  0.0412, -0.0696],\n",
      "        [-0.0705, -0.0256,  0.0048,  0.0744, -0.0385, -0.0531,  0.0698,  0.0859,\n",
      "         -0.0714, -0.0297,  0.0115, -0.0424,  0.0354, -0.0275,  0.0086,  0.0986,\n",
      "          0.0409,  0.0215, -0.0743,  0.0517, -0.0770,  0.0390, -0.1040,  0.0766,\n",
      "         -0.0330,  0.0179,  0.0009, -0.0740,  0.1048, -0.1087, -0.0317, -0.0949,\n",
      "          0.0656,  0.0602,  0.0937,  0.0963, -0.0558,  0.0688,  0.0430, -0.0143,\n",
      "         -0.0899, -0.0311, -0.0466,  0.0374,  0.0923, -0.0895, -0.0774,  0.0559,\n",
      "         -0.0739,  0.0215,  0.0514,  0.0544,  0.0252, -0.0015, -0.0609, -0.0820,\n",
      "         -0.0063, -0.0026,  0.1033, -0.0480,  0.0109,  0.0288,  0.0269, -0.0867,\n",
      "         -0.0860,  0.1043, -0.0658,  0.0956,  0.0457,  0.0150,  0.0524,  0.0897,\n",
      "          0.0293, -0.0161, -0.0281,  0.0984, -0.0889,  0.1083, -0.0905, -0.0672,\n",
      "         -0.0909,  0.0541, -0.0247, -0.0489],\n",
      "        [ 0.0806,  0.0643,  0.0344, -0.0181, -0.0663, -0.0166, -0.0418, -0.0544,\n",
      "         -0.0466, -0.0326,  0.0601, -0.0276, -0.0193,  0.0228, -0.0910, -0.0593,\n",
      "         -0.0649,  0.0372, -0.0226, -0.1060,  0.0154,  0.0922, -0.0250, -0.0294,\n",
      "          0.0238, -0.0054,  0.0824,  0.0880, -0.0209, -0.0408, -0.0910,  0.0718,\n",
      "          0.0796,  0.0356, -0.0984,  0.0673, -0.0280,  0.0837,  0.0176,  0.0489,\n",
      "          0.0290,  0.0187, -0.0846, -0.0045,  0.1070,  0.0141,  0.0765, -0.0646,\n",
      "         -0.1064,  0.0608, -0.0489,  0.0103,  0.0606, -0.0288,  0.0740, -0.0136,\n",
      "          0.0821,  0.0243,  0.0251,  0.0430, -0.0300,  0.0797,  0.0832,  0.0898,\n",
      "          0.1084,  0.0497, -0.0029, -0.0399, -0.0522, -0.0686, -0.0508, -0.0442,\n",
      "         -0.0934,  0.0132, -0.0716, -0.0398,  0.0959, -0.1082,  0.0790, -0.0799,\n",
      "         -0.0834,  0.0021,  0.0454,  0.0419],\n",
      "        [-0.0316, -0.0982, -0.0488, -0.0067,  0.0641, -0.1020, -0.0922,  0.0317,\n",
      "          0.0326, -0.0905, -0.0586, -0.0869,  0.0817, -0.0890, -0.0141,  0.0689,\n",
      "         -0.0450,  0.0359,  0.0393,  0.0990, -0.0859,  0.0806, -0.0343,  0.0502,\n",
      "          0.0841,  0.0145,  0.0843,  0.0224, -0.0673, -0.1089,  0.0232, -0.0351,\n",
      "          0.0288,  0.0244,  0.0895, -0.1053,  0.0505, -0.1006,  0.0223, -0.0949,\n",
      "         -0.0090,  0.0601, -0.0003,  0.0062, -0.0686, -0.0760,  0.0266,  0.0549,\n",
      "         -0.0510, -0.0881, -0.0684, -0.0894, -0.0192, -0.0110,  0.0128, -0.0966,\n",
      "          0.0735,  0.0173,  0.0760,  0.0709, -0.0022, -0.0755,  0.0976,  0.0882,\n",
      "         -0.0482, -0.0571, -0.0681,  0.0864, -0.0844, -0.0328, -0.0244, -0.0778,\n",
      "         -0.0828,  0.0979,  0.0748,  0.0934, -0.0215,  0.0816,  0.0635, -0.0818,\n",
      "         -0.0054, -0.0027, -0.0111, -0.0237],\n",
      "        [-0.0749, -0.0592, -0.0130,  0.0038,  0.0637, -0.0804,  0.0559,  0.0604,\n",
      "         -0.0246,  0.0119, -0.0177,  0.0031,  0.0165,  0.1072,  0.0581, -0.0081,\n",
      "          0.0664, -0.0262, -0.0951,  0.0330,  0.0024,  0.0904, -0.0010,  0.0047,\n",
      "          0.0078, -0.0317, -0.0954, -0.0201, -0.1022, -0.0566,  0.0347, -0.0203,\n",
      "         -0.0559, -0.0166,  0.0674,  0.0593,  0.0874,  0.0231,  0.0549,  0.0935,\n",
      "          0.0979,  0.0989, -0.0251, -0.0570,  0.1036,  0.0630, -0.0068, -0.0941,\n",
      "         -0.0493, -0.1059, -0.0868, -0.0885, -0.0201,  0.0559, -0.0934, -0.0530,\n",
      "         -0.0954,  0.0679, -0.0669,  0.0654,  0.0741,  0.0356,  0.0189,  0.0193,\n",
      "          0.0119,  0.0213,  0.0706,  0.0395, -0.1069, -0.0548, -0.0921, -0.0971,\n",
      "          0.0201,  0.1048,  0.0283, -0.0368, -0.0651,  0.0481,  0.0141, -0.0042,\n",
      "          0.0890,  0.0363, -0.0225, -0.0127],\n",
      "        [-0.0498,  0.0437,  0.0529, -0.0668,  0.0199, -0.0389, -0.0204, -0.0234,\n",
      "          0.0032,  0.0841, -0.0381,  0.0993,  0.0768, -0.0106, -0.0237, -0.0965,\n",
      "          0.0215,  0.0139,  0.0808,  0.0371, -0.0156,  0.0995,  0.0904,  0.0472,\n",
      "         -0.0646,  0.0656, -0.0973,  0.1010,  0.0446,  0.0257, -0.0593, -0.0467,\n",
      "          0.0217, -0.0846,  0.0578,  0.0737,  0.0106, -0.0136,  0.0308, -0.0729,\n",
      "          0.0262,  0.0733, -0.0250, -0.0009, -0.0824,  0.1022,  0.1057, -0.0540,\n",
      "          0.0852, -0.0924,  0.0584, -0.0775, -0.0829,  0.0512, -0.1028, -0.0429,\n",
      "          0.0121, -0.0023, -0.0236,  0.0462,  0.0119, -0.0818, -0.0408,  0.0430,\n",
      "         -0.0310, -0.0760, -0.0332,  0.0094, -0.0087,  0.0960,  0.0615,  0.0245,\n",
      "         -0.0782,  0.0818, -0.0371,  0.0111, -0.0048, -0.0669, -0.0934, -0.1037,\n",
      "         -0.0038, -0.0503,  0.0837,  0.0809],\n",
      "        [-0.1072, -0.0061, -0.0473,  0.0847, -0.0108,  0.0192, -0.0609,  0.0471,\n",
      "         -0.0349, -0.0257,  0.0195, -0.1028,  0.0491,  0.0391,  0.0893, -0.1084,\n",
      "          0.0901, -0.0223, -0.0357, -0.0902, -0.0022, -0.0383,  0.0187, -0.0031,\n",
      "         -0.0070, -0.0539, -0.0660, -0.0248,  0.0522,  0.0784, -0.0959, -0.0863,\n",
      "          0.0533,  0.0763,  0.0253,  0.0789,  0.0885,  0.0060,  0.0952, -0.0459,\n",
      "         -0.0198,  0.0683,  0.0076,  0.0235, -0.0199, -0.0328, -0.0546, -0.0368,\n",
      "         -0.0053,  0.0192, -0.0075,  0.0052, -0.0170,  0.1033,  0.1031, -0.0034,\n",
      "         -0.0615,  0.0398, -0.0414, -0.0743, -0.0693, -0.0245, -0.0130,  0.0094,\n",
      "         -0.0364,  0.1014,  0.0063,  0.0843,  0.0299,  0.0349,  0.1043,  0.0553,\n",
      "          0.0444, -0.0808, -0.0846, -0.0571,  0.0198, -0.0733, -0.0027,  0.0747,\n",
      "         -0.0047,  0.0579,  0.0524,  0.1003]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0740,  0.0635,  0.1077, -0.0462, -0.0459, -0.0768,  0.0636,  0.0623,\n",
      "        -0.0041, -0.0742], requires_grad=True)]\n",
      "[('', LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")), ('conv1', Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))), ('conv2', Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))), ('fc1', Linear(in_features=400, out_features=120, bias=True)), ('fc2', Linear(in_features=120, out_features=84, bias=True)), ('fc3', Linear(in_features=84, out_features=10, bias=True))]\n",
      "[LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "), Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)), Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)), Linear(in_features=400, out_features=120, bias=True), Linear(in_features=120, out_features=84, bias=True), Linear(in_features=84, out_features=10, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "print(list(model.named_parameters()))\n",
    "print(list(model.parameters()))\n",
    "print(list(model.named_modules()))\n",
    "print(list(model.modules()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b6e939",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Create an instance of the class below and print its modules and parameters. What do we observe? Fix this issue by using the object `torch.nn.ModuleList`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b6a3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SomeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SomeModel, self).__init__()\n",
    "\n",
    "        self.layers = [torch.nn.Linear(5, 5),\n",
    "                      torch.nn.ReLU(),\n",
    "                      torch.nn.Linear(5, 1)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b61bdcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[('', SomeModel())]\n",
      "[SomeModel()]\n"
     ]
    }
   ],
   "source": [
    "some_model = SomeModel()\n",
    "print(list(some_model.named_parameters()))\n",
    "print(list(some_model.parameters()))\n",
    "print(list(some_model.named_modules()))\n",
    "print(list(some_model.modules()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "130444d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SomeModelFixed(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SomeModelFixed, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([torch.nn.Linear(5, 5),\n",
    "                      torch.nn.ReLU(),\n",
    "                      torch.nn.Linear(5, 1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89d53d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('layers.0.weight', Parameter containing:\n",
      "tensor([[-0.3480, -0.4076, -0.4393, -0.2295, -0.4308],\n",
      "        [-0.4243,  0.2087, -0.0594, -0.2793, -0.2291],\n",
      "        [ 0.4358,  0.3769,  0.0044, -0.3196, -0.1559],\n",
      "        [-0.3162,  0.2490,  0.2747, -0.2072, -0.3103],\n",
      "        [-0.2905, -0.1874,  0.4316, -0.0739,  0.2940]], requires_grad=True)), ('layers.0.bias', Parameter containing:\n",
      "tensor([ 0.2284, -0.2385,  0.2326, -0.0547, -0.1750], requires_grad=True)), ('layers.2.weight', Parameter containing:\n",
      "tensor([[-0.3083,  0.1192, -0.3707, -0.3792, -0.3933]], requires_grad=True)), ('layers.2.bias', Parameter containing:\n",
      "tensor([-0.4044], requires_grad=True))]\n",
      "[Parameter containing:\n",
      "tensor([[-0.3480, -0.4076, -0.4393, -0.2295, -0.4308],\n",
      "        [-0.4243,  0.2087, -0.0594, -0.2793, -0.2291],\n",
      "        [ 0.4358,  0.3769,  0.0044, -0.3196, -0.1559],\n",
      "        [-0.3162,  0.2490,  0.2747, -0.2072, -0.3103],\n",
      "        [-0.2905, -0.1874,  0.4316, -0.0739,  0.2940]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2284, -0.2385,  0.2326, -0.0547, -0.1750], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.3083,  0.1192, -0.3707, -0.3792, -0.3933]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4044], requires_grad=True)]\n",
      "[('', SomeModelFixed(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=1, bias=True)\n",
      "  )\n",
      ")), ('layers', ModuleList(\n",
      "  (0): Linear(in_features=5, out_features=5, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=5, out_features=1, bias=True)\n",
      ")), ('layers.0', Linear(in_features=5, out_features=5, bias=True)), ('layers.1', ReLU()), ('layers.2', Linear(in_features=5, out_features=1, bias=True))]\n",
      "[SomeModelFixed(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=1, bias=True)\n",
      "  )\n",
      "), ModuleList(\n",
      "  (0): Linear(in_features=5, out_features=5, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=5, out_features=1, bias=True)\n",
      "), Linear(in_features=5, out_features=5, bias=True), ReLU(), Linear(in_features=5, out_features=1, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "some_model = SomeModelFixed()\n",
    "print(list(some_model.named_parameters()))\n",
    "print(list(some_model.parameters()))\n",
    "print(list(some_model.named_modules()))\n",
    "print(list(some_model.modules()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce5895",
   "metadata": {},
   "source": [
    "**Question 3 (optional)**\n",
    "\n",
    "Check out the method `register_buffer` of `Module`, explain why it can be useful and show a use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ed12bf",
   "metadata": {},
   "source": [
    "# Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b782325",
   "metadata": {},
   "source": [
    "## Basic training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe731fa",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "We want to train LeNet on MNIST. Fill the blanks in the following pieces of code.\n",
    "\n",
    "Wrap the training process into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01889385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, nepochs):\n",
    "    #List to store loss to visualize\n",
    "    valid_loss_min = np.inf # track change in validation loss\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    acc_eval = []\n",
    "    #test_counter = [i*len(train_loader.dataset) for i in n_epochs]\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device = device)\n",
    "            print(data.size())\n",
    "            target = target.to(device = device)\n",
    "            break\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            with torch.no_grad():\n",
    "                data = data.to(device = device)\n",
    "                target = target.to(device = device)\n",
    "\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                output = model(data)\n",
    "\n",
    "                # calculate the batch loss\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                # update average validation loss \n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(test_loader.dataset)\n",
    "        acc_eval.append(correct/len(test_loader.dataset)*100)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(valid_loss)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f172534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "Epoch: 0 \tTraining Loss: 0.000000 \tValidation Loss: 2.304021\n",
      "torch.Size([64, 1, 28, 28])\n",
      "Epoch: 1 \tTraining Loss: 0.000000 \tValidation Loss: 2.304021\n",
      "torch.Size([64, 1, 28, 28])\n",
      "Epoch: 2 \tTraining Loss: 0.000000 \tValidation Loss: 2.304021\n",
      "torch.Size([64, 1, 28, 28])\n",
      "Epoch: 3 \tTraining Loss: 0.000000 \tValidation Loss: 2.304021\n",
      "torch.Size([64, 1, 28, 28])\n",
      "Epoch: 4 \tTraining Loss: 0.000000 \tValidation Loss: 2.304021\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = LeNet().to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = .01)\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4783bc6-83a3-4497-84e6-bee14176693d",
   "metadata": {},
   "source": [
    "## Saving and loading a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cbafc2-e19e-45a8-b597-a14223a3a228",
   "metadata": {},
   "source": [
    "When training a model, it is a good practice to make checkpoints every few epochs. That is, store the current state of the model **and the optimizer**. Doing so, we are able to reload the current state of a training process if:\n",
    " * the training has been interrupted for an unknown reason (which occurs when using a cluster);\n",
    " * we want to diagnose an issue with the model at an early stage (drop of performance, instabilities of the loss, etc.).\n",
    "\n",
    "It is essential to save the current state of the optimizer, since it contains frequently information acquired during the early stages of training (momentum with SGD + momentum, running means and moments with Adam).\n",
    "\n",
    "Above all, we define again a model to train and a training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e065a-83e8-49c7-8d47-16493a1af501",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 79 (3284574207.py, line 86)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'if' statement on line 79\n"
     ]
    }
   ],
   "source": [
    "# Initial version of train_model\n",
    "\n",
    "def train_model(model, criterion, optimizer, nepochs, save_path = None, load_path = None):\n",
    "    #List to store loss to visualize\n",
    "    valid_loss_min = np.inf # track change in validation loss\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    acc_eval = []\n",
    "    #test_counter = [i*len(train_loader.dataset) for i in n_epochs]\n",
    "\n",
    "    if load_path is not None:\n",
    "        checkpoint = torch.load(load_path, weights_only = False)\n",
    "        # load the model\n",
    "        # ...\n",
    "        # ...\n",
    "        # ...\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device = device)\n",
    "            target = target.to(device = device)\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            with torch.no_grad():\n",
    "                data = data.to(device = device)\n",
    "                target = target.to(device = device)\n",
    "\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                output = model(data)\n",
    "\n",
    "                # calculate the batch loss\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                # update average validation loss \n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(test_loader.dataset)\n",
    "        acc_eval.append(correct/len(test_loader.dataset)*100)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(valid_loss)\n",
    "\n",
    "        # Save\n",
    "        #if save_path is not None:\n",
    "            # Save the model and other useful variables\n",
    "            # torch.save(... \n",
    "            #            ...\n",
    "            #            ..., save_path)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a42d2-505d-4216-9eac-12febefa4b6c",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Add lines of code to the function `train_model` to store both the model and the optimizer. One can use `torch.save` and the methods `state_dict` of `Module` and `Optimizer`. Note: one can use `torch.save` to save any PyTorch object **and** any native Python object, such as `dict`.\n",
    "\n",
    "Add options to `train_model` to resume training from a specific checkpoint.\n",
    "\n",
    "Launch a training with Adam, stop it, and then resume it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "640f166d-4326-4a5d-9c60-84101aaf361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, nepochs, save_path = None,\n",
    "               load_path = None):\n",
    "    #List to store loss to visualize\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    acc_eval = []\n",
    "    start_epoch = 0\n",
    "    \n",
    "    if load_path is not None:\n",
    "        checkpoint = torch.load(load_path, weights_only = False)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        acc_eval = checkpoint[\"acc_eval\"]\n",
    "        train_losses = checkpoint[\"train_losses\"]\n",
    "        test_losses = checkpoint[\"test_losses\"]\n",
    "\n",
    "    for epoch in range(start_epoch, nepochs):\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device = device)\n",
    "            target = target.to(device = device)\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            with torch.no_grad():\n",
    "                data = data.to(device = device)\n",
    "                target = target.to(device = device)\n",
    "\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                output = model(data)\n",
    "\n",
    "                # calculate the batch loss\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                # update average validation loss \n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(test_loader.dataset)\n",
    "        acc_eval.append(correct/len(test_loader.dataset)*100)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(valid_loss)\n",
    "        \n",
    "        # Save\n",
    "        if save_path is not None:\n",
    "            torch.save({\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"acc_eval\": acc_eval,\n",
    "                \"train_losses\": train_losses,\n",
    "                \"test_losses\": test_losses\n",
    "            }, save_path)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b6bb975-029d-4291-a227-18603f6260ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 0.238965 \tValidation Loss: 0.072114\n",
      "Epoch: 1 \tTraining Loss: 0.064469 \tValidation Loss: 0.061626\n",
      "Epoch: 2 \tTraining Loss: 0.043855 \tValidation Loss: 0.048788\n",
      "Epoch: 3 \tTraining Loss: 0.031895 \tValidation Loss: 0.045996\n",
      "Epoch: 4 \tTraining Loss: 0.023832 \tValidation Loss: 0.047996\n",
      "Epoch: 5 \tTraining Loss: 0.019301 \tValidation Loss: 0.054660\n",
      "Epoch: 6 \tTraining Loss: 0.014384 \tValidation Loss: 0.052934\n",
      "Epoch: 7 \tTraining Loss: 0.012474 \tValidation Loss: 0.053506\n",
      "Epoch: 8 \tTraining Loss: 0.011132 \tValidation Loss: 0.049493\n",
      "Epoch: 9 \tTraining Loss: 0.009348 \tValidation Loss: 0.057679\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = LeNet().to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "nepochs = 10\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs, save_path = \"checkpoint.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e09afa",
   "metadata": {},
   "source": [
    "## Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4420490",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "To make sure that the inputs of the neural network are within a controlled range, we usually transform the dataset to be sure that the data are centered with variance 1. It is not always necessary, but it is worth knowing it.\n",
    "\n",
    "Check the range of values on a sample of MNIST. Compute the mean and the standard deviation of the training dataset of MNIST and normalize the dataset accordingly by using `transforms.Normalize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af99f662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1307) tensor(0.3081)\n"
     ]
    }
   ],
   "source": [
    "mean = 0\n",
    "\n",
    "total_sum = 0\n",
    "for data, target in train_loader:\n",
    "    total_sum += data.sum()\n",
    "mean = total_sum / (28**2 * len(train_loader.dataset))\n",
    "\n",
    "total_var = 0\n",
    "for data, target in train_loader:\n",
    "    total_var += (data - mean).pow(2).sum()\n",
    "std = (total_var / (28**2 * len(train_loader.dataset))).sqrt()\n",
    "\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61f0c9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# build transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "    ]) \n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(datasets_path, train = True,\n",
    "                              download = True, transform = transform)\n",
    "test_data = datasets.MNIST(datasets_path, train=False,\n",
    "                             download = True, transform = transform)\n",
    "\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "# build the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# specify the image classes\n",
    "classes = [f\"{i}\" for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aa70c8",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41a7be9",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "It is very common to face overfitting when doing deep learning. So, several methods can be used to solve this problem. One of them is called \"data augmentation\". It consists in adding \"noise\" to data points of the training dataset in order to make the model resistant to small changes of the data. \n",
    "\n",
    "When training on images, it is common to perform \"random crops\", \"random flips\", and small \"random rotations\". With MNIST, it is meaningless to add random flips, because most digits are not supposed to be invariant by vertical or horizontal symmetries.\n",
    "\n",
    "Add random crops with `transforms.RandomCrop` with a reasonable number of pixels to the transforms to do on the dataset, visualize the resulting images and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eca736ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# build transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "    transforms.RandomCrop(28, padding = 3)\n",
    "    ]) \n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(datasets_path, train = True,\n",
    "                              download = True, transform = transform)\n",
    "test_data = datasets.MNIST(datasets_path, train=False,\n",
    "                             download = True, transform = transform)\n",
    "\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "# build the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# specify the image classes\n",
    "classes = [f\"{i}\" for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e66bfb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x350 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lst_idx = np.random.choice(64, 10, replace = False)\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize = (7.5, 3.5))\n",
    "\n",
    "for data, target in train_loader:\n",
    "    for i, idx in enumerate(lst_idx):\n",
    "        img, tgt = data[idx], target[idx]\n",
    "        ax[i//5, i%5].imshow(img.numpy().transpose(1, 2, 0), cmap=\"Greys\")\n",
    "        ax[i//5, i%5].set_title(classes[tgt])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68234341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 1.845252 \tValidation Loss: 1.302585\n",
      "Epoch: 1 \tTraining Loss: 1.036268 \tValidation Loss: 0.737955\n",
      "Epoch: 2 \tTraining Loss: 0.550661 \tValidation Loss: 0.396217\n",
      "Epoch: 3 \tTraining Loss: 0.347517 \tValidation Loss: 0.277226\n",
      "Epoch: 4 \tTraining Loss: 0.259967 \tValidation Loss: 0.220671\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = LeNet().to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = .01)\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc16b6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8a8b9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 400])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc885868",
   "metadata": {},
   "source": [
    "## Influence of the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa18b085",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "Build a Multilayer Perceptron with ReLU activation functions, which takes 3 arguments: \n",
    " * `layers`: the list of layer sizes;\n",
    " * `sigma_w`: the standard deviation chosen for initializing the weights;\n",
    " * `with_scaling`: if True, we multiply the generated weights by $1/\\sqrt{\\# \\text{inputs}}$.\n",
    "\n",
    "Write the `reset_parameters` method, which initialize the weights according to a Gaussian distribution, either with variance $\\sigma_w^2$, or $\\sigma_w^2/\\# \\text{inputs}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0fad359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, layers, sigma_w, scaling = False):\n",
    "        super(Perceptron, self).__init__()\n",
    "\n",
    "        self.act_function = torch.relu\n",
    "        self.scaling = scaling\n",
    "        self.sigma_w = sigma_w\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for l_in, l_out in zip(layers[:-1], layers[1:]):\n",
    "            self.layers.append(nn.Linear(l_in, l_out))\n",
    "        self.nb_layers = len(self.layers)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        with torch.no_grad():\n",
    "            for i, l in enumerate(self.layers):\n",
    "                l.weight.data.normal_()\n",
    "                l.weight.data.mul_(self.sigma_w)\n",
    "                \n",
    "                if self.scaling:\n",
    "                    l.weight.data.div_(np.sqrt(l.weight.size(1)))\n",
    "                    \n",
    "                l.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for l in self.layers[:-1]:\n",
    "            x = l(x)\n",
    "            x = self.act_function(x)\n",
    "                                       \n",
    "        x = self.layers[-1](x)\n",
    "        x = torch.nn.functional.log_softmax(x, dim = 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1396cf",
   "metadata": {},
   "source": [
    "**Question 6**\n",
    "\n",
    "Train the model with various choices of initialization (which ones?) and try various numbers of layers with various widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84868847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 2.301863 \tValidation Loss: 2.301313\n",
      "Epoch: 1 \tTraining Loss: 2.301321 \tValidation Loss: 2.301091\n",
      "Epoch: 2 \tTraining Loss: 2.301232 \tValidation Loss: 2.301045\n",
      "Epoch: 3 \tTraining Loss: 2.301216 \tValidation Loss: 2.301033\n",
      "Epoch: 4 \tTraining Loss: 2.301213 \tValidation Loss: 2.301030\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Perceptron([28**2, 100, 50, 10], 0).to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = .01)\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d4e018b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 1.135886 \tValidation Loss: 0.686852\n",
      "Epoch: 1 \tTraining Loss: 0.569070 \tValidation Loss: 0.466009\n",
      "Epoch: 2 \tTraining Loss: 0.417303 \tValidation Loss: 0.364266\n",
      "Epoch: 3 \tTraining Loss: 0.350832 \tValidation Loss: 0.307382\n",
      "Epoch: 4 \tTraining Loss: 0.309635 \tValidation Loss: 0.289449\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Perceptron([28**2, 100, 50, 10], np.sqrt(2), True).to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = .01)\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "310c2676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 28.882729 \tValidation Loss: 2.824367\n",
      "Epoch: 1 \tTraining Loss: 2.675157 \tValidation Loss: 2.521701\n",
      "Epoch: 2 \tTraining Loss: 2.451378 \tValidation Loss: 2.489884\n",
      "Epoch: 3 \tTraining Loss: 2.439415 \tValidation Loss: 2.391674\n",
      "Epoch: 4 \tTraining Loss: 2.357333 \tValidation Loss: 2.399544\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Perceptron([28**2, 100, 50, 10], np.sqrt(2), False).to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = .01)\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8cb9c4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 2.188669 \tValidation Loss: 1.941735\n",
      "Epoch: 1 \tTraining Loss: 1.513406 \tValidation Loss: 1.184177\n",
      "Epoch: 2 \tTraining Loss: 0.966840 \tValidation Loss: 0.811256\n",
      "Epoch: 3 \tTraining Loss: 0.713968 \tValidation Loss: 0.626425\n",
      "Epoch: 4 \tTraining Loss: 0.576745 \tValidation Loss: 0.558349\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Perceptron([28**2, 100, 100, 10, 100, 50, 10], .1, False).to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = .01)\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38414621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 1.427609 \tValidation Loss: 0.777761\n",
      "Epoch: 1 \tTraining Loss: 0.600385 \tValidation Loss: 0.490437\n",
      "Epoch: 2 \tTraining Loss: 0.434239 \tValidation Loss: 0.384619\n",
      "Epoch: 3 \tTraining Loss: 0.355170 \tValidation Loss: 0.364568\n",
      "Epoch: 4 \tTraining Loss: 0.311441 \tValidation Loss: 0.311846\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Perceptron([28**2, 100, 100, 10, 100, 50, 10], 1.4, True).to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = .01)\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39428313",
   "metadata": {},
   "source": [
    "**Question 7 (optional)**\n",
    "\n",
    "Implement the NTK parameterization in the `Perceptron` model: divide by $1/\\sqrt{\\# \\text{inputs}}$ the result of each layer.\n",
    "\n",
    "Train such a network with the SGD (with `scaling = False`) with learning rates of order $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3d4e1",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa7de74",
   "metadata": {},
   "source": [
    "**Question 8**\n",
    "\n",
    "Train LeNet with the SGD and Adam with default parameters and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c63435e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 0.397324 \tValidation Loss: 0.155435\n",
      "Epoch: 1 \tTraining Loss: 0.121312 \tValidation Loss: 0.098655\n",
      "Epoch: 2 \tTraining Loss: 0.094322 \tValidation Loss: 0.079411\n",
      "Epoch: 3 \tTraining Loss: 0.083575 \tValidation Loss: 0.078121\n",
      "Epoch: 4 \tTraining Loss: 0.075009 \tValidation Loss: 0.067071\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = LeNet().to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c86e80b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 1.871598 \tValidation Loss: 1.298234\n",
      "Epoch: 1 \tTraining Loss: 0.945447 \tValidation Loss: 0.632537\n",
      "Epoch: 2 \tTraining Loss: 0.483287 \tValidation Loss: 0.374193\n",
      "Epoch: 3 \tTraining Loss: 0.322417 \tValidation Loss: 0.263256\n",
      "Epoch: 4 \tTraining Loss: 0.252259 \tValidation Loss: 0.215762\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = LeNet().to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = .01)\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a4f46",
   "metadata": {},
   "source": [
    "## Other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6a816",
   "metadata": {},
   "source": [
    "**Additional questions**\n",
    "\n",
    " * explore data augmentation with the dataset CIFAR-10\n",
    " * test different batch sizes. How to change the learning rate when we change the batch size?\n",
    " * test drop-out layers\n",
    " * read the documentation on SGD and Adam an propose a way to assign different learning rates to different sets of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585527f",
   "metadata": {},
   "source": [
    "# Forward/backward hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ad7b5",
   "metadata": {},
   "source": [
    "To help debugging, it is possible to use forward and backward hooks. They are functions that the user can define and register into a module in order to call them during the forward/backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b5ec77",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Use the method `register_forward_hook` on a submodule (for instance `fc3`) of `model` to print the input and the output of the layer at each pass.\n",
    "\n",
    "For the backward, use `register_full_backward_hook` to print the gradients according to the output and to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a1034933",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet().to(device = device)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df80ae00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x71523c09e180>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hook(module, input, output):\n",
    "    print(\"=== FORWARD ===\")\n",
    "    print(\"module:\", module)\n",
    "    print(\"input:\", input)\n",
    "    print(\"output:\", output)\n",
    "\n",
    "model.fc3.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9359b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FORWARD ===\n",
      "module: Linear(in_features=84, out_features=10, bias=True)\n",
      "input: (tensor([[ 0.1480, -0.0148,  0.0545,  ...,  0.2864, -0.0673, -0.0177],\n",
      "        [ 0.0926, -0.1170,  0.0310,  ...,  0.2038, -0.0846,  0.0268],\n",
      "        [ 0.0844,  0.0043, -0.0059,  ...,  0.1417, -0.0768,  0.0069],\n",
      "        ...,\n",
      "        [ 0.1168, -0.0919,  0.0311,  ...,  0.2726, -0.0480,  0.0030],\n",
      "        [ 0.0943, -0.0383, -0.0171,  ...,  0.1983, -0.0479, -0.0196],\n",
      "        [ 0.0794, -0.0517,  0.0125,  ...,  0.1614,  0.0144, -0.0123]],\n",
      "       device='cuda:0'),)\n",
      "output: tensor([[-4.6495e-02, -3.3797e-02,  1.1431e-01, -2.7589e-02, -1.6828e-01,\n",
      "          1.0346e-01,  6.3908e-03, -3.9804e-02, -1.4590e-01,  2.5746e-03],\n",
      "        [-5.6215e-02, -3.1727e-02,  4.3208e-02, -1.7750e-02, -1.6603e-01,\n",
      "          4.4770e-02, -1.9576e-02, -1.1434e-02, -7.4667e-02,  4.6413e-02],\n",
      "        [ 2.7413e-03, -2.2309e-02,  7.2945e-03, -2.6348e-02, -1.5573e-01,\n",
      "          7.8673e-02,  5.6941e-03,  1.7133e-03, -9.6948e-02,  9.9543e-03],\n",
      "        [-7.3434e-02, -8.2401e-02,  2.4310e-02, -1.1355e-01, -2.1614e-01,\n",
      "          6.4183e-02,  1.0879e-02, -9.9419e-02, -1.4913e-01,  2.8941e-03],\n",
      "        [-4.2098e-02, -3.1737e-02,  4.7134e-02,  1.4548e-02, -1.4293e-01,\n",
      "          7.8900e-02,  1.3550e-02,  1.7892e-02, -1.4075e-01,  1.3050e-02],\n",
      "        [-5.5954e-03,  2.5147e-02,  4.4306e-02, -4.9126e-03, -1.6796e-01,\n",
      "          1.3024e-01, -3.0166e-03,  9.6956e-03, -1.4979e-01,  6.1061e-02],\n",
      "        [-5.1632e-02, -3.2037e-02,  9.4652e-02, -1.6982e-02, -1.9739e-01,\n",
      "          4.0957e-02, -2.6580e-04, -4.5391e-03, -1.0055e-01,  4.0826e-02],\n",
      "        [-1.2030e-02, -5.1114e-03, -3.4238e-02, -1.3401e-02, -1.5496e-01,\n",
      "          5.4760e-02, -1.5436e-02, -3.8915e-03, -1.2400e-01,  2.5640e-02],\n",
      "        [-6.2475e-02, -1.1287e-02,  5.4774e-02,  6.8038e-04, -2.0023e-01,\n",
      "          4.4791e-02,  4.5986e-02,  1.7976e-02, -1.2463e-01,  5.4596e-02],\n",
      "        [ 2.1601e-02,  8.1941e-04,  7.5459e-02, -1.4934e-02, -2.1769e-01,\n",
      "          8.0546e-02,  2.0639e-02, -4.2448e-02, -1.9833e-01,  5.8024e-02],\n",
      "        [ 7.6036e-03, -5.9699e-02,  8.3471e-02, -1.3093e-02, -1.6460e-01,\n",
      "          1.7049e-02,  6.3817e-02, -8.8157e-02, -1.4784e-01, -2.2462e-02],\n",
      "        [-2.2562e-02, -8.9482e-02,  5.2754e-02, -2.9250e-02, -1.4720e-01,\n",
      "          5.9537e-04, -3.6667e-03, -1.9010e-02, -1.2424e-01,  5.2131e-03],\n",
      "        [-6.1275e-03, -2.1363e-02,  5.8561e-02,  9.4229e-03, -1.9342e-01,\n",
      "          5.8584e-02, -4.5664e-02, -5.6182e-03, -1.4094e-01,  4.0037e-02],\n",
      "        [-2.4579e-02, -1.2607e-02,  4.8331e-02,  1.4479e-03, -1.4827e-01,\n",
      "          7.7033e-02,  5.4932e-02, -6.8992e-02, -1.0515e-01,  2.9170e-02],\n",
      "        [-4.7234e-03, -8.8996e-03,  5.9724e-02, -3.8149e-02, -1.3237e-01,\n",
      "          1.4854e-01, -9.8953e-04, -1.1267e-02, -1.2462e-01,  4.1482e-02],\n",
      "        [-3.2008e-02, -9.8990e-03,  1.8632e-02,  8.5469e-03, -1.6159e-01,\n",
      "          7.4601e-02,  3.6769e-02,  1.3524e-02, -1.2245e-01,  6.1996e-03],\n",
      "        [-2.9094e-02, -6.8876e-02,  1.0408e-01, -6.5682e-02, -1.9742e-01,\n",
      "          2.5020e-02,  1.5484e-02, -1.9976e-02, -1.2755e-01, -6.0469e-03],\n",
      "        [-6.4975e-02,  7.9831e-03,  7.5639e-02, -4.0205e-02, -1.8861e-01,\n",
      "          8.8978e-02,  4.2677e-02, -2.1185e-02, -1.3516e-01, -1.3285e-02],\n",
      "        [-9.6589e-02, -5.0082e-02,  6.0020e-02, -8.9461e-02, -2.0637e-01,\n",
      "          1.4633e-02, -5.9751e-03, -2.1103e-02, -8.9272e-02,  2.0189e-02],\n",
      "        [-4.0376e-02, -2.4114e-02,  5.4426e-02, -3.3141e-02, -2.0282e-01,\n",
      "          4.8264e-02,  1.3828e-02, -2.3766e-02, -9.8455e-02,  1.8631e-02],\n",
      "        [-5.5082e-02, -5.1868e-02, -1.9276e-02, -5.4404e-03, -1.8327e-01,\n",
      "          9.5654e-02, -1.4917e-03, -1.7165e-02, -1.4496e-01, -1.1873e-04],\n",
      "        [-2.2693e-02, -1.6680e-02,  3.5069e-02,  2.6432e-02, -1.3353e-01,\n",
      "          1.7415e-02,  3.2808e-02, -4.2438e-03, -1.1931e-01,  2.1868e-02],\n",
      "        [-6.4171e-02, -6.1398e-02,  2.2825e-02, -3.0776e-02, -2.1993e-01,\n",
      "          4.6010e-02, -1.8999e-03,  1.8231e-02, -1.2896e-01,  2.8065e-02],\n",
      "        [-2.9338e-02, -6.8224e-03,  5.6370e-03, -4.5128e-02, -1.3878e-01,\n",
      "          3.5853e-02,  9.2315e-03,  4.0099e-05, -1.0380e-01,  2.3239e-03],\n",
      "        [-6.7722e-02, -3.6795e-02,  4.2677e-02, -2.3718e-02, -1.7243e-01,\n",
      "          9.0264e-02,  8.7723e-03,  1.1390e-02, -1.4505e-01,  1.5727e-02],\n",
      "        [ 1.4804e-02, -1.2551e-01,  2.2453e-02, -1.4617e-02, -1.6388e-01,\n",
      "          5.3346e-02,  5.5194e-02, -1.0749e-01, -1.1044e-01,  4.8192e-02],\n",
      "        [-1.3656e-02,  1.4470e-02, -1.2262e-02, -6.0736e-03, -1.4802e-01,\n",
      "          6.1663e-02, -3.2492e-02, -6.8822e-03, -1.5304e-01, -2.2960e-02],\n",
      "        [-3.4837e-02, -2.9544e-02,  8.8911e-02, -6.9044e-02, -2.2735e-01,\n",
      "          6.0524e-02,  4.0418e-02,  1.2270e-03, -1.5178e-01,  1.4998e-02],\n",
      "        [-6.0623e-02, -4.8681e-02,  3.1755e-02, -7.3840e-02, -1.6589e-01,\n",
      "          5.1248e-02,  2.1056e-02, -1.0421e-02, -1.0573e-01,  7.1761e-02],\n",
      "        [-5.1127e-02, -9.3127e-03,  3.6638e-02, -7.1360e-03, -1.8541e-01,\n",
      "          7.1343e-02, -2.6698e-02,  1.6694e-02, -9.2597e-02, -3.9233e-02],\n",
      "        [-4.3246e-02, -5.4332e-03,  7.4427e-02, -4.6812e-02, -2.4414e-01,\n",
      "          9.2108e-02, -1.4582e-02, -2.3463e-03, -1.3375e-01,  2.3298e-02],\n",
      "        [-2.3641e-02, -1.2718e-03,  2.4993e-02, -1.9422e-03, -1.3474e-01,\n",
      "          5.0692e-02, -2.5468e-02, -2.0422e-02, -8.3546e-02, -3.4945e-02],\n",
      "        [-5.0295e-02, -2.1239e-02,  2.7402e-02, -7.0142e-03, -1.8360e-01,\n",
      "          3.2203e-02,  1.9463e-02, -2.8518e-03, -1.1046e-01, -4.7323e-02],\n",
      "        [-5.9621e-02, -8.0655e-02,  3.3145e-02, -4.4011e-02, -2.0619e-01,\n",
      "          2.0056e-02,  4.1705e-04,  2.2624e-02, -1.0213e-01,  2.5079e-02],\n",
      "        [-4.1299e-02, -5.6065e-02,  2.9661e-02, -5.2968e-02, -2.1313e-01,\n",
      "          4.3858e-02, -2.0351e-02,  2.1560e-02, -1.3571e-01,  8.1744e-02],\n",
      "        [-6.9396e-02, -7.0197e-02,  5.9561e-02,  8.2436e-03, -1.6108e-01,\n",
      "          5.5147e-02,  1.9792e-02,  2.0383e-02, -1.2403e-01,  8.2305e-02],\n",
      "        [-6.3476e-02, -5.5614e-02, -1.4075e-02, -5.1472e-02, -2.1850e-01,\n",
      "          1.6289e-02, -1.7320e-02,  5.0024e-02, -1.0351e-01,  1.5861e-03],\n",
      "        [-4.9053e-02,  1.9776e-02,  4.2761e-02, -9.9872e-03, -1.3858e-01,\n",
      "          9.3151e-02, -1.1035e-03, -2.2560e-02, -1.2910e-01,  3.0624e-02],\n",
      "        [-4.1385e-02, -3.1262e-02,  4.8252e-02, -6.7039e-02, -1.5590e-01,\n",
      "          4.3235e-02, -5.0818e-02, -3.2023e-02, -8.8745e-02, -2.3327e-02],\n",
      "        [-4.5060e-02,  5.1086e-03,  2.3413e-02, -2.0451e-02, -1.6643e-01,\n",
      "          1.3671e-01, -2.4548e-02,  1.6576e-02, -1.0997e-01,  2.1469e-02],\n",
      "        [-3.2023e-02, -9.4804e-03,  1.8387e-02, -4.5764e-02, -1.7698e-01,\n",
      "          8.3427e-02, -1.9944e-02,  1.8748e-02, -9.7954e-02, -6.7253e-03],\n",
      "        [-2.6973e-02,  1.6920e-02,  7.1721e-02, -1.2530e-02, -1.5029e-01,\n",
      "          5.3666e-02,  8.5398e-03, -2.5103e-02, -1.6056e-01,  5.4416e-02],\n",
      "        [-2.7213e-02, -3.8570e-02, -3.3651e-03, -4.4832e-02, -2.2545e-01,\n",
      "          5.7777e-02, -6.5096e-03,  1.2137e-02, -9.4433e-02,  4.7341e-02],\n",
      "        [-3.7881e-04, -4.7270e-02,  3.8219e-02, -3.6202e-02, -1.5603e-01,\n",
      "          5.0746e-02, -2.5017e-02, -5.8146e-03, -9.4812e-02,  7.0069e-03],\n",
      "        [-5.1907e-02, -1.9200e-02,  6.1118e-02, -3.8663e-02, -1.6249e-01,\n",
      "          9.0822e-02, -1.1772e-02, -1.3255e-02, -1.4129e-01,  5.2735e-02],\n",
      "        [-2.2788e-02, -4.9572e-03,  6.4890e-02, -6.4510e-02, -2.1249e-01,\n",
      "          7.9326e-02, -3.4158e-03, -2.7464e-02, -1.2329e-01, -4.8955e-02],\n",
      "        [-5.0032e-02, -7.5327e-03,  2.0687e-02, -9.9172e-03, -1.4074e-01,\n",
      "          5.7339e-02, -1.1341e-02,  1.8404e-02, -8.5241e-02,  2.6773e-02],\n",
      "        [-7.3483e-02,  6.1470e-03,  3.4345e-02, -4.9217e-02, -1.5873e-01,\n",
      "          1.1251e-01, -6.8175e-03,  3.8845e-02, -1.3417e-01,  9.0687e-02],\n",
      "        [-2.1261e-02, -4.3156e-02,  1.8315e-02,  5.0868e-03, -1.5686e-01,\n",
      "          7.4738e-02, -9.7178e-03, -1.8040e-02, -1.3953e-01,  3.7427e-02],\n",
      "        [-8.4185e-02, -5.0799e-02,  9.8912e-02, -4.7379e-02, -2.1227e-01,\n",
      "         -1.7277e-02,  5.0468e-02,  4.0975e-02, -1.2768e-01,  7.8310e-02],\n",
      "        [-3.7698e-03, -2.8601e-02,  4.8254e-02, -5.1654e-02, -2.1035e-01,\n",
      "          5.1708e-02, -7.3726e-03, -5.8566e-02, -1.5475e-01, -2.1899e-02],\n",
      "        [-6.6750e-02, -3.7775e-02,  2.6304e-02, -3.9497e-02, -2.1601e-01,\n",
      "          3.1640e-02,  5.4968e-03, -9.8834e-03, -1.2865e-01,  2.5547e-02],\n",
      "        [ 2.2139e-02, -4.8160e-02,  1.2767e-02, -9.7254e-03, -1.8670e-01,\n",
      "          4.0638e-02, -1.5705e-02, -1.6971e-02, -1.6919e-01, -1.3672e-02],\n",
      "        [-9.4291e-02, -1.7692e-02,  7.2642e-02, -4.1760e-02, -1.7339e-01,\n",
      "          8.5288e-02, -2.3028e-02,  2.2206e-03, -8.8507e-02, -1.3517e-02],\n",
      "        [-3.0365e-02, -2.4302e-02,  5.3540e-02, -5.9718e-03, -1.6151e-01,\n",
      "          4.3098e-02,  2.4896e-02,  3.2859e-02, -1.2607e-01,  6.5688e-02],\n",
      "        [-2.9927e-02, -7.3186e-02,  5.4240e-02, -3.7573e-02, -1.8851e-01,\n",
      "          1.0766e-01,  2.6593e-02, -4.8960e-02, -1.3648e-01, -2.6981e-02],\n",
      "        [-3.5039e-02, -4.6553e-02,  3.0171e-02,  1.0463e-02, -1.4077e-01,\n",
      "          7.6110e-02,  3.3525e-02, -6.2161e-03, -1.4256e-01,  6.0640e-02],\n",
      "        [-1.6796e-02, -1.5920e-02,  1.6791e-02, -4.1330e-02, -1.6866e-01,\n",
      "          9.7706e-02, -3.8496e-02, -1.2053e-02, -1.1230e-01, -2.1147e-02],\n",
      "        [-3.3624e-02, -1.8474e-02,  1.1478e-03, -5.8157e-03, -2.2460e-01,\n",
      "          7.3638e-02,  1.7242e-02,  2.0223e-02, -1.4532e-01,  2.8561e-02],\n",
      "        [ 2.9641e-02, -3.8129e-02,  2.9873e-02, -1.5803e-02, -1.5421e-01,\n",
      "          5.1907e-02, -3.9242e-02, -1.6912e-02, -1.3920e-01, -2.4977e-02],\n",
      "        [-5.5580e-02,  2.3423e-02,  4.3103e-02, -2.4488e-02, -1.4366e-01,\n",
      "          8.9742e-02,  1.9164e-03, -1.0621e-03, -1.5590e-01,  4.2598e-02],\n",
      "        [-1.1210e-01, -3.0804e-02,  4.7548e-02, -2.6462e-02, -2.4837e-01,\n",
      "          3.7843e-02,  1.1146e-02,  4.0140e-02, -1.0277e-01,  7.0443e-02],\n",
      "        [-6.9797e-02, -1.6807e-02,  3.7203e-02, -3.5249e-02, -2.0139e-01,\n",
      "          6.8084e-02,  4.2711e-02,  5.6709e-02, -7.7532e-02,  2.5056e-02],\n",
      "        [-4.1401e-02, -7.8469e-03, -4.1664e-02, -3.1071e-02, -1.9118e-01,\n",
      "          6.8058e-02,  1.5795e-02, -9.6349e-03, -1.1331e-01,  1.4133e-02]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for data, target in test_loader:\n",
    "    with torch.no_grad():\n",
    "        data = data.to(device = device)\n",
    "        target = target.to(device = device)\n",
    "\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa0f7649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FORWARD ===\n",
      "module: Linear(in_features=84, out_features=10, bias=True)\n",
      "input: (tensor([[ 0.1938, -0.0961,  0.0605,  ...,  0.2565, -0.0562, -0.0951],\n",
      "        [ 0.1226, -0.0816, -0.0051,  ...,  0.1753, -0.0573, -0.0439],\n",
      "        [ 0.1605, -0.0075, -0.0045,  ...,  0.2128, -0.0574, -0.0567],\n",
      "        ...,\n",
      "        [ 0.1423, -0.0936, -0.0359,  ...,  0.2858, -0.0511, -0.0020],\n",
      "        [ 0.1197, -0.0335,  0.0488,  ...,  0.1880, -0.0946,  0.0036],\n",
      "        [ 0.1168, -0.1262,  0.0306,  ...,  0.2001, -0.0941,  0.0332]],\n",
      "       device='cuda:0', grad_fn=<BackwardHookFunctionBackward>),)\n",
      "output: tensor([[-2.3684e-02,  1.0965e-02,  9.5936e-02, -5.7654e-03, -1.5007e-01,\n",
      "          9.0610e-02, -2.8898e-03, -2.0115e-02, -1.2361e-01,  1.3031e-02],\n",
      "        [-1.3795e-02, -6.1894e-02,  4.7505e-02, -3.4628e-02, -1.5252e-01,\n",
      "          6.9504e-02, -3.0853e-02, -2.5416e-02, -1.4652e-01,  2.6459e-02],\n",
      "        [-2.0293e-02,  3.4434e-03,  7.7792e-02, -1.8158e-02, -1.5684e-01,\n",
      "          1.1801e-01,  5.0319e-03, -1.5712e-02, -1.3289e-01,  5.4643e-02],\n",
      "        [ 2.0507e-02, -4.5096e-02,  2.5561e-02, -6.9286e-02, -2.1030e-01,\n",
      "          8.8633e-02,  4.3523e-02, -5.9669e-02, -1.3818e-01,  4.3432e-02],\n",
      "        [-6.0420e-02, -4.2547e-02,  9.8780e-02, -6.5042e-02, -1.9785e-01,\n",
      "          1.5183e-02,  1.7920e-02,  1.5301e-02, -9.4492e-02,  3.7935e-02],\n",
      "        [-2.6977e-02, -4.2918e-02,  2.0511e-02, -4.0318e-02, -1.5162e-01,\n",
      "          5.6713e-02, -1.0890e-02, -2.4570e-02, -1.0373e-01, -3.0343e-02],\n",
      "        [-3.8573e-02, -1.5072e-02,  5.2067e-02, -1.6662e-03, -2.1138e-01,\n",
      "          8.6493e-02,  2.8052e-02, -2.3981e-03, -1.3349e-01,  5.5904e-02],\n",
      "        [-4.6575e-02, -5.7510e-03,  1.3027e-02, -1.6987e-02, -2.1803e-01,\n",
      "          5.0220e-02,  3.7792e-02, -1.3915e-02, -9.9893e-02,  4.3642e-02],\n",
      "        [-9.0929e-02, -5.1705e-02, -7.8635e-03, -4.7917e-03, -1.5201e-01,\n",
      "          3.6375e-02,  3.9733e-02,  1.3162e-02, -9.1454e-02,  7.6836e-02],\n",
      "        [-8.9916e-02, -6.5220e-02,  6.7542e-03,  1.3315e-03, -1.6144e-01,\n",
      "          1.4307e-02,  1.5776e-02,  2.9722e-02, -9.8748e-02, -3.3574e-02],\n",
      "        [-2.0503e-02, -7.2776e-02,  6.5738e-02, -4.4002e-02, -1.6772e-01,\n",
      "          8.0679e-02,  4.7498e-03, -5.7829e-02, -1.3469e-01,  6.7494e-03],\n",
      "        [-4.0916e-02, -5.7088e-02, -1.9526e-03, -5.5855e-02, -2.3570e-01,\n",
      "          3.8789e-02, -8.3396e-03,  9.4642e-03, -1.4883e-01,  3.9109e-02],\n",
      "        [-5.7029e-03, -2.7529e-02, -2.7577e-02, -1.2579e-02, -1.8403e-01,\n",
      "          5.8362e-02, -8.8742e-03, -1.6345e-02, -1.1327e-01,  1.7934e-02],\n",
      "        [-2.4579e-02, -1.2607e-02,  4.8331e-02,  1.4479e-03, -1.4827e-01,\n",
      "          7.7033e-02,  5.4932e-02, -6.8992e-02, -1.0515e-01,  2.9170e-02],\n",
      "        [-4.9529e-03,  2.4323e-03,  6.2927e-02, -9.3897e-03, -1.5515e-01,\n",
      "          1.3513e-01, -8.3776e-03, -4.7808e-02, -1.5743e-01,  2.1362e-02],\n",
      "        [-7.0492e-02, -1.0793e-02,  5.1882e-02, -6.9613e-02, -1.4595e-01,\n",
      "          1.0503e-01,  4.2346e-03, -1.9083e-02, -8.1692e-02, -2.8823e-03],\n",
      "        [-4.3268e-02, -7.0441e-02,  8.5757e-02, -7.0930e-02, -2.1497e-01,\n",
      "          1.8375e-02,  5.2894e-03, -2.5807e-02, -1.3713e-01,  5.0533e-03],\n",
      "        [-4.6216e-02, -1.8370e-02,  8.4098e-02,  2.5446e-03, -1.5556e-01,\n",
      "          7.3035e-02,  3.9670e-02, -1.7674e-02, -1.1590e-01,  4.5969e-02],\n",
      "        [-6.9913e-02, -9.8614e-03, -9.2350e-03, -5.0983e-02, -1.7781e-01,\n",
      "          1.1777e-02,  2.7326e-02,  2.4434e-02, -7.5209e-02,  4.9460e-02],\n",
      "        [-3.5554e-02, -4.4383e-03,  1.6623e-03, -3.9177e-03, -1.9142e-01,\n",
      "          6.8848e-02,  6.4458e-03,  7.9951e-03, -1.5587e-01,  2.0575e-02],\n",
      "        [ 6.8672e-03, -2.1830e-02, -3.8709e-02, -8.0656e-02, -2.4446e-01,\n",
      "          7.4247e-02, -2.2954e-02,  1.8131e-03, -8.2715e-02,  6.2530e-02],\n",
      "        [-4.2661e-02, -7.1822e-03,  6.3239e-04, -5.2818e-02, -1.5469e-01,\n",
      "          1.9923e-02, -6.9436e-03, -3.5884e-02, -1.1504e-01,  2.1686e-02],\n",
      "        [-1.8179e-02, -6.5355e-02,  7.1747e-02,  4.9553e-03, -2.1336e-01,\n",
      "          4.7398e-02, -5.8387e-03,  1.7937e-03, -1.4975e-01,  5.3516e-02],\n",
      "        [ 2.9014e-02, -2.0313e-02,  4.4496e-02, -4.1715e-02, -2.2472e-01,\n",
      "          8.2613e-02,  5.8682e-04, -3.7773e-02, -1.4591e-01,  4.8957e-02],\n",
      "        [-3.6073e-02, -3.1210e-02,  4.8192e-02, -1.0882e-02, -2.0200e-01,\n",
      "          4.8644e-02, -1.5720e-03, -1.0897e-02, -1.2177e-01,  2.8242e-02],\n",
      "        [-1.6881e-02, -4.9466e-02,  3.7006e-02, -2.6285e-02, -1.8145e-01,\n",
      "          8.9845e-02,  4.6078e-02, -7.8073e-02, -8.8113e-02,  2.3731e-02],\n",
      "        [-2.3552e-02, -6.2149e-03,  5.3856e-03, -5.0729e-03, -2.0825e-01,\n",
      "          5.6587e-02,  1.4747e-02, -1.3271e-02, -1.0912e-01, -2.4699e-02],\n",
      "        [-1.6915e-03,  7.8061e-03,  7.5652e-03,  1.5079e-02, -1.8268e-01,\n",
      "          4.2125e-02, -1.7389e-04, -1.3038e-02, -1.1987e-01,  3.0148e-02],\n",
      "        [-3.6386e-02, -3.6510e-02,  2.1587e-02,  2.0749e-02, -1.8844e-01,\n",
      "          9.2680e-02,  3.5656e-02, -6.5977e-02, -1.5945e-01,  8.2749e-03],\n",
      "        [-3.3755e-02, -1.7955e-02, -2.3335e-03, -2.2339e-02, -1.6558e-01,\n",
      "          7.7625e-02, -2.6142e-02,  2.0281e-02, -1.0190e-01,  6.3858e-03],\n",
      "        [-5.4172e-02,  1.8945e-03,  9.0043e-02, -3.2354e-02, -2.2614e-01,\n",
      "          6.5537e-02, -3.7221e-02, -1.3118e-02, -1.2928e-01,  1.6429e-02],\n",
      "        [-4.6328e-02, -8.6139e-03,  4.0188e-02,  2.0032e-02, -1.4258e-01,\n",
      "          9.9435e-02,  9.7549e-03, -2.3142e-02, -1.0005e-01,  2.5272e-02],\n",
      "        [-6.0733e-02, -3.6442e-02,  6.0866e-02, -4.1241e-04, -2.1368e-01,\n",
      "          3.1882e-02,  6.3247e-03, -3.6570e-02, -1.4665e-01, -3.0727e-02],\n",
      "        [-4.8209e-02, -6.4671e-02,  3.0137e-02, -5.7550e-02, -1.9119e-01,\n",
      "          1.3952e-02,  2.4432e-02, -5.6145e-03, -9.0551e-02,  4.4944e-02],\n",
      "        [-7.6087e-02, -8.4325e-02, -7.0024e-03, -3.7798e-02, -1.9740e-01,\n",
      "          3.1246e-02, -1.4099e-02,  4.2417e-02, -1.0823e-01, -1.3879e-02],\n",
      "        [-4.1190e-02, -4.8873e-02,  6.5562e-02, -3.8551e-02, -1.6576e-01,\n",
      "          6.2166e-02, -1.4964e-02,  1.3461e-02, -9.2971e-02, -2.2453e-02],\n",
      "        [-6.9279e-02, -1.4965e-02,  4.3329e-02, -2.1350e-02, -1.9424e-01,\n",
      "          7.3463e-02, -7.5442e-03,  4.8713e-02, -1.2295e-01,  5.1655e-02],\n",
      "        [-3.5430e-02,  1.2262e-02,  3.4915e-02, -3.1479e-02, -1.6845e-01,\n",
      "          9.2724e-02, -7.1998e-03, -1.2576e-02, -1.4298e-01,  4.5038e-02],\n",
      "        [ 1.2232e-02, -2.9134e-02,  5.4594e-02, -6.0299e-02, -2.0747e-01,\n",
      "          6.3302e-02, -2.0701e-02, -4.0320e-02, -9.4515e-02,  3.9723e-02],\n",
      "        [-3.3236e-02,  2.3911e-02,  1.5628e-02, -1.3685e-02, -1.7746e-01,\n",
      "          1.3313e-01, -3.7185e-03, -3.8240e-03, -1.3218e-01,  6.4772e-02],\n",
      "        [-3.9661e-02,  7.2398e-03,  5.9665e-03, -4.4468e-02, -1.6313e-01,\n",
      "          8.5047e-02, -4.3377e-02,  5.1164e-02, -9.1295e-02, -4.1186e-03],\n",
      "        [-5.6158e-02,  2.7089e-02,  4.3366e-02, -2.6044e-02, -1.8421e-01,\n",
      "          7.7995e-02, -5.3224e-03,  3.0824e-02, -1.2966e-01,  5.4529e-02],\n",
      "        [-3.6830e-02,  9.0764e-03, -1.0279e-02,  1.6505e-04, -1.7869e-01,\n",
      "          7.3453e-02, -2.9238e-02, -1.2762e-04, -1.3634e-01,  3.1452e-02],\n",
      "        [-1.1185e-02, -5.3069e-02,  1.2942e-02, -3.2396e-02, -1.6392e-01,\n",
      "          3.4279e-02,  9.1944e-03, -2.2979e-02, -1.1786e-01,  2.4551e-03],\n",
      "        [-5.8000e-02, -1.7553e-02, -2.6716e-02, -7.7115e-02, -1.5764e-01,\n",
      "          3.1342e-02, -4.0896e-02,  6.3715e-02, -8.5919e-02,  8.6709e-03],\n",
      "        [ 1.7272e-03, -4.1423e-03, -7.8945e-03,  3.9462e-02, -1.2520e-01,\n",
      "          3.7231e-02, -1.2994e-02, -3.8054e-02, -1.3394e-01, -7.5647e-02],\n",
      "        [-5.2999e-02, -1.8123e-02,  2.7787e-02, -1.9569e-02, -1.3360e-01,\n",
      "          6.2559e-02, -2.2730e-02,  1.5039e-02, -8.1622e-02,  1.3140e-02],\n",
      "        [-6.1469e-02, -2.8834e-02,  9.2107e-03, -4.7431e-02, -2.1548e-01,\n",
      "          5.5451e-02, -2.9489e-02,  2.6844e-02, -1.1427e-01, -9.3025e-03],\n",
      "        [-5.5876e-02, -7.1788e-03,  2.2231e-02,  3.7291e-02, -1.6654e-01,\n",
      "          8.7158e-02,  2.7491e-02, -3.1097e-02, -1.3464e-01,  5.4608e-02],\n",
      "        [-7.8775e-02, -3.6681e-02,  3.2116e-02,  1.4996e-02, -1.9848e-01,\n",
      "          7.5753e-02, -1.5195e-02,  1.3710e-02, -1.4751e-01,  4.7370e-02],\n",
      "        [-7.3665e-02, -3.8713e-02, -6.5664e-03, -4.3641e-02, -1.5894e-01,\n",
      "          5.7984e-02, -2.7025e-03,  2.1700e-02, -1.1715e-01,  1.6261e-02],\n",
      "        [-7.1623e-02, -3.4475e-02,  3.7844e-03, -5.9325e-02, -2.2246e-01,\n",
      "          6.1400e-02,  5.9498e-02, -7.8509e-03, -1.3216e-01,  9.7120e-02],\n",
      "        [-4.0250e-02, -5.2528e-02,  6.2277e-02, -5.3544e-02, -2.0749e-01,\n",
      "          3.9316e-02,  1.7231e-02, -3.6745e-02, -1.0321e-01, -2.4542e-02],\n",
      "        [-4.4032e-02, -2.0856e-02,  8.2743e-02, -4.7450e-02, -1.6919e-01,\n",
      "          3.8092e-02,  8.3058e-03, -2.0637e-02, -1.1944e-01,  2.5868e-03],\n",
      "        [-6.8324e-02, -5.0753e-02,  9.3188e-02, -7.8766e-02, -1.9024e-01,\n",
      "         -3.3711e-03,  3.2919e-02, -4.5026e-02, -9.9888e-02,  5.9659e-02],\n",
      "        [ 1.6158e-02, -9.5379e-03,  3.5430e-02,  1.9999e-02, -1.4293e-01,\n",
      "          5.8039e-02,  1.4415e-02, -7.2196e-02, -1.1141e-01, -2.8537e-03],\n",
      "        [-5.0662e-02, -5.0604e-02,  7.4331e-02, -3.2720e-02, -2.1685e-01,\n",
      "         -1.1027e-02,  6.3976e-02, -3.9861e-02, -9.6569e-02,  3.2213e-02],\n",
      "        [-1.6796e-02, -1.5920e-02,  1.6791e-02, -4.1330e-02, -1.6866e-01,\n",
      "          9.7706e-02, -3.8496e-02, -1.2053e-02, -1.1230e-01, -2.1147e-02],\n",
      "        [-1.4058e-02, -1.6118e-02,  4.8166e-02,  2.8551e-04, -2.1352e-01,\n",
      "          2.1878e-02, -2.1855e-02, -1.8649e-02, -1.7242e-01,  7.9717e-02],\n",
      "        [ 1.6749e-02, -4.0109e-02,  2.8741e-02, -3.7444e-02, -1.6240e-01,\n",
      "          7.3130e-02, -2.0716e-03, -2.8705e-02, -1.1689e-01,  2.9993e-02],\n",
      "        [-3.7158e-02,  3.8030e-02,  4.8024e-02, -1.2494e-02, -1.8821e-01,\n",
      "          9.8431e-02, -1.7993e-02,  1.9873e-02, -1.5085e-01,  6.8044e-02],\n",
      "        [-1.8984e-02, -8.0972e-02,  3.3626e-02, -2.2928e-02, -1.6777e-01,\n",
      "          7.5312e-02, -3.3340e-03, -2.9609e-02, -1.8873e-01,  2.4987e-02],\n",
      "        [-3.9035e-02,  1.1912e-02,  8.1731e-03,  1.2866e-02, -1.3048e-01,\n",
      "          7.3375e-02,  2.0298e-02,  4.5176e-02, -1.3151e-01,  7.5763e-03],\n",
      "        [-3.3735e-02, -3.2020e-02, -2.9111e-02, -4.8272e-03, -1.5064e-01,\n",
      "          4.7637e-02, -4.8962e-03, -2.2500e-02, -1.0663e-01, -1.7790e-02]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "=== BACKWARD ===\n",
      "module: Linear(in_features=84, out_features=10, bias=True)\n",
      "input: (tensor([[ 9.9006e-04,  6.4977e-05, -4.8583e-04,  ..., -1.4549e-03,\n",
      "          1.7187e-04,  4.5778e-04],\n",
      "        [ 7.2530e-05, -1.1156e-03, -1.2699e-04,  ..., -1.2823e-03,\n",
      "          8.9624e-04, -3.6626e-04],\n",
      "        [-4.5157e-04,  5.3521e-04, -1.8514e-03,  ...,  1.0940e-03,\n",
      "          1.5184e-04,  1.0946e-04],\n",
      "        ...,\n",
      "        [ 2.1784e-04, -7.8883e-04,  1.2213e-03,  ...,  8.4904e-04,\n",
      "         -2.6833e-04, -6.6892e-04],\n",
      "        [-2.6126e-04,  1.6264e-03,  1.4955e-03,  ..., -7.6670e-04,\n",
      "         -2.8025e-04,  1.0599e-03],\n",
      "        [-5.7624e-04,  7.7426e-04, -9.8555e-05,  ..., -8.4770e-04,\n",
      "          3.1620e-04,  1.0441e-03]], device='cuda:0'),)\n",
      "output: (tensor([[ 0.0015,  0.0016,  0.0017,  0.0016,  0.0014,  0.0017,  0.0016, -0.0141,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0016,  0.0015, -0.0139,  0.0016,  0.0014,  0.0017,  0.0016,  0.0016,\n",
      "          0.0014,  0.0017],\n",
      "        [ 0.0015, -0.0140,  0.0017,  0.0015,  0.0013,  0.0018,  0.0016,  0.0015,\n",
      "          0.0014,  0.0017],\n",
      "        [-0.0140,  0.0015,  0.0016,  0.0015,  0.0013,  0.0018,  0.0017,  0.0015,\n",
      "          0.0014,  0.0017],\n",
      "        [ 0.0015,  0.0015,  0.0018,  0.0015, -0.0143,  0.0016,  0.0016,  0.0016,\n",
      "          0.0015,  0.0017],\n",
      "        [ 0.0016, -0.0141,  0.0016,  0.0016,  0.0014,  0.0017,  0.0016,  0.0016,\n",
      "          0.0015,  0.0016],\n",
      "        [ 0.0015,  0.0016,  0.0017,  0.0016, -0.0143,  0.0017,  0.0016,  0.0016,\n",
      "          0.0014,  0.0017],\n",
      "        [ 0.0015,  0.0016,  0.0016,  0.0016,  0.0013,  0.0017,  0.0017,  0.0016,\n",
      "          0.0014, -0.0140],\n",
      "        [ 0.0015,  0.0015,  0.0016,  0.0016,  0.0014, -0.0140,  0.0017,  0.0016,\n",
      "          0.0015,  0.0017],\n",
      "        [ 0.0015,  0.0015,  0.0016,  0.0016,  0.0014,  0.0016,  0.0016,  0.0017,\n",
      "          0.0015, -0.0141],\n",
      "        [-0.0140,  0.0015,  0.0017,  0.0015,  0.0014,  0.0017,  0.0016,  0.0015,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0016,  0.0015,  0.0016,  0.0015,  0.0013,  0.0017, -0.0140,  0.0016,\n",
      "          0.0014,  0.0017],\n",
      "        [ 0.0016,  0.0016,  0.0016,  0.0016,  0.0013,  0.0017,  0.0016,  0.0016,\n",
      "          0.0014, -0.0140],\n",
      "        [-0.0141,  0.0016,  0.0017,  0.0016,  0.0014,  0.0017,  0.0017,  0.0015,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0016, -0.0140,  0.0017,  0.0016,  0.0014,  0.0018,  0.0016,  0.0015,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0015,  0.0016,  0.0017,  0.0015,  0.0014, -0.0139,  0.0016,  0.0016,\n",
      "          0.0015,  0.0016],\n",
      "        [ 0.0016,  0.0015,  0.0018,  0.0015,  0.0013,  0.0017,  0.0016,  0.0016,\n",
      "          0.0014, -0.0140],\n",
      "        [ 0.0015,  0.0015,  0.0017,  0.0016,  0.0013,  0.0017,  0.0016, -0.0141,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0015,  0.0016,  0.0016, -0.0141,  0.0013,  0.0016,  0.0016,  0.0016,\n",
      "          0.0015,  0.0017],\n",
      "        [ 0.0015,  0.0016,  0.0016,  0.0016, -0.0143,  0.0017,  0.0016,  0.0016,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0016,  0.0016,  0.0016,  0.0015,  0.0013,  0.0017,  0.0016,  0.0016,\n",
      "          0.0015, -0.0139],\n",
      "        [ 0.0016,  0.0016,  0.0016,  0.0015,  0.0014,  0.0017, -0.0140,  0.0016,\n",
      "          0.0014,  0.0017],\n",
      "        [ 0.0016,  0.0015,  0.0017,  0.0016,  0.0013,  0.0017, -0.0140,  0.0016,\n",
      "          0.0014,  0.0017],\n",
      "        [ 0.0016,  0.0016,  0.0017,  0.0015,  0.0013, -0.0139,  0.0016,  0.0015,\n",
      "          0.0014,  0.0017],\n",
      "        [ 0.0015,  0.0016,  0.0017,  0.0016, -0.0143,  0.0017,  0.0016,  0.0016,\n",
      "          0.0014,  0.0017],\n",
      "        [-0.0141,  0.0015,  0.0017,  0.0016,  0.0013,  0.0017,  0.0017,  0.0015,\n",
      "          0.0015,  0.0016],\n",
      "        [ 0.0016,  0.0016,  0.0016,  0.0016,  0.0013,  0.0017,  0.0016, -0.0140,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0016,  0.0016,  0.0016,  0.0016, -0.0143,  0.0017,  0.0016,  0.0016,\n",
      "          0.0014,  0.0016],\n",
      "        [-0.0141,  0.0015,  0.0016,  0.0016,  0.0013,  0.0018,  0.0017,  0.0015,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0015, -0.0141,  0.0016,  0.0016,  0.0014,  0.0017,  0.0016,  0.0016,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0015,  0.0016,  0.0018, -0.0141,  0.0013,  0.0017,  0.0015,  0.0016,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0015, -0.0141,  0.0016,  0.0016,  0.0014,  0.0017,  0.0016,  0.0015,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0015,  0.0016,  0.0017, -0.0140,  0.0013,  0.0017,  0.0016,  0.0016,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0015,  0.0015,  0.0017,  0.0015, -0.0143,  0.0016,  0.0017,  0.0016,\n",
      "          0.0015,  0.0017],\n",
      "        [ 0.0015,  0.0015,  0.0016,  0.0016,  0.0013,  0.0017,  0.0016, -0.0139,\n",
      "          0.0015,  0.0016],\n",
      "        [ 0.0015,  0.0015, -0.0139,  0.0015,  0.0014,  0.0017,  0.0016,  0.0016,\n",
      "          0.0015,  0.0016],\n",
      "        [ 0.0015,  0.0016,  0.0017,  0.0016,  0.0013,  0.0017,  0.0016, -0.0140,\n",
      "          0.0014,  0.0017],\n",
      "        [ 0.0015, -0.0140,  0.0016,  0.0015,  0.0013,  0.0017,  0.0016,  0.0016,\n",
      "          0.0014,  0.0017],\n",
      "        [ 0.0016,  0.0016, -0.0139,  0.0015,  0.0013,  0.0017,  0.0016,  0.0015,\n",
      "          0.0015,  0.0017],\n",
      "        [ 0.0015, -0.0140,  0.0016,  0.0016,  0.0013,  0.0018,  0.0016,  0.0016,\n",
      "          0.0014,  0.0017],\n",
      "        [ 0.0015, -0.0140,  0.0016,  0.0015,  0.0014,  0.0017,  0.0015,  0.0017,\n",
      "          0.0015,  0.0016],\n",
      "        [ 0.0015,  0.0016,  0.0017,  0.0015,  0.0013,  0.0017,  0.0016, -0.0140,\n",
      "          0.0014,  0.0017],\n",
      "        [ 0.0015,  0.0016,  0.0016,  0.0016, -0.0143,  0.0017,  0.0016,  0.0016,\n",
      "          0.0014,  0.0017],\n",
      "        [ 0.0016,  0.0015, -0.0140,  0.0016,  0.0014,  0.0017,  0.0016,  0.0016,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0015,  0.0016,  0.0016, -0.0141,  0.0014,  0.0017,  0.0016,  0.0017,\n",
      "          0.0015,  0.0016],\n",
      "        [ 0.0016,  0.0016,  0.0016,  0.0017,  0.0014, -0.0140,  0.0016,  0.0016,\n",
      "          0.0014,  0.0015],\n",
      "        [ 0.0015, -0.0141,  0.0016,  0.0016,  0.0014,  0.0017,  0.0016,  0.0016,\n",
      "          0.0015,  0.0016],\n",
      "        [ 0.0015,  0.0016, -0.0140,  0.0015,  0.0013,  0.0017,  0.0016,  0.0017,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0015,  0.0016,  0.0016,  0.0016, -0.0143,  0.0017,  0.0016,  0.0015,\n",
      "          0.0014,  0.0017],\n",
      "        [ 0.0015,  0.0015,  0.0017,  0.0016, -0.0143,  0.0017,  0.0016,  0.0016,\n",
      "          0.0014,  0.0017],\n",
      "        [ 0.0015,  0.0016,  0.0016,  0.0015,  0.0014,  0.0017, -0.0140,  0.0016,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0015,  0.0016,  0.0016, -0.0141,  0.0013,  0.0017,  0.0017,  0.0016,\n",
      "          0.0014,  0.0018],\n",
      "        [ 0.0016,  0.0015,  0.0017,  0.0015,  0.0013, -0.0139,  0.0017,  0.0016,\n",
      "          0.0015,  0.0016],\n",
      "        [ 0.0015,  0.0016,  0.0017,  0.0015,  0.0014, -0.0140,  0.0016,  0.0016,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0015,  0.0015,  0.0018,  0.0015,  0.0013,  0.0016, -0.0140,  0.0015,\n",
      "          0.0015,  0.0017],\n",
      "        [-0.0140,  0.0016,  0.0016,  0.0016,  0.0014,  0.0017,  0.0016,  0.0015,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0015,  0.0015,  0.0017,  0.0016, -0.0143,  0.0016,  0.0017,  0.0015,\n",
      "          0.0015,  0.0017],\n",
      "        [ 0.0016, -0.0140,  0.0016,  0.0015,  0.0014,  0.0018,  0.0015,  0.0016,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0016,  0.0016,  0.0017,  0.0016,  0.0013,  0.0016,  0.0016,  0.0016,\n",
      "          0.0014, -0.0139],\n",
      "        [ 0.0016,  0.0015,  0.0016,  0.0015,  0.0014, -0.0139,  0.0016,  0.0016,\n",
      "          0.0014,  0.0016],\n",
      "        [ 0.0015,  0.0016,  0.0017,  0.0016,  0.0013,  0.0017,  0.0015, -0.0140,\n",
      "          0.0014,  0.0017],\n",
      "        [ 0.0016,  0.0015,  0.0017,  0.0016,  0.0014,  0.0017,  0.0016,  0.0016,\n",
      "         -0.0143,  0.0017],\n",
      "        [ 0.0015,  0.0016,  0.0016,  0.0016,  0.0014,  0.0017,  0.0016,  0.0017,\n",
      "          0.0014, -0.0140],\n",
      "        [ 0.0016,  0.0016,  0.0016, -0.0140,  0.0014,  0.0017,  0.0016,  0.0016,\n",
      "          0.0015,  0.0016]], device='cuda:0'),)\n"
     ]
    }
   ],
   "source": [
    "def backhook(module, grad_input, grad_output):\n",
    "    print(\"=== BACKWARD ===\")\n",
    "    print(\"module:\", module)\n",
    "    print(\"input:\", grad_input)\n",
    "    print(\"output:\", grad_output)\n",
    "\n",
    "model.fc3.register_full_backward_hook(backhook)\n",
    "\n",
    "for data, target in test_loader:\n",
    "    optimizer.zero_grad()\n",
    "    data = data.to(device = device)\n",
    "    target = target.to(device = device)\n",
    "\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
