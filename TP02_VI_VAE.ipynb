{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a06fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a32680f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = \"~/datasets\"\n",
    "\n",
    "with_cuda = torch.cuda.is_available()\n",
    "if with_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33cf2e",
   "metadata": {},
   "source": [
    "# Reminders on a toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ab8d51",
   "metadata": {},
   "source": [
    "## Building the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "074cc262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(height, width):\n",
    "    img = torch.zeros((1, height, width), device = device)\n",
    "    j_pos = torch.randint(2, width - 2, (1,))\n",
    "    for i in range(height):\n",
    "        for j in range(j_pos - 2, j_pos + 2):\n",
    "            img[0, i, j] = 1\n",
    "    cl = torch.randint(0, 4, (1,)).item()\n",
    "    img = transforms.functional.rotate(img, 45*cl)\n",
    "    return img, cl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e27087",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "With the class `torch.utils.data.TensorDataset`, build a dataset generated by the function `generate_image`. Show some samples with matplotlib functiob `imshow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41da4358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFwAAABZCAYAAACzIkPrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAABJpJREFUeJztnE1LMlEUgI/2okX0QUSalOSidYugaF3QKuoX1K5Nm2jXwtoERkFEEbRsV9Ki+gERtOkD+qBFIAURQli0SC36AD0v91KDmryvozPHO+N54EKjjnN5up45M/eecSAiAkOGk+5QDAsvAzzCiWHhxLBwYlg4MSycGBZODAsnhoXbRfja2hp0dHRAdXU19Pb2wunpqVmHshQOM+6lhMNhGB0dhfX1dSl7eXkZtre3IRKJQEtLyz/3TafT8PDwAHV1deBwOEBVhLZkMgk+nw+cTh3jFk2gp6cHJyYmtO1UKoU+nw9DodB/941Go2IAWKaJ/urB8JDy9fUFZ2dnMDAwoL0mRoDYPjo6+vX5z89PSCQSWrPazUvxS9SD4cKfn58hlUqBx+PJel1sx2KxX58PhULQ0NCgNb/fD1ZCb9gre5YyPT0N8Xhca9FoFOzMH6O/sLm5GaqqquDx8THrdbHt9Xp/fd7tdstWKRg+wl0uF3R3d8P+/n5W5iG2+/r6jD6c9UAT2NraQrfbjRsbG3h9fY3j4+PY2NiIsVjsv/vG4/GyZx65LZPc90R/9WCKcMHq6ir6/X50uVwyTTw+Pi5oP7sLN+XCpxQSiYTMVlQiU1FuViJO9PX19dbJUioNFp6H71CrtXzviZFdDCycGBZu9Qsfq4JEuQOPcGJYODEsnJiKjeFYpus9HuHEsHBiKiakoEEhpNSJbR7hxLBwYlg4MbaO4ahI3M6ERzgxLJwYFk6MrWI4Khizc+ERTgwLJ8byIQUtEEYy4RFOjG7hh4eHMDQ0JFf+i1Gxu7v7a8TNzMxAa2sr1NTUyHXhNzc3Rva5soS/vb1BV1eXrOHJx8LCAqysrMhyk5OTE6itrYXBwUH4+Pgwor/WB0tA7L6zs6Ntp9Np9Hq9uLi4qL328vIiF3Zubm4asrbQKIxad6h3baGhMfzu7k5WOWSWm4h1gqKwKl+5Sb6SE9HsjKHCf0pKCi03yVdy0t7eDnbGciUnDocjqxVKsfspLfynpKTQchOBKDcRy30zm50xVHggEJBiM8tNREwW2QqXmxR5pfn6+gq3t7dZJ8rLy0toamqSJX+Tk5MwNzcHnZ2d8h8QDAZlzj4yMmLKlWO5T7K6r3T1plMHBwd506OxsTEtNQwGg+jxeGQ62N/fj5FIpODvj9q8Elm5kpP0d6296Jb4xYiTaLnjuvgViewpsy/F1tord/PK6XRCW1ubFipUOpHm9qWYWqSyp4WVBgsnRlnhbrcbZmdnlSgLN7Ivyp007Y6yI9yusHBiWDgxLJwYFk6MssLXyvDcQ4oJciWFh8NhmJqakrnv+fm5nLQWE9FPT0+mHpdkghwVpKeE5x4ahRkT5ALlRviXzuceUlHMBHk+lBP+rPO5h1QUM0FuCeF2RznhzTqfe0hFMRPklhDuUvS5h4ZNkKOCbJXw3MNSSCaTeHFxIZtQs7S0JP++v7+X78/Pz8t+7O3t4dXVFQ4PD2MgEMD39/eCj6Gk8FKee1gKZk+QC/h+ODHKxXC7w8KJYeHEsHBiWDgxLJwYFk4MCyeGhRPDwolh4UDLX10nRYpH3zRdAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFwAAABZCAYAAACzIkPrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAABMxJREFUeJztnDtLK0EUx8/GS6KID0RMDBpMYW0hKNYKVqKfQDsbG7FLEUUQIgoiimBpp8FC/QAi2PgAH1iIQUEkIFEsjFF8QDKXGW5CvIkxm8yezG7ODwayz5n89+zZsztzRmOMMSDQsOFVRZDgJYAsHBkSHBkSHBkSHBkSHBkSHBkSHBkS3CqCr6ysQFtbG1RWVkJ3dzccHx8bVZWp0Iz4lhIMBmF4eBhWV1eF2IuLi7C5uQmhUAiamppyHptIJOD+/h5qampA0zRQFS5bLBYDt9sNNpsOu2UG0NXVxcbGxlLL8Xicud1uFggEfj02HA5zAzBN4e3Vwx/ZV/7r6wtOTk7A5/Ol1nEL6Ovrg4ODg4z9Pz8/RUkzAF31RaPRH7fV1dWB0fA7UQ/SBX96eoJ4PA5Op/Pber58dXWVsX8gEIDp6emC66utrYVSotftlTxK8fl8wkqTJRwO59yf3wHpJd99VUG6hTc2NkJFRQU8PDx8W8+XXS5Xxv4Oh0OUckG6hdvtdujs7ITd3d1vkQdf7unpkV2d+WAGsLGxwRwOB1tbW2OXl5dsdHSU1dfXs0gk8uux0Wg0IxIwAllRCm+vrnoN+TeMseXlZebxeJjdbhdh4uHhYV7HWV1wQ158iuHl5SUjnDOiibJeqviDXk+kVPIopdyQHqUYgZZmjbKs/f/zYH1GIAtHhgRHxhQuJdetb4SLMdK9kIUjQ4IjQ4IjYzofbvaQkSwcGRIcGdO7FLOFjGThyJDgyJDgyFhacE3TUkUWyU7pXMMzylZwFSHBkSHBkbFUHF6KGF0vZOHIkODIlI1LkeVi6NXe6i5lf38fBgYGxMh/frW3t7czLGVychKam5uhqqpKjAu/vr6W2ebyEvzt7Q06OjpEDk825ubmYGlpSaSbHB0dQXV1NfT398PHx4eM9pqfYsfnbW1tpZYTiQRzuVxsfn4+te75+VkM7FxfXy94bCF2yfY/ZY0tlBql3N7eQiQSEW4kCR8nyBOrsqWbcHi6CR9PmF6sjFTBudicbOkmyW3ZUk74RUmW1tZWsDKmSznB/sooe1CQVMGTKSX5pptweLoJH+6bXqyMVMG9Xq8QNj3dhPtkHq1QukmBb5qvr69wc3Pz7UF5fn4ODQ0N4PF4YHx8HGZmZqC9vV1cAL/fL2L2oaGhvM6vWH6A/PbqimkYY3t7e1nDo5GRkVRo6Pf7mdPpFOFgb28vC4VCeZ8/bPFMZOVSThL/cu15s/gdwx+ipfbr3C3y6Cm9LYXm2iv38cpms0FLS0sqHlfpQfp/WwpJLS95WFhukODIKCu4w+GAqakpJdLCZbZFuYem1VHWwq0KCY4MCY4MCY4MCY6MsoKvlGDeQ4wOciUFDwaDMDExIWLf09NT0WnNO6IfHx8NrRelg5wpSFcR8x7KwogOco5yFv71b97D9I7oXPMeYlFIB3k2lBP8Kce8hz91RGNQSAe5KQS3OsoJ3qhz3kMsCukgN4XgdkXnPZTWQc4UZKOIeQ+LIRaLsbOzM1G4NAsLC+L33d2d2D47OyvasbOzwy4uLtjg4CDzer3s/f097zqUFLyYeQ+LwegOcg59D0dGOR9udUhwZEhwZEhwZEhwZEhwZEhwZEhwZEhwZEhwZEhwwOUvlm2YF5ryXuwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFwAAABZCAYAAACzIkPrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAABGxJREFUeJztnM1LKlEUwI/2cIrog5A0KclF6xaB0bqgVdRfULs2baKdC2sTGAURhdCyXUqL6g+IoE0f0ActAimIEMLCRWpRCXof9/Ic9CXvOTpzvDOeH1xoRmfm8uvOnePce66NMcaAQMOOdymChNcBauHIkHBkSDgyJBwZEo4MCUeGhCNDwq0iPBwOQ39/PzQ3N8Pw8DBcXFwYdSlTYTPiXUo0GoXp6WnY3t4Wsjc2NmBvbw9isRh0d3f/89h8Pg/Pz8/Q1tYGNpsNZIVry2Qy4PF4wG7X0G6ZAfj9fjY3N6du53I55vF4WCgU+u+x8XicNwDTFF5fLejepWSzWbi8vISxsTF1H28BfPv09PTH97+/vyGdTqvFbC8v+Z2oBd2FJ5NJyOVy4HK5Svbz7UQi8eP7oVAIOjo61OL1esFMaO326h6lBAIBSKVSaonH42Blful9QqfTCU1NTfDy8lKyn2+73e4f31cURZRGQfcW7nA4YGhoCI6OjkoiD749MjKi9+XMBzOASCTCFEVhOzs77O7ujs3OzrLOzk6WSCT+e2wqlap75KGl8PpqwRDhnK2tLeb1epnD4RBh4tnZWUXHWV24IT98aiGdTotoxSzwB317e7t5opRGg4QjQ8KRIeHIkHBkSDgyJBwZEo4MCUeGhCNDwpEh4ciQcGRIODIkHBkSjgwJR4aEI0PCzT4vpV5jhWYZe6UWjgwJR4aEI0PCZRd+cnICExMTYuY/n6p7cHBQ8jmfV7S4uAg9PT3Q0tIi5oXf39/rWefGEv7x8QGDg4Mih6ccq6ursLm5KdJNzs/PobW1FcbHx+Hr60uP+pofVgP88P39fXU7n88zt9vN1tbW1H1vb29iYufu7q6muYVa5+xhU209de3DHx8fRZZDcboJj1V5YlW5dJNyKSe8WBldhRdSSipNNymXctLX1wdWpu5RSqDBUk50FV5IKak03YTD0034T/jiYmV0Fe7z+YTY4nQT3ifzaIXSTap8efX+/g4PDw8lD8qbmxvo6uoSKX/z8/OwvLwMAwMD4h8QDAZFzD41NVXR+Qv5AbI/PAv105zPoDUcOj4+Lpt6MTMzo4aGwWCQuVwuEQ6Ojo6yWCxW8fnjFs9Eli7lJP8n155Xi98x/CFa736dt2YePRXXpdpce+neh9vtdujt7VVvWZkepH/Xhd6Hm4C6x+GNhrTCFUWBpaUlKdLC9ayLdA9NqyNtC7cqJBwZEo4MCUeGhCMjrfBwHdY9xBggl1J4NBqFhYUFEfteXV2JQWs+EP36+mrodVEGyJmE+GtY91AvjBgg50jXwrMa1z3EopoB8nJIJzypcd1DLKoZIDeFcKsjnXCnxnUPsahmgNwUwh2Srnuo2wA5k5BIDese1kImk2HX19eicDXr6+vi76enJ/H5ysqKqMfh4SG7vb1lk5OTzOfzsc/Pz4qvIaXwWtY9rAWjB8g59D4cGen6cKtDwpEh4ciQcGRIODIkHBkSjgwJR4aEI0PCkSHhgMtvAI9W/Pz46yYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFwAAABZCAYAAACzIkPrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAABIpJREFUeJztnMtLckEUwI/24S2iBxFpUpKL1i0Co3VBq6i/oHZt2kQ7FxZBYBREFEHLdiUtqj8ggjY9oActIimIEMKiRWbRA3Q+ZkixLz/zXucex+v5wUD3eh/Tz3Hu0ZkzNsYYAwINO96tCBJeBKiFI0PCkSHhyJBwZEg4MiQcGRKODAm3ivDl5WVoa2uDyspK6OrqgqOjI7NuVVLYzPgtJRQKwdDQEKysrAjZCwsLsLGxAeFwGJqamnKem0wm4e7uDmpqasBms4GqcG3xeBzcbjfY7TraLTMBn8/HRkdH09uJRIK53W4WDAZ/PTcSifAGUDKF11cPf2S/85+fn3B8fAx+vz+9j7eA3t5e2N/f/3H8x8eHKBkNQFpdYrGYofPq6uryPpZ/EvUgXfjj4yMkEglwOp3f9vPty8vLH8cHg0GYmpoCM6itrQWz0dvtFT1K8fv9oiWmSiQSASsjvYU3NjZCRUUF3N/ff9vPt10u14/jNU0TpVyQ3sIdDgd0dnbCzs7Ot8iDb3d3d8u+XenBTGB9fZ1pmsZWV1fZxcUFGxkZYfX19Swajf56biwWkxZBGEXPPXh9dV2bmcTS0hLzeDzM4XCIMPHg4CCv86wu3JQvPoXw/PysKyzLhdF/TU/kwR/0eqKhokcp5QYJR4aEI0PCkSHhyJBwZEg4MiQcGRKODAlHhoQjQ8KRIeHIkHBkSDgyJBwZEo4MCUeGhCNDwpEh4ciQcGRIODIkXHXhe3t70N/fL2b+8wkzW1tbPybfTExMQHNzM1RVVYl54VdXVzLrXF7CX19foaOjQ+TwZGN2dhYWFxdFusnh4SFUV1dDX18fvL+/y6hv6WN4At7XHLzNzc30djKZZC6Xi83NzaX3PT09iYmda2treV3T6nMLpfbhNzc3EI1GRTeSgs8T5IlV2dJNODzdhM8nzCxWRqpwLpuTLd0k9Vq2lBP+pqRKa2srWJmiRyn+Mks5kSo8lVKSb7oJh6eb8Om+mcXKSBXu9XqF2Mx0E94n82iF0k0MJlW9vLzA9fX1twfl2dkZNDQ0gMfjgbGxMZienob29nbxBgQCARGzDw4O5nV9mfkBGA9g3fXVGzLt7u5mDY+Gh4fToWEgEGBOp1OEgz09PSwcDud9/YjFM5GVSzlJfuXa82rxTwx/iBa7X+efFB49ZdbFaK699DzNQrHb7dDS0pLuDlR6kP5bFyO5SEUPC8sNEo6MssI1TYPJyUkl0sJl1kW5h6bVUbaFWxUSjgwJR4aEI0PCkVFW+HIR1j3EGCBXUngoFILx8XER+56cnIhBaz4Q/fDwYOp9UQbImYL4Clj3UBZmDJBzlGvhn1/rHmYOROda9xALIwPk2VBO+GOOdQ//NxCNgZEB8pIQbnWUE96oc91DLIwMkJeEcIei6x5KGyBnCrJewLqHhRCPx9np6akoXM38/Lz4+/b2Vrw+MzMj6rG9vc3Oz8/ZwMAA83q97O3tLe97KCm8kHUPC8HsAXIO/R6OjHJ9uNUh4ciQcGRIODIkHBkSjgwJR4aEI0PCkSHhyJBwwOUvr+ZTHQ1MrRsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFwAAABZCAYAAACzIkPrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAABGtJREFUeJztnM1LKlEUwI/2cIooIyRNSnLRukVgtC5oFfUX1K5Nm2jnwtoERkFEIbRsl9Ki+gMiaNMH9EGLQAoihLBwkVpUgt7HvbwGfcl7js4c74znBxea0Zm5/LreOTP3nmtjjDEg0LDjXYog4XWAWjgyJBwZEo4MCUeGhCNDwpEh4ciQcKsIj0Qi0NfXB83NzTA0NATn5+dGXcpU2Ix4lxKLxWBqagq2traE7PX1ddjd3YV4PA5dXV3/PLZQKMDT0xO0tbWBzWYDWeHastkseL1esNs1tFtmAIFAgM3Ozqrb+Xyeeb1eFg6H/3tsIpHgDcA0hddXC7p3KblcDi4uLmB0dFTdx1sA3z45Ofnx/a+vL8hkMmox28tL/kvUgu7CU6kU5PN5cLvdJfv5djKZ/PH9cDgMTqdTLT6fD8yE1m6v7lFKMBiEdDqtlkQiAVbml94ndLlc0NTUBM/PzyX7+bbH4/nxfUVRRGkUdG/hDocDBgcH4fDwsCTy4NvDw8N6X858MAOIRqNMURS2vb3Nbm9v2czMDOvo6GDJZPK/x6bT6bpHHloKr68WDBHO2dzcZD6fjzkcDhEmnp6eVnSc1YUb8uBTC5lMRkQrZoHf6Nvb280TpTQaJBwZEo4MCUeGhCNDwpEh4ciQcLO/vKrXA4VZHtCohSNDwpEh4ciQcGRIODIkHBkSjgwJR4aEI0PCkSHhyJBwZEg4MtK+LXSaaKqEFqiFyy78+PgYxsfHxcx/PlV3f3+/5HM+r2hhYQG6u7uhpaVFzAu/u7vTs86NJfz9/R0GBgZEDk85VlZWYGNjQ6SbnJ2dQWtrK4yNjcHn56ce9TU/rAb44Xt7e+p2oVBgHo+Hra6uqvteX1/FxM6dnZ2Kzmn1uYW69uEPDw8iy6E43YTf/HhiVbl0k3IpJ7xYGV2Ff6eUVJpuUi7lpLe3F6xM3aOUYIOlnOgq/DulpNJ0Ew5PN+Gj88XFyugq3O/3C7HF6Sa8T+bRCqWbVPmk+fb2Bvf39yU3yuvra+js7BQpf3Nzc7C0tAT9/f3iHxAKhUTMPjk5WdH5JcsP0L++mmIaxtjR0VHZ8Gh6eloNDUOhEHO73SIcHBkZYfF4vOLzJyyeiSxdyknhT649rxb/xfCbaL37dd4t8uipuC7V5tpL9/LKbrdDT0+PGo/LdCP9uy401c0E1D0ObzSkFa4oCiwuLkqRFq5nXaS7aVodaVu4VSHhyJBwZEg4MiQcGWmFR+qw7iHGALmUwmOxGMzPz4vY9/LyUgxa84Hol5cXQ6+LMkDOJCRQw7qHemHEADlHuhae07juIRbVDJCXQzrhKY3rHmJRzQC5KYRbHemEuzSue4hFNQPkphDukHTdQ90GyJmERGtY97AWstksu7q6EoWrWVtbE38/Pj6Kz5eXl0U9Dg4O2M3NDZuYmGB+v599fHxUfA0phdey7mEtGD1AzqH34chI14dbHRKODAlHhoQjQ8KRIeHIkHBkSDgyJBwZEo4MCQdcfgPm0dc/QoheLwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFwAAABZCAYAAACzIkPrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAABL5JREFUeJztnDtLK0EUx0/iJVHEByImBg2msLYQFGsFK9FPoJ2NjdiliCIIEQURRbC002ChfgARbHyADyzEoCASkCgWxig+IJnLDCYkmpub3cyezG7ODwbczW5m/Ofs2bN75oyNMcaAQMOO1xVBgpcAsnBkSHBkSHBkSHBkSHBkSHBkSHBkSHCrCL6ysgJtbW1QWVkJ3d3dcHx8bFRXpsJmxLuUUCgEw8PDsLq6KsReXFyEzc1NCIfD0NTUlPfcZDIJ9/f3UFNTAzabDVSFyxaPx8Hj8YDdrsFumQF0dXWxsbGx9HYikWAej4cFg8H/nhuJRLgBmKbx8Wrhj+xf/uvrC05OTsDv96f3cQvo6+uDg4ODX8d/fn6KlmEAuvuOxWIgg7q6uoKP5VeiFqQL/vT0BIlEAlwuV9Z+vn11dfXr+GAwCNPT01L6rq2tBWy0ur2SRyl+v19YZqpFIhFN5/MrItXMgHQLb2xshIqKCnh4eMjaz7fdbvev451Op2jlgnQLdzgc0NnZCbu7u1mRB9/u6emR3Z35YAawsbHBnE4nW1tbY5eXl2x0dJTV19ezaDT633NjsVjeqMAo9EYpfLya+jHqH1heXmZer5c5HA4RJh4eHhZ0ntUFN+TBpxheXl7yhmVGDVfvQxa/0WuJjkoepZQb0qMUI2AGWHWpXhuQhSNDgiOjrOCxWEzqEyR3IZmtVCgruFUhwZEhwZExRVioFxUzRmThyJDgyFjKpdgUdCE/IQtHhgRHhgRHxvQ+3GYCv50JWTgyJDgyJDgypvPhNpP57J+QhSNDgiOjrEup0zCD1UyQhasu+P7+PgwMDIiZ//wGtr29nfU5z0FOTk5Cc3MzVFVViXnh19fXMsdcXoK/vb1BR0eHqOHJxdzcHCwtLYlyk6OjI6iurob+/n74+PiQMV7zU+x8vK2trfR2Mplkbrebzc/Pp/c9Pz+LiZ3r6+tS5haq1rTOLZTqw29vbyEajQo3knnz44VVucpNOLzchM8nzGxWRqrgXGxOrnKT1Ge5Sk74j5Jqra2tYGVMX3JS1oKnSkoKLTfh8HITPt03s1kZqYL7fD4hbGa5CffJPFqhchOdT5qvr69wc3OTdaM8Pz+HhoYG8Hq9MD4+DjMzM9De3i5+gEAgIGL2oaGhgr5fsfoA+ePVFNMwxvb29nKGRyMjI+nQMBAIMJfLJcLB3t5eFg6HC/7+iMUrkZUrOUl+19rzYfErht9ES+3XuVvk0VPmWPTW2iv38sput0NLS0s6HlfpRvpzLHpesJU8LCw3SHBklBXc6XTC1NSUEmXhMsei3E3T6ihr4VaFBEeGBEeGBEeGBEdGWcFXSrDuIUaCXEnBQ6EQTExMiNj39PRUJK15Ivrx8dHQflES5ExBuopY91AWRiTIOcpZ+Nf3uoeZieh86x5ioSdBngvlBH/Ks+7hvxLRGOhJkJtCcKujnOCNGtc9xEJPgtwUgjsUXfdQWoKcKchGEeseFkM8HmdnZ2eicWkWFhbE33d3d+Lz2dlZMY6dnR12cXHBBgcHmc/nY+/v7wX3oaTgxax7WAxGJ8g59D4cGeV8uNUhwZEhwZEhwZEhwZEhwZEhwZEhwZEhwZEhwZEhwQGXvwYka1asQorZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFwAAABZCAYAAACzIkPrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAABKVJREFUeJztnD1LM0EQxyfxIaeIL4iYGDSYwtpCUKwVrEQ/gXY2NmJnEW2EiIKIIljaabBQP4AINr6AL1gIQUEkIFEsTKL4Ask+7KJHNFFzl7vJ7jk/WHDP27v179zs3O3OuhhjDAg03Hi3IkjwEkAWjgwJjgwJjgwJjgwJjgwJjgwJjgwJ7hTBl5aWoKWlBcrLy6GzsxMODw/tupVSuOz4lhKJRGBwcBCWl5eF2PPz87C+vg7RaBQaGhp+bJvJZODm5gaqqqrA5XKBrHDZUqkU+P1+cLsN2C2zgY6ODjYyMqLX0+k08/v9LBwO/9o2FotxA1Cm8P4awXKX8vb2BkdHR9DT06Mf4xbA63t7eznnv76+QjKZ1ItqHy/5k2gEywW/v7+HdDoNXq/303Fej8fjOeeHw2GoqanRSyAQAJUw6vZKHqWMj49DIpHQSywW+/F8/gR8V1Tgn9UXrK+vh7KyMri9vf10nNd9Pl/O+ZqmifJXsNzCPR4PtLe3w/b29qfIg9e7urqsvp16MBtYW1tjmqaxlZUVdn5+zoaHh1ltbS2Lx+O/tk0kEjmRgFkwohTeX0N9YjaxuLjIAoEA83g8Ikzc398vqJ3TBbflxacYksmkiFayMdtFjBcnPtBXV1erE6X8NSyPUmSCZT0ZsnwmIAtHhgRHRgmX4spyB2YH0K/tSuViyMKRIcGRIcGRUcKH/+R7VfPpZOHIkODIKOdS7AgZv7a1072QhSNDgiNDgiOjvA9XLWQkC0eGBEeGBEfGUT5chRidLBwZEhwZR7sUO0LGYiELl13w3d1d6OvrEyv/udVsbm7mWM7ExAQ0NjZCRUWFWBd+cXFhZZ//luBPT0/Q1tYmcnjyMTMzAwsLCyLd5ODgACorK6G3txdeXl6s6K/6mF649752b2NjQ69nMhnm8/nY7Oysfuzh4UEs7FxdXTW9thCj/PZ3WrW20FIffnV1JbIcstNN+DpBnliVL90kX8oJL07GUsE/UkoKTTfJl3LS3NwMTka5lBO74AHAd0VawT9SSgpNN+HwdBO+3De7OBlLBQ8Gg0LY7HQT7pN5tELpJibfNB8fH+Hy8vLTQHl6egp1dXUi5W90dBSmpqagtbVV/ANCoZCI2QcGBgq6vmT5Adb311BMwxjb2dnJGx4NDQ3poWEoFGJer1eEg93d3SwajRZ8/ZjDM5GlSznJvOfa827xJ4YPoqX269wt8ugpuy9mc+2l+3jldruhqalJj8dlGki/9uVrLpISYeFfgwRHRlrBNU2DyclJKdLCreyLdIOm05HWwp0KCY4MCY4MCY4MCY6MtIIvlWDfQ4wJcikFj0QiMDY2JmLf4+NjMWnNJ6Lv7u5svS/KBDmTkI4i9j20CjsmyDnSWfibwX0PsTAzQZ4P6QS/N7jvIRZmJsiVENzpSCd4vcF9D7EwM0GuhOAeSfc9tGyCnEnIWhH7HhZDKpViJycnonBp5ubmxM/X19fi99PT06IfW1tb7OzsjPX397NgMMien58LvoeUghez72Ex2D1BzqHv4chI58OdDgmODAmODAmODAmODAmODAmODAmODAmODAmODAkOuPwHRJ91wrcmcP4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFwAAABZCAYAAACzIkPrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAABGxJREFUeJztnDtLK0EUgE/iJauIRiSYGDSYwtpCUKwVrER/gXY2NmJnEW2EiIKIIljaabBQf4AINj7ABxZCUBAJSBQLkyg+IJnLGa6LuYZ7s8nuyex6PhhwN9ns8DmZPZmZMy4hhACGDDfdrRgWXgG4hRPDwolh4cSwcGJYODEsnBgWTgwLd4rwlZUVaGtrg+rqauju7obj42OrbmUrXFaMpcRiMRgeHobV1VUpe3FxETY3NyEej0NTU9M/r83lcnB3dwd1dXXgcrlAVVBbJpOBYDAIbreBdissoKurS4yNjenH2WxWBINBEY1G/3ttIpHABmCbgvU1guldysfHB5ycnEBfX59+DlsAHh8cHHx7//v7O6TTab3YbfASv4lGMF344+MjZLNZ8Pv9eefxOJlMfnt/NBoFr9erl1AoBHbCaLdX8ShlcnISUqmUXhKJBDiZX2Z/oM/ng6qqKri/v887j8eBQODb+zVNk+WnYHoL93g80NnZCbu7u3mRBx739PSYfTv7ISxgY2NDaJom1tbWxOXlpRgdHRUNDQ0imUz+99pUKlXxyMNIwfoawRLhyPLysgiFQsLj8cgw8fDwsKjrnC7ckh8+5ZBOp2W0YhfwQV9fX2+fKOWnwcKJYeHEsHBiWDgxLJwYFk4MCyeGhRPDwolh4cSwcGJYODEsnBgWbvc5zUqNM9tl3J5bODEsnBgWTgwLJ4aFE8PCiWHhxLBw1YXv7+/DwMCAXPmPS3W3t7fzXsd1RVNTU9Dc3Aw1NTVyXfjV1ZWZdf5Zwl9eXqCjo0Pm8BRibm4OlpaWZLrJ0dER1NbWQn9/P7y9vZlRX/sjygAv39ra0o9zuZwIBAJifn5eP/f09CQXdq6vrxtaW2h0zR41pdbT1D785uZGZjl8TTfB8QZMrCqUblIo5QSLkzFV+GdKSbHpJoVSTlpbW8HJKJty4vV65UNZ1VLqCl9ThX+mlBSbboJgugkOw34tTsZU4eFwWIr9mm6CfTJGK5xuUuIExPPzM1xfX+c9KM/Pz6GxsVGm/I2Pj8PMzAy0t7fLf0AkEpEx+9DQUFGfr1h+gPn1NRoO7e3tFUy9GBkZ0UPDSCQi/H6/DAd7e3tFPB4v+vMTDs9EVi7lJPcn1x6rhd8YfIhWul/HbhGjp691KTXXXrk5TbfbDS0tLXo8rtKD9O+68JymDah4HP7TUFa4pmkwPT2tRFq4mXVR7qHpdJRt4U6FhRPDwolh4cSwcGKUFb5SgX0PKSbIlRQei8VgYmJCxr6np6dy0honoh8eHiy9L8kEuVCQrjL2PTQLKybIEeVa+IfBfQ+pKGWCvBDKCX80uO8hFaVMkNtCuNNRTrjP4L6HVJQyQW4L4R5F9z00bYJcKMhGGfselkMmkxFnZ2eyoJqFhQX59+3trXx9dnZW1mNnZ0dcXFyIwcFBEQ6Hxevra9H3UFJ4OfseloPVE+QIj4cTo1wf7nRYODEsnBgWTgwLJ4aFE8PCiWHhxLBwYlg4MSwcaPkNMApSHwaiuWYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFwAAABZCAYAAACzIkPrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAABIhJREFUeJztnM1LMkEYwB/txS2iDyLSpCQPnTsEReeCTlF/Qd26dIluHiyCwCiIKIKO3Uo6VH9ABF36gD7oEElBhBAWHTKLPkDnZYYUe/Mtd519dlyfHwy4q+sMP8fZR2eecTDGGBBoOPGqIki4BVAPR4aEI0PCkSHhyJBwZEg4MiQcGRJuF+FLS0vQ0tIC5eXl0NnZCYeHh2ZVVVQ4zPgvJRwOw+DgICwvLwvZ8/PzsL6+DpFIBBoaGn68NpVKwe3tLVRVVYHD4QBV4doSiQR4vV5wOnX0W2YCHR0dbGRkJHOcTCaZ1+tloVDo12uj0SjvAEVTeHv18Ef2J//x8QFHR0cQCAQy53gP6Onpgb29vW+vf39/FyWrA+iqLx6PG2pnTU0NyIB/E/UgXfjDwwMkk0lwu91fzvPji4uLb68PhUIwOTlpuL7q6mqwEr3DnuVRSiAQEL00XaLRKNgZ6T28vr4eysrK4O7u7st5fuzxeL69XtM0UUoF6T3c5XJBe3s7bG9vf4k8+HFXV5fs6ooPZgJra2tM0zS2srLCzs/P2fDwMKutrWWxWOzXa+PxuK4owSiyohTeXl31MpNYXFxkPp+PuVwuESbu7+/ndZ3dhZvyw6cQnp6edIVsRpsv60cVv9HriZQsj1JKDRKODAlHhoQjQ8KRIeHIkHBkSDgyJBwZEo4MCUeGhCNDwpEh4ciQcGRIODIkHBkSjgwJR4aEI0PCkSHhyJBwZEi46sJ3d3ehr69PrPzni2k2Nze/LcwZHx+HxsZGqKioEOvCLy8vZba5tIS/vLxAW1ubyOHJxczMDCwsLIh0k4ODA6isrITe3l54e3uT0d7ix/DivM/1eRsbG5njVCrFPB4Pm52dzZx7fHwUCztXV1fzek+7ry2UOoZfX19DLBYTw0gavk6QJ1blSjfh8HQTvp4wu9gZqcK5bE6udJP0c7lSTviHki7Nzc1gZyyPUgIllnIiVXg6pSTfdBMOTzfhy32zi52RKtzv9wux2ekmfEzm0QqlmxhMqnp+foarq6svN8rT01Ooq6sDn88Ho6OjMDU1Ba2treIDCAaDImYfGBgwZYG91TdZ3QkBesOpnZ2dnOHR0NBQJjQMBoPM7XaLcLC7u5tFIpG83z9q80xk5VJOUp+59rxZ/BvDb6JWj+v8W8Sjp+y2GM21l56nWShOpxOampoyQ4VKN9J/22IkfdzysLDUIOHIKCtc0zSYmJhQIi1cZluUu2naHWV7uF0h4ciQcGRIODIkHBllhS9ZsO8hxgS5ksLD4TCMjY2J2Pf4+FhMWvOJ6Pv7e1PrRZkgZwrSUcC+h7IwY4Kco1wP//jc9zB7IvqnfQ+xMDJBngvlhD/8sO/h/yaiMTAyQV4Uwu2OcsLrde57iIWRCfKiEO5SdN9DaRPkTEHWCtj3sBASiQQ7OTkRhauZm5sTj29ubsTz09PToh1bW1vs7OyM9ff3M7/fz15fX/OuQ0nhhex7WAhmT5Bz6P9wZJQbw+0OCUeGhCNDwpEh4ciQcGRIODIkHBkSjgwJR4aEAy5/AWb/Ux21pOENAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFwAAABZCAYAAACzIkPrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAABGpJREFUeJztnM1LKlEUwI/2cIooIyRNSnLRukVgtC5oFfUX1K5Nm2jnwtoERkFEIbRsl9Ki+gMiaNMH9EGLQAoihLBwkVpUgt7HvbwGfcl7js4c74znBxeacT4uv653jnPvuTbGGAMCDTverQgSXgeohSNDwpEh4ciQcGRIODIkHBkSjgwJt4rwSCQCfX190NzcDENDQ3B+fm7UrUyFzYh3KbFYDKampmBra0vIXl9fh93dXYjH49DV1fXPcwuFAjw9PUFbWxvYbDaQFa4tm82C1+sFu11Du2UGEAgE2OzsrLqdz+eZ1+tl4XD4v+cmEgneAExTeH21oHuXksvl4OLiAkZHR9V9vAXw7ZOTkx/Hf319QSaTUYvZXl7yb6IWdBeeSqUgn8+D2+0u2c+3k8nkj+PD4TA4nU61+Hw+MBNau726RynBYBDS6bRaEokEWJlfel/Q5XJBU1MTPD8/l+zn2x6P58fxiqKI0ijo3sIdDgcMDg7C4eFhSeTBt4eHh/W+nflgBhCNRpmiKGx7e5vd3t6ymZkZ1tHRwZLJ5H/PTafTdY88tBReXy0YIpyzubnJfD4fczgcIkw8PT2t6DyrCzfkh08tZDIZEa2YBf6gb29vN0+U0miQcGRIODIk3Ow/fOr1MDLLw51aODIkHBkSjgwJR4aEI0PCkSHhyJBwZEg4MiQcGRKODAlHhoQjQ8KRkfb1rNNE45paoBaODAlHhoQjQ8JlF358fAzj4+Ni5j+fqru/v1/yOZ9XtLCwAN3d3dDS0iLmhd/d3elZ58YS/v7+DgMDAyKHpxwrKyuwsbEh0k3Ozs6gtbUVxsbG4PPzU4/6mh9WA/z0vb09dbtQKDCPx8NWV1fVfa+vr2Ji587OTkXXtPrcQl378IeHB5HlUJxuwuNpnlhVLt2kXMoJL1ZGV+HfKSWVppuUSznp7e0FK1P3KCXYYCknugr/TimpNN2Ew9NN+Ayr4mJldBXu9/uF2OJ0E94n82iF0k2qfHn19vYG9/f3JQ/K6+tr6OzsFCl/c3NzsLS0BP39/eIfEAqFRMw+OTlZ0fUlyw/Qv76aYhrG2NHRUdnwaHp6Wg0NQ6EQc7vdIhwcGRlh8Xi84usnLJ6JLF3KSeFPrj2vFv/G8Idovft13i3y6Km4LtXm2kv3Ptxut0NPT48aj8v0IP27LjRd2QTUPQ5vNKQVrigKLC4uSpEWrmddpHtoWh1pW7hVIeHIkHBkSDgyJBwZaYVH6rDuIcYAuZTCY7EYzM/Pi9j38vJSDFrzgeiXlxdD74syQM4kJFDDuod6YcQAOUe6Fp7TuO4hFtUMkJdDOuEpjeseYlHNALkphFsd6YS7NK57iEU1A+SmEO6QdN1D3QbImYREa1j3sBay2Sy7uroShatZW1sTfz8+PorPl5eXRT0ODg7Yzc0Nm5iYYH6/n318fFR8DymF17LuYS0YPUDOoffhyEjXh1sdEo4MCUeGhCNDwpEh4ciQcGRIODIkHBkSjgwJB1x+A1ey1z8xeKZIAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 1000\n",
    "Liste = [(0,0)]*N \n",
    "images = [0]*N\n",
    "labels = [0]*N\n",
    "for i in range(0, N): \n",
    "    Liste[i]= generate_image(12, 12)\n",
    "    images[i]=Liste[i][0]\n",
    "    labels[i]=Liste[i][1]\n",
    "\n",
    "images= torch.stack(images, dim=0)\n",
    "labels = torch.LongTensor(labels)\n",
    "\n",
    "\n",
    "#la dimension de images (Tensor de taille n*channels*heigth*width)\n",
    "#ici 1000*1(niveaux de gris)*12*12\n",
    "#labels : Tensor d'ordre n avec des entiers (long tensor)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(images, labels)\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "for i in range (1, 11):\n",
    "    plt.subplot(1, 10, i)\n",
    "    plt.imshow(images[i].squeeze(), cmap = \"gray\") #supprime les dimensions égales à 1 (channel)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "414469c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correction\n",
    "n = 1000\n",
    "height = 12\n",
    "width = 12\n",
    "\n",
    "images = torch.zeros(n, 1, height, width, device = device)\n",
    "classes = torch.LongTensor(n).to(device)\n",
    "\n",
    "for i in range(n):\n",
    "    images[i], classes[i] = generate_image(height, width)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(images, classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb47af6",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Put the dataset into a `DataLoader`. What are the differences between a dataset and a data loader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1923cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5084f3f2",
   "metadata": {},
   "source": [
    "What are Conv2d, Linear and MaxPool?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c149d2",
   "metadata": {},
   "source": [
    "Con2d --> Idée : les masques représentent la reconnaissance d'un **motif local dans l'image**, quand il rencontre le même motif sur l'image, on a un pixel qui va être de grande valeur et inversement, si le motif n'est pas reconnu, le pixel renvoyé sera d'une plus faible valeur. Les réseaux de neurones convolutionnels fonctionnent en **local**, c'est pour ça qu'ils sont efficaces pour les images naturelles. \\\n",
    "Quand on empile les couches de convolutions, on considère des éléments étendus dans l'image pour vu de faire des masques plus grands, ou que l'on empile plusieurs couches. \\\n",
    "Une couche FC serait mieux mais il y a trop de paramètres et ce ne sera pas de l'information locale. \\\n",
    "C'est uniquement pour un canal de sortie et un canal d'entrée (donc il faut multiplier l'opération par le nombre de canaux d'entrée et de sortie). \\\n",
    "**Idée générale** : commencer par 3 canaux (RGB), et augmenter le nombre de canaux (jusqu'à 64) et le mettre dans une couche FC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f2633b",
   "metadata": {},
   "source": [
    "In a dataset, the elements can be accessed individually given an index (with `[i]`). \n",
    "\n",
    "A data loader is an `Iterable`, which means that its elements can only be accessed through an `Iterator` that traverses the dataset in a way that is provided by the arguments of the data loader (batch size, random/deterministic choice of the data points, etc.). In particular, it can be traversed with a `for` loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b077372d",
   "metadata": {},
   "source": [
    "## Simple NN/training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a580ae6",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Write a neural network that classifies the data points previously generated. One can use a convolutional and a fully-connected layer.\n",
    "\n",
    "What are the parameters of a convolutional layer? What is their shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "83e6ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCVNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallCVNN, self).__init__()\n",
    "\n",
    "        self.act_function = torch.relu\n",
    "        layer = [1, 6, 4]\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(layer[0], layer[1], 5)\n",
    "        self.fc = torch.nn.Linear(4*4*layer[1], layer[2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.act_function(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc(x)\n",
    "        x = torch.nn.functional.log_softmax(x, dim = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed3549d",
   "metadata": {},
   "source": [
    "Parameters: `conv.bias`, `conv.weight`.\n",
    "\n",
    "`conv.bias`: size `out_channels`\n",
    "\n",
    "`conv.weight`: size `out_channels` * `in_channels` * `kernel_height` * `kernel_width`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f81f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [128, 144]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m         \u001b[38;5;66;03m# print losses statistics \u001b[39;00m\n\u001b[32m     50\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mTraining Loss: \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mTraining accuracy: \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m     51\u001b[39m             epoch, train_loss, accuracy))\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, criterion, optimizer, nepochs)\u001b[39m\n\u001b[32m     22\u001b[39m optimizer.zero_grad()\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# forward pass: compute predicted outputs by passing inputs to the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# calculate the batch loss\u001b[39;00m\n\u001b[32m     28\u001b[39m loss = criterion(output, target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\camil\\Desktop\\L2\\TP_Deep_Learning_L3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\camil\\Desktop\\L2\\TP_Deep_Learning_L3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mSmallCVNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     12\u001b[39m     x = x.view(x.size(\u001b[32m0\u001b[39m),-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.act_function(x)\n\u001b[32m     15\u001b[39m     x = torch.nn.functional.max_pool2d(x, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\camil\\Desktop\\L2\\TP_Deep_Learning_L3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\camil\\Desktop\\L2\\TP_Deep_Learning_L3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\camil\\Desktop\\L2\\TP_Deep_Learning_L3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:553\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\camil\\Desktop\\L2\\TP_Deep_Learning_L3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    537\u001b[39m         F.pad(\n\u001b[32m    538\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    545\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    546\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [128, 144]"
     ]
    }
   ],
   "source": [
    "def train_model(model, criterion, optimizer, nepochs):\n",
    "    #List to store loss to visualize\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    start_epoch = 0\n",
    "\n",
    "    for epoch in range(start_epoch, nepochs):\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "        correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            # accuracy\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(data_loader.dataset)\n",
    "        accuracy = correct/len(data_loader.dataset)*100\n",
    "        train_acc.append(accuracy)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # print losses statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining accuracy: {:.6f}'.format(\n",
    "            epoch, train_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd6303",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "Choose the loss and train the NN. Why do we choose to minimize the loss instead of maximizing the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2189d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [128, 144]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m optimizer = optim.Adam(model.parameters())\n\u001b[32m     10\u001b[39m nepochs = \u001b[32m10\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, criterion, optimizer, nepochs)\u001b[39m\n\u001b[32m     22\u001b[39m optimizer.zero_grad()\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# forward pass: compute predicted outputs by passing inputs to the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# calculate the batch loss\u001b[39;00m\n\u001b[32m     28\u001b[39m loss = criterion(output, target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\camil\\Desktop\\L2\\TP_Deep_Learning_L3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\camil\\Desktop\\L2\\TP_Deep_Learning_L3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mSmallCVNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     12\u001b[39m     x = x.view(x.size(\u001b[32m0\u001b[39m),-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.act_function(x)\n\u001b[32m     15\u001b[39m     x = torch.nn.functional.max_pool2d(x, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\camil\\Desktop\\L2\\TP_Deep_Learning_L3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\camil\\Desktop\\L2\\TP_Deep_Learning_L3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\camil\\Desktop\\L2\\TP_Deep_Learning_L3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:553\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\camil\\Desktop\\L2\\TP_Deep_Learning_L3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    537\u001b[39m         F.pad(\n\u001b[32m    538\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    545\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    546\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [128, 144]"
     ]
    }
   ],
   "source": [
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bce1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SmallCVNN().to(device = device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "nepochs = 10\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c1441d",
   "metadata": {},
   "source": [
    "The derivative of the accuracy w.r.t. the parameters of the NN is zero, so the NN cannot be trained with this \"loss\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314aacf1",
   "metadata": {},
   "source": [
    "# Variational Auto-Encoders (VAE): MNIST and FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd960b53",
   "metadata": {},
   "source": [
    "## Building a standard fully-connected VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54feff84",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "Complete the class `VAE_FC`:\n",
    " * build an encoder module;\n",
    " * build a decoder module;\n",
    " * write the method `encode`, which takes an input `x` and returns means and log-variances;\n",
    " * write the method `decode`, which takes a random variable `z` and returns the reconstructed image;\n",
    " * write the method `reparameterization`, taking means and variances and returning normal samples with these means and variances;\n",
    " * write the method `forward`, which takes an input `x` and returns its reconstruction `x_hat`, and the mean/variance of the latent representation.\n",
    "\n",
    "Additionally:\n",
    " * write the method `sample`, which generates new data;\n",
    " * write the method `reconstruct`, which attempts to reconstruct the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6d49bb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 363kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.73MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.62MB/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "#mean, std = .2860, .3530\n",
    "\n",
    "# build transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ]) \n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(datasets_path, train = True,\n",
    "                              download = True, transform = transform)\n",
    "test_data = datasets.MNIST(datasets_path, train=False,\n",
    "                             download = True, transform = transform)\n",
    "\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "# build the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# specify the image classes\n",
    "classes = [f\"{i}\" for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9007b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#layers: liste de tailles de couches (encoder)\n",
    "layers = [8, 4, 2]\n",
    "lst = [nn.Linear(layers[i], layers[i+1]) for i in range (len(layers)-1)]\n",
    "lst2 = [[lst[i//2] for i in range(len(layers)-1)] if i%2 == 0 else nn.ReLU()]\n",
    "\n",
    "\n",
    "class VAE_FC(nn.Module):\n",
    "    def __init__(self, layers, latent_dim = 200, leak = .1):\n",
    "        super(VAE_FC, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        encoder = []\n",
    "        for l_in, l_out in zip[layers[:-1], layers[:1]]:\n",
    "            encoder +=(nn.Linear(l_in, l_out), nn.LeakyReLU(leak))\n",
    "        self.encoder = nn.Sequential(*encoder) #* permet de dérouler la liste, c'est comme si on écrivait encoder[0], ecnoder[1]...\n",
    "    \n",
    "    \n",
    "    def reparameterization(self, mean, logvar):\n",
    "        pass \n",
    "\n",
    "    def decode(self, x):\n",
    "       \n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(self.f, 1, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Encoder\"\"\"\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(d4)\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "        self.fc_mean = nn.Linear(layers[-1], latent_dim)\n",
    "        self.fc_logvar = nn.Linear(layers[-1], latent_dim)\n",
    "        \n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        pass\n",
    "    \n",
    "    def reconstruct(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff15532",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Write the function `build_loss_vae`, which builds the loss function for the VAE. The reconstruction loss and the KL loss will be balanced by two parameters `lambda_reconstruct` and `lambda_kl` (by default, they are equal to 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed80b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f414fdbe",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a66daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_vae(data_loader, model, criterion, optimizer, nepochs):\n",
    "    #List to store loss to visualize\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    start_epoch = 0\n",
    "\n",
    "    for epoch in range(start_epoch, nepochs):\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "        correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (input_, target) in enumerate(data_loader):\n",
    "            input_ = input_.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            # ...\n",
    "\n",
    "            # calculate the batch loss\n",
    "            # ...\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training loss\n",
    "            train_loss += loss.item() * input_.size(0)\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(data_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # print losses statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0000beea",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Train the VAE on MNIST or FashionMNIST. Don't forget to save it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bbc2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bad9c556",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "With the resulting model:\n",
    " * show some generated samples;\n",
    " * check the quality of the reconstruction;\n",
    " * show some interpolations between images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec17841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9913d9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9082b156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b4850b0",
   "metadata": {},
   "source": [
    "## Conditional VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0757ecf",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "Modify the VAE to build a conditional VAE:\n",
    " * the input vectors should be enriched with a one-hot vector coding for the class;\n",
    " * went sent to the decoder, the latent representation should be enriched with a one-hot vector coding for the class.\n",
    "\n",
    "Modify also the function `train_model_vae`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f35d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d37850",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 20 (2948407004.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 22\u001b[0;36m\u001b[0m\n",
      "\u001b[0;31m    else:\u001b[0m\n",
      "\u001b[0m    ^\u001b[0m\n",
      "\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 20\n"
     ]
    }
   ],
   "source": [
    "def train_model_vae(data_loader, model, criterion, optimizer, nepochs, cond = False):\n",
    "    #List to store loss to visualize\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    start_epoch = 0\n",
    "\n",
    "    for epoch in range(start_epoch, nepochs):\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "        correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (input_, target) in enumerate(data_loader):\n",
    "            input_ = input_.to(device)\n",
    "            target = target.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            if cond:\n",
    "                # ...\n",
    "            else:\n",
    "                # ...\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(input_, output_, mean, logvar)\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training loss\n",
    "            train_loss += loss.item() * input_.size(0)\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(data_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # print losses statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598221b0",
   "metadata": {},
   "source": [
    "**Question 6**\n",
    "\n",
    "Train the model.\n",
    "\n",
    "With the resulting model:\n",
    " * show some generated samples;\n",
    " * check the quality of the reconstruction;\n",
    " * show some interpolations between images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc4241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c0294b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8a1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d488a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd0f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934990c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c317ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aadafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "778afc65",
   "metadata": {},
   "source": [
    "# Variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951a921",
   "metadata": {},
   "source": [
    "The goal is to build a variational inference (VI) model. We will use the family of independent Gaussian distributions, which means that each parameter $\\theta_k$ of the model will be randomly generated w.r.t. a Gaussian distribution $\\mathcal{N}(\\mu_k, \\sigma_k^2)$. The parameters to be learned are the $(\\mu_1, \\sigma_1^2, \\cdots, \\mu_p, \\sigma_p^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7bb15c",
   "metadata": {},
   "source": [
    "To simplify, we will work with a multilayer perceptron. For each pass trhough the model, we will generate randomly the weights and the biases w.r.t. their own Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c084a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "#mean, std = .2860, .3530\n",
    "\n",
    "# build transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ]) \n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(datasets_path, train = True,\n",
    "                              download = True, transform = transform)\n",
    "test_data = datasets.MNIST(datasets_path, train=False,\n",
    "                             download = True, transform = transform)\n",
    "\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "# build the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# specify the image classes\n",
    "classes = [f\"{i}\" for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d621fc",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "Build a `Module` named `LinearVI` that is similar to the `Linear` layer, but contains the means and variances of the independent Gaussian distributions generating the weights and the biases:\n",
    "* the log-variances will be stored instead the variances themselves;\n",
    "* the means/log-variances should be implemented both for the weights and the biases;\n",
    "* the `forward` method should be implemented: it is recommended to start with generating Gaussian noise, and then translate and scale it to obtain the weights and biases; one can use the function `F.Linear` to compute the output;\n",
    "* the attributes `weight_prior_logvar` and `bias_prior_logvar` contain the initialization log-variance for the weighs and for the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a30d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearVI(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        self.weight_prior_logvar = np.log(1 / n_in)\n",
    "        self.bias_prior_logvar = np.log(.01)\n",
    "        \n",
    "        ...\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9fc39",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Build a multilayer perceptron `PerceptronVI` made of fully-connected variational layers (`LinearVI`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c43b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronVI(torch.nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a38ce36",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Complete the train function below. The `loss_kl` is defined as:\n",
    "$$\n",
    "L(\\boldsymbol{\\theta}) = \\lambda \\sum_{k = 1}^p \\left[ \\frac{1}{2} \\log\\left(\\frac{\\sigma_{0k}^2}{\\sigma_k^2}\\right) +\\frac{\\sigma_k^2 + \\mu_k^2}{2 \\sigma_{0k}^2} + \\frac{1}{2}\\right] ,\n",
    "$$\n",
    "where $\\sigma_{0k}^2$ is the variance of the prior distribution on $\\theta_k$ (defined for each layer via `weight_prior_logvar` and `bias_prior_logvar`) and $\\lambda$ is the penalty factor `pen_factor`.\n",
    "\n",
    "It is recommended to add new methods to `LinearVI` and `PerceptronVI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca522212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_vi(model, criterion, optimizer, nepochs, pen_factor = 1/60000):\n",
    "    #List to store loss to visualize\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    start_epoch = 0\n",
    "\n",
    "    for epoch in range(start_epoch, nepochs):\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "        correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss_fit = criterion(output, target)\n",
    "            loss_kl = # ... TODO HERE\n",
    "            loss = loss_fit + loss_kl\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            # accuracy\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        accuracy = correct/len(train_loader.dataset)*100\n",
    "        train_acc.append(accuracy)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # print losses statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining accuracy: {:.6f}'.format(\n",
    "            epoch, train_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea178db",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "Train the model for various $\\lambda$ and plot the posterior distributions obtained for parameters of different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d399b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36042453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
