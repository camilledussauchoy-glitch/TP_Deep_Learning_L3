{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b45f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77d1ead",
   "metadata": {},
   "source": [
    "# Manipulate the device and the precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4be7b2",
   "metadata": {},
   "source": [
    "The point is to understand the basic pytorch structure which is a Tensor, typically a multi-dimensional matrix which contains a certain type 'dtype' data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f110a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_cuda = torch.cuda.is_available()\n",
    "if with_cuda : \n",
    "    device = torch.device(\"cuda\")\n",
    "else : \n",
    "    device = torch.device(\"cpu\") #by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e715b562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1967, -1.3958,  0.3857,  0.0059,  0.6488, -0.1323,  1.8120, -0.8132,\n",
      "         0.2035,  1.8036])\n",
      "tensor([ 1.9082, -0.8696, -0.0415, -0.0483,  2.0312, -0.8687, -0.4263,  0.1675,\n",
      "        -0.7954, -0.9126], dtype=torch.float16)\n",
      "tensor([ 0.7332, -1.3008,  0.9171, -0.9777,  0.3466])\n",
      "tensor([ 0.9038, -0.0241,  0.7373, -0.3384,  1.2129], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, device = device )\n",
    "print(x)\n",
    "\n",
    "x = torch.randn(10, device = device, dtype = torch.float16) #the degree of precision we want to have\n",
    "print(x)\n",
    "\n",
    "x = torch.randn(5, device = device)\n",
    "x = x.to(device)\n",
    "print(x)\n",
    "\n",
    "x = torch.randn(5)\n",
    "x = x.to(device = device, dtype = torch.float16)\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae77d9",
   "metadata": {},
   "source": [
    "# Backpropagation of the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce87444",
   "metadata": {},
   "source": [
    "## Parameters and tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc2c888",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "Build some `torch.nn.Parameter`, some `torch.Tensor` and some `torch.Tensor` with `requires_grad = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebe610c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.1957,  1.4508, -0.9671,  0.1087,  2.3250], requires_grad=True) requires_grad: True\n",
      "Parameter containing:\n",
      "tensor(-0.8408, requires_grad=True) requires_grad: True\n",
      "tensor([0.8455, 0.2452, 0.9908, 0.5523, 0.8587]) requires_grad: False\n",
      "tensor(0.8857)\n",
      "tensor([[-0.6282,  0.5380,  1.2627, -0.9518],\n",
      "        [-1.0271,  0.6482, -1.2423, -0.3486],\n",
      "        [-1.0635,  0.4053,  0.3507, -0.6813]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.nn.Parameter(torch.randn(5))\n",
    "x2 = torch.nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.rand(5)\n",
    "b = torch.rand(1).squeeze()\n",
    "c = torch.randn(3, 4, requires_grad = True)\n",
    "\n",
    "print(x1, \"requires_grad:\", x1.requires_grad)\n",
    "print(x2, \"requires_grad:\", x2.requires_grad)\n",
    "print(a, \"requires_grad:\", a.requires_grad)\n",
    "print(b)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb04799",
   "metadata": {},
   "source": [
    "## Computing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb07f19",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Let $f$ be the function defined below. Compute its derivative with respect to `x1` and `x2` by using `backward`, and then by using `torch.autograd.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772991f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "tensor([0.1504, 0.0669, 0.1562, 0.1780, 0.2217]) tensor(0.8756)\n",
      "<MulBackward0 object at 0x000001EC4D985510>\n",
      "((<AccumulateGrad object at 0x000001EC7C465990>, 0), (<SinBackward0 object at 0x000001EC7C466200>, 0))\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.nn.Parameter(torch.randn(5))\n",
    "x2 = torch.nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.rand(5)\n",
    "b = torch.rand(1).squeeze()\n",
    "\n",
    "y = x2 * torch.sin((a * x1).sum() + b)\n",
    "\n",
    "print(x1.grad, x2.grad)\n",
    "y.backward()\n",
    "print(x1.grad, x2.grad)\n",
    "print(y.grad_fn)\n",
    "print(y.grad_fn.next_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716e0f0",
   "metadata": {},
   "source": [
    "Torch.autograd.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e243f12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.1982, -0.1316, -0.1404, -0.0363, -0.0148]), tensor(-0.2963))\n",
      "tensor(-0.2963)\n",
      "tensor([-0.1982, -0.1316, -0.1404, -0.0363, -0.0148])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.nn.Parameter(torch.randn(5))\n",
    "x2 = torch.nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.rand(5)\n",
    "b = torch.rand(1).squeeze()\n",
    "\n",
    "y = x2 * torch.sin((a * x1).sum() + b)\n",
    "out = torch.autograd.grad(y, (x1, x2)) #with x1 and x2 being two parameters required for the GD\n",
    "\n",
    "print(out)\n",
    "dervwrtx1 = out[0]\n",
    "dervwrtx2 = out[1]\n",
    "\n",
    "print(dervwrtx2)\n",
    "print(dervwrtx1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac125d9f",
   "metadata": {},
   "source": [
    "High order derivatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a06a423",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Compute $\\frac{\\partial f}{\\partial x_1 \\partial x_2}$ by using `torch.autograd.grad` twice. One check how to use the additional parameters `create_graph` and `allow_unused`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "565b8600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.7695, -0.3503,  0.9337,  1.7783,  0.5225]),)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.nn.Parameter(torch.randn(5))\n",
    "x2 = torch.nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "y = x2 * torch.sin((a * x1).sum() + b)\n",
    "\n",
    "grad_x1, grad_x2 = torch.autograd.grad(y, (x1, x2), create_graph = True, allow_unused = True)\n",
    "\n",
    "hessian = torch.autograd.grad(grad_x2, (x1, ))\n",
    "\n",
    "print(hessian)\n",
    "\n",
    "#se renseigner sur comment regarder la taille d'un tensor, autre que sa dimension, peut -etre traduire en \n",
    "#tableau numpy ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f0ad0e",
   "metadata": {},
   "source": [
    "Forwar-only mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f55587d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6873)\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.nn.Parameter(torch.randn(5))\n",
    "x2 = torch.nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x2 * torch.sin((a * x1).sum() + b)\n",
    "\n",
    "print(y)\n",
    "\n",
    "print(x1.grad, x2.grad)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "375dae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.nn.Parameter(torch.randn(5))\n",
    "x2 = torch.nn.Parameter(torch.randn(1).squeeze())\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(1).squeeze()\n",
    "\n",
    "y = x2.detach() * torch.sin((a * x1.detach()).sum() + b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2483cdf",
   "metadata": {},
   "source": [
    "Managing a Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "150d1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = \"/home/camduss/datatsets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cd5f68c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:01<00:00, 9.14MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 207kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.64MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.27MB/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# build transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ]) \n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(datasets_path, train = True,\n",
    "                              download = True, transform = transform)\n",
    "test_data = datasets.MNIST(datasets_path, train=False,\n",
    "                             download = True, transform = transform)\n",
    "\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "# build the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# specify the image classes\n",
    "classes = [f\"{i}\" for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ab1dd",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "By using `numpy.random.choice`, subplots and the function `imshow` of matplotlib, print 10 (or more) random data points of the training set (image + label).\n",
    "\n",
    "Is the task feasible for a human? Do the data look clean ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e4972b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x350 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lst_idx = np.random.choice(train_size, 10, replace = False)\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize = (7.5, 3.5))\n",
    "\n",
    "for i, idx in enumerate(lst_idx): \n",
    "    img, target = train_data[idx]\n",
    "    ax[i//5, i%5].imshow(img.numpy().transpose(1, 2, 0), cmap=\"Greys\")\n",
    "    ax[i//5, i%5].set_title(classes[target])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f402c5",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "In classification tasks, it is common that the different classes are inequally represented in the dataset. If this \"class imbalance\" is too severe, the model is likely to fail to learn well the least represented classes.\n",
    "\n",
    "Compute the number of data points in each class (in the training dataset). What do we observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "94ec3c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])\n"
     ]
    }
   ],
   "source": [
    "nb_data_per_class = 0\n",
    "\n",
    "for img, target in train_loader:\n",
    "    nb_data_per_class+=torch.bincount(target)\n",
    "\n",
    "print(nb_data_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1c8ec",
   "metadata": {},
   "source": [
    "Managing a model /\n",
    "Building a CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2ba85697",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        #define the activation function \n",
    "        self.act_function = torch.tanh\n",
    "        layers = [1, 6, 16, 120, 84, 10]\n",
    "\n",
    "        #on définit les couches de convolutions et d'activation \n",
    "        self.conv1= torch.nn.Conv2d(layers[0], layers[1], 5, padding = 2)\n",
    "        #from fc1 to fc2, we should have that the intput channels of fc2 is the output channels of fc1\n",
    "        self.conv2 = torch.nn.Conv2d(layers[1], layers[2], 5)\n",
    "        #fc1 : size of the kernel times the output of fc2 and layers 3\n",
    "        self.fc1 = torch.nn.Linear(5*5*layers[2], layers[3]) \n",
    "        self.fc2 = torch.nn.Linear(layers[3], layers[4])\n",
    "        self.fc3 = torch.nn.Linear(layers[4], layers[5])\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = self.act_function(x)\n",
    "            x = torch.nn.functional.max_pool2d(x, 2)\n",
    "\n",
    "            x = self.conv2(x)\n",
    "            x = self.act_function(x)\n",
    "            x = torch.nn.functional.max_pool2d(x, 2)\n",
    "\n",
    "            x = x.view(x.size(0), -1)\n",
    "\n",
    "            x = self.fc1(x)\n",
    "            x = self.act_function(x)\n",
    "\n",
    "            x = self.fc2(x)\n",
    "            x = self.act_function(x)\n",
    "\n",
    "            x = self.fc3(x)\n",
    "            x = torch.nn.functional.log_softmax(x, dim=1)\n",
    "\n",
    "            return x\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930162a",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "When a `torch.nn.Module` is created, its parameters and its submodules are automatically registered, which is necessary to optimize them with an `Optimizer`. One can access them with the methods `named_parameters`, `parameters`, `named_modules`, `modules`.\n",
    "\n",
    "Access the various elements on an instance of LeNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fd91d4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('conv1.weight', Parameter containing:\n",
      "tensor([[[[-0.0853,  0.0259, -0.1353, -0.1591,  0.1841],\n",
      "          [-0.0578, -0.1340,  0.1382, -0.0614, -0.1107],\n",
      "          [ 0.0715, -0.1830,  0.1895, -0.0060, -0.0329],\n",
      "          [ 0.0978,  0.1967,  0.1908,  0.1854,  0.1111],\n",
      "          [ 0.1166, -0.0787,  0.0061,  0.0560, -0.1067]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1763,  0.0658,  0.0384, -0.0494, -0.0980],\n",
      "          [-0.0579,  0.0167, -0.0629, -0.1563,  0.0857],\n",
      "          [ 0.1075, -0.1408,  0.1752, -0.1713,  0.1558],\n",
      "          [ 0.1045, -0.1933, -0.1849,  0.0725,  0.1523],\n",
      "          [ 0.1928,  0.1108, -0.0329,  0.1358,  0.0879]]],\n",
      "\n",
      "\n",
      "        [[[-0.0225,  0.0709,  0.1168,  0.0247, -0.1249],\n",
      "          [ 0.0682, -0.0889,  0.0442,  0.0234, -0.0219],\n",
      "          [-0.0119,  0.1589,  0.0332,  0.1510,  0.0026],\n",
      "          [ 0.1021,  0.1403, -0.1616,  0.1320,  0.1635],\n",
      "          [-0.1633, -0.0975, -0.1950, -0.1938,  0.0261]]],\n",
      "\n",
      "\n",
      "        [[[-0.1310, -0.0213, -0.1168,  0.1326,  0.0142],\n",
      "          [-0.1107, -0.1345,  0.0002,  0.0845, -0.1744],\n",
      "          [-0.1335, -0.1718, -0.1341, -0.1297,  0.1698],\n",
      "          [ 0.0946,  0.1710,  0.1697,  0.1459, -0.0926],\n",
      "          [ 0.1898,  0.1420, -0.0659,  0.0733,  0.0317]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0877,  0.0502, -0.0618, -0.0413,  0.1183],\n",
      "          [ 0.1802, -0.0496,  0.1836, -0.0242, -0.1479],\n",
      "          [-0.1537, -0.0741, -0.1641, -0.1254,  0.1039],\n",
      "          [-0.0693, -0.0144,  0.0324,  0.1287, -0.0938],\n",
      "          [ 0.1804, -0.1108, -0.0689, -0.1077,  0.0831]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0527,  0.1926,  0.0053,  0.0525, -0.0321],\n",
      "          [ 0.0120,  0.0013,  0.1825, -0.1574, -0.0424],\n",
      "          [ 0.1855,  0.1606, -0.0261,  0.0691, -0.1108],\n",
      "          [ 0.0399,  0.1221,  0.0359, -0.1821,  0.1601],\n",
      "          [-0.0959,  0.0081,  0.0045, -0.0336,  0.1305]]]], requires_grad=True)), ('conv1.bias', Parameter containing:\n",
      "tensor([-0.1937, -0.1225, -0.1504, -0.0871,  0.1506, -0.0757],\n",
      "       requires_grad=True)), ('conv2.weight', Parameter containing:\n",
      "tensor([[[[ 0.0090,  0.0094,  0.0321, -0.0535, -0.0211],\n",
      "          [-0.0119,  0.0376,  0.0023, -0.0109, -0.0380],\n",
      "          [ 0.0447, -0.0111,  0.0468, -0.0634,  0.0091],\n",
      "          [ 0.0751, -0.0473,  0.0771, -0.0045, -0.0759],\n",
      "          [-0.0030,  0.0057, -0.0722, -0.0526, -0.0638]],\n",
      "\n",
      "         [[-0.0732,  0.0040, -0.0166, -0.0497, -0.0650],\n",
      "          [-0.0788,  0.0410,  0.0163, -0.0450,  0.0045],\n",
      "          [-0.0666,  0.0610,  0.0043, -0.0351, -0.0639],\n",
      "          [ 0.0199,  0.0716, -0.0712,  0.0203,  0.0814],\n",
      "          [-0.0236, -0.0086,  0.0132, -0.0261,  0.0378]],\n",
      "\n",
      "         [[-0.0215, -0.0590,  0.0752, -0.0247,  0.0629],\n",
      "          [ 0.0430,  0.0502,  0.0309, -0.0733, -0.0169],\n",
      "          [ 0.0687,  0.0334,  0.0589,  0.0153, -0.0201],\n",
      "          [-0.0267, -0.0676,  0.0588, -0.0722, -0.0060],\n",
      "          [ 0.0150,  0.0041, -0.0288, -0.0540,  0.0544]],\n",
      "\n",
      "         [[-0.0772, -0.0166,  0.0071,  0.0329, -0.0154],\n",
      "          [-0.0192, -0.0382,  0.0627, -0.0657, -0.0265],\n",
      "          [ 0.0621,  0.0269,  0.0152,  0.0567,  0.0777],\n",
      "          [ 0.0778, -0.0285,  0.0054,  0.0181, -0.0720],\n",
      "          [ 0.0466, -0.0724, -0.0014,  0.0105,  0.0421]],\n",
      "\n",
      "         [[ 0.0762,  0.0217, -0.0092, -0.0297,  0.0147],\n",
      "          [-0.0477,  0.0573,  0.0455, -0.0633,  0.0801],\n",
      "          [-0.0022, -0.0544,  0.0435, -0.0183, -0.0438],\n",
      "          [-0.0621, -0.0297,  0.0089, -0.0689,  0.0784],\n",
      "          [-0.0332,  0.0291,  0.0511,  0.0190,  0.0549]],\n",
      "\n",
      "         [[ 0.0274, -0.0645,  0.0197,  0.0685, -0.0294],\n",
      "          [ 0.0088,  0.0526,  0.0424, -0.0525,  0.0232],\n",
      "          [ 0.0616, -0.0356,  0.0148,  0.0270,  0.0565],\n",
      "          [-0.0703,  0.0070, -0.0293, -0.0126, -0.0771],\n",
      "          [ 0.0344,  0.0343, -0.0663, -0.0516,  0.0282]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0668, -0.0255,  0.0786,  0.0252,  0.0587],\n",
      "          [-0.0616, -0.0516, -0.0480,  0.0397,  0.0432],\n",
      "          [-0.0578, -0.0576, -0.0170,  0.0068,  0.0815],\n",
      "          [-0.0751, -0.0318,  0.0754,  0.0058, -0.0341],\n",
      "          [-0.0011, -0.0688,  0.0013,  0.0667,  0.0047]],\n",
      "\n",
      "         [[ 0.0240,  0.0622, -0.0300,  0.0253, -0.0273],\n",
      "          [-0.0460,  0.0030,  0.0600,  0.0786, -0.0359],\n",
      "          [-0.0252, -0.0059,  0.0367,  0.0234, -0.0780],\n",
      "          [-0.0457,  0.0385,  0.0088,  0.0580,  0.0606],\n",
      "          [ 0.0706,  0.0808,  0.0651,  0.0216, -0.0286]],\n",
      "\n",
      "         [[ 0.0005, -0.0225,  0.0240,  0.0629, -0.0431],\n",
      "          [ 0.0600,  0.0523, -0.0310,  0.0224, -0.0411],\n",
      "          [-0.0466,  0.0670,  0.0060,  0.0245,  0.0602],\n",
      "          [-0.0351,  0.0409,  0.0288, -0.0302, -0.0265],\n",
      "          [ 0.0145, -0.0728, -0.0760,  0.0639, -0.0268]],\n",
      "\n",
      "         [[ 0.0373,  0.0596, -0.0642, -0.0146, -0.0745],\n",
      "          [-0.0401,  0.0567,  0.0074,  0.0042, -0.0382],\n",
      "          [-0.0610,  0.0360,  0.0352,  0.0206, -0.0586],\n",
      "          [ 0.0778, -0.0406,  0.0535, -0.0675, -0.0557],\n",
      "          [-0.0345, -0.0078, -0.0477, -0.0516,  0.0256]],\n",
      "\n",
      "         [[-0.0808,  0.0606,  0.0003,  0.0265, -0.0609],\n",
      "          [ 0.0136, -0.0178, -0.0701, -0.0175,  0.0527],\n",
      "          [-0.0458,  0.0406, -0.0531, -0.0562, -0.0755],\n",
      "          [ 0.0745, -0.0712,  0.0458,  0.0492,  0.0567],\n",
      "          [-0.0128,  0.0370,  0.0383, -0.0152, -0.0725]],\n",
      "\n",
      "         [[ 0.0542,  0.0657,  0.0414,  0.0663,  0.0007],\n",
      "          [-0.0448,  0.0573, -0.0587,  0.0674,  0.0023],\n",
      "          [-0.0754,  0.0091, -0.0146, -0.0767, -0.0706],\n",
      "          [-0.0342, -0.0124, -0.0372, -0.0614,  0.0685],\n",
      "          [ 0.0813,  0.0674,  0.0325,  0.0079,  0.0111]]],\n",
      "\n",
      "\n",
      "        [[[-0.0394, -0.0806, -0.0163, -0.0563, -0.0761],\n",
      "          [-0.0641, -0.0513,  0.0497, -0.0653,  0.0229],\n",
      "          [-0.0745, -0.0015, -0.0433, -0.0203, -0.0191],\n",
      "          [ 0.0769,  0.0411,  0.0037,  0.0808,  0.0025],\n",
      "          [ 0.0435, -0.0169, -0.0379,  0.0648, -0.0476]],\n",
      "\n",
      "         [[-0.0147,  0.0129,  0.0547, -0.0744, -0.0167],\n",
      "          [-0.0409,  0.0219, -0.0592,  0.0385, -0.0801],\n",
      "          [ 0.0135, -0.0721, -0.0277,  0.0097,  0.0417],\n",
      "          [ 0.0290,  0.0023,  0.0287, -0.0761, -0.0500],\n",
      "          [ 0.0802,  0.0513, -0.0707, -0.0646,  0.0119]],\n",
      "\n",
      "         [[-0.0749, -0.0015,  0.0013,  0.0281, -0.0366],\n",
      "          [ 0.0282,  0.0266,  0.0810, -0.0533, -0.0510],\n",
      "          [ 0.0251, -0.0765,  0.0033,  0.0616,  0.0784],\n",
      "          [-0.0300,  0.0078, -0.0357, -0.0716, -0.0411],\n",
      "          [-0.0108, -0.0433, -0.0408, -0.0364,  0.0439]],\n",
      "\n",
      "         [[ 0.0791,  0.0531, -0.0069, -0.0467, -0.0101],\n",
      "          [-0.0796, -0.0509, -0.0391, -0.0219,  0.0732],\n",
      "          [ 0.0258, -0.0101,  0.0770, -0.0252,  0.0102],\n",
      "          [-0.0485,  0.0191, -0.0074,  0.0351, -0.0256],\n",
      "          [-0.0735, -0.0628, -0.0288, -0.0520,  0.0192]],\n",
      "\n",
      "         [[ 0.0263, -0.0744,  0.0073,  0.0003,  0.0541],\n",
      "          [-0.0536, -0.0395,  0.0303,  0.0408, -0.0725],\n",
      "          [ 0.0073,  0.0284,  0.0751, -0.0192,  0.0493],\n",
      "          [ 0.0171,  0.0769, -0.0309,  0.0478, -0.0460],\n",
      "          [-0.0187, -0.0002, -0.0632, -0.0206,  0.0208]],\n",
      "\n",
      "         [[ 0.0319,  0.0645,  0.0600,  0.0816, -0.0533],\n",
      "          [-0.0598, -0.0682,  0.0379, -0.0563,  0.0077],\n",
      "          [ 0.0729, -0.0137,  0.0309,  0.0051,  0.0458],\n",
      "          [-0.0479,  0.0690,  0.0140, -0.0613,  0.0238],\n",
      "          [ 0.0799,  0.0360, -0.0056, -0.0616,  0.0355]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0173, -0.0430, -0.0669,  0.0281, -0.0182],\n",
      "          [ 0.0590,  0.0180, -0.0158, -0.0688, -0.0699],\n",
      "          [ 0.0101,  0.0691, -0.0784, -0.0715,  0.0429],\n",
      "          [ 0.0178, -0.0553, -0.0100, -0.0359, -0.0713],\n",
      "          [-0.0749, -0.0704,  0.0801, -0.0790,  0.0141]],\n",
      "\n",
      "         [[ 0.0623, -0.0670, -0.0743,  0.0744,  0.0464],\n",
      "          [-0.0043,  0.0644, -0.0166,  0.0776,  0.0197],\n",
      "          [-0.0109, -0.0572,  0.0009, -0.0440,  0.0679],\n",
      "          [ 0.0648, -0.0323, -0.0332,  0.0138,  0.0636],\n",
      "          [ 0.0537, -0.0374, -0.0117, -0.0499,  0.0398]],\n",
      "\n",
      "         [[-0.0467,  0.0111,  0.0164, -0.0735, -0.0424],\n",
      "          [-0.0387,  0.0307, -0.0385,  0.0701,  0.0281],\n",
      "          [-0.0375, -0.0624,  0.0547,  0.0358,  0.0814],\n",
      "          [ 0.0537, -0.0605,  0.0719, -0.0062, -0.0402],\n",
      "          [-0.0339,  0.0130,  0.0430,  0.0483, -0.0114]],\n",
      "\n",
      "         [[ 0.0796, -0.0338,  0.0626, -0.0505,  0.0657],\n",
      "          [-0.0212, -0.0638, -0.0552, -0.0300,  0.0589],\n",
      "          [-0.0628, -0.0573, -0.0730,  0.0100, -0.0399],\n",
      "          [-0.0395,  0.0261,  0.0334, -0.0109,  0.0807],\n",
      "          [-0.0385,  0.0754, -0.0580,  0.0310, -0.0469]],\n",
      "\n",
      "         [[-0.0056, -0.0460, -0.0660,  0.0049,  0.0002],\n",
      "          [ 0.0770,  0.0355,  0.0287, -0.0031, -0.0432],\n",
      "          [-0.0313, -0.0540,  0.0281,  0.0477,  0.0118],\n",
      "          [ 0.0226, -0.0420, -0.0809,  0.0335, -0.0714],\n",
      "          [ 0.0791,  0.0169, -0.0412, -0.0525,  0.0807]],\n",
      "\n",
      "         [[ 0.0157, -0.0636,  0.0494,  0.0612, -0.0130],\n",
      "          [ 0.0759,  0.0262, -0.0087, -0.0609,  0.0514],\n",
      "          [ 0.0535,  0.0317, -0.0543, -0.0223, -0.0182],\n",
      "          [-0.0572, -0.0333, -0.0779, -0.0348, -0.0265],\n",
      "          [-0.0750,  0.0242,  0.0024, -0.0003,  0.0155]]],\n",
      "\n",
      "\n",
      "        [[[-0.0215, -0.0746, -0.0340, -0.0475,  0.0649],\n",
      "          [ 0.0204,  0.0205,  0.0484, -0.0291,  0.0416],\n",
      "          [ 0.0007, -0.0735, -0.0609,  0.0753,  0.0690],\n",
      "          [-0.0256, -0.0223, -0.0540,  0.0339, -0.0740],\n",
      "          [ 0.0677,  0.0458,  0.0612,  0.0665, -0.0302]],\n",
      "\n",
      "         [[-0.0612, -0.0681,  0.0680, -0.0599,  0.0562],\n",
      "          [ 0.0217, -0.0638,  0.0501,  0.0589, -0.0524],\n",
      "          [-0.0232, -0.0471,  0.0352,  0.0708, -0.0580],\n",
      "          [-0.0716,  0.0111,  0.0598, -0.0212,  0.0139],\n",
      "          [ 0.0590, -0.0685,  0.0071, -0.0184, -0.0250]],\n",
      "\n",
      "         [[ 0.0594,  0.0464,  0.0135, -0.0081,  0.0077],\n",
      "          [ 0.0097, -0.0622,  0.0046, -0.0616, -0.0080],\n",
      "          [-0.0642,  0.0179, -0.0438,  0.0589, -0.0266],\n",
      "          [-0.0185,  0.0159,  0.0647,  0.0510,  0.0292],\n",
      "          [ 0.0801,  0.0267,  0.0442, -0.0118, -0.0472]],\n",
      "\n",
      "         [[-0.0062, -0.0021,  0.0752,  0.0301,  0.0358],\n",
      "          [-0.0729,  0.0472, -0.0707,  0.0748, -0.0002],\n",
      "          [ 0.0235,  0.0444, -0.0612, -0.0668,  0.0780],\n",
      "          [ 0.0029, -0.0055,  0.0658, -0.0552, -0.0529],\n",
      "          [ 0.0607, -0.0113,  0.0745,  0.0615, -0.0332]],\n",
      "\n",
      "         [[-0.0711,  0.0816, -0.0146,  0.0497, -0.0638],\n",
      "          [-0.0711, -0.0186, -0.0334,  0.0740, -0.0317],\n",
      "          [-0.0620,  0.0312, -0.0127, -0.0484, -0.0283],\n",
      "          [-0.0320,  0.0168, -0.0628, -0.0248, -0.0045],\n",
      "          [ 0.0180, -0.0003,  0.0769, -0.0736, -0.0798]],\n",
      "\n",
      "         [[ 0.0461, -0.0400,  0.0258,  0.0387, -0.0450],\n",
      "          [-0.0774,  0.0014, -0.0647,  0.0431, -0.0782],\n",
      "          [-0.0530,  0.0429, -0.0253,  0.0466,  0.0056],\n",
      "          [ 0.0041,  0.0383,  0.0331, -0.0661, -0.0598],\n",
      "          [ 0.0081,  0.0421, -0.0102,  0.0785,  0.0722]]],\n",
      "\n",
      "\n",
      "        [[[-0.0494,  0.0342, -0.0218,  0.0332,  0.0203],\n",
      "          [ 0.0585,  0.0462, -0.0250, -0.0095,  0.0743],\n",
      "          [ 0.0034, -0.0809,  0.0156,  0.0791,  0.0455],\n",
      "          [-0.0149, -0.0508,  0.0539,  0.0098, -0.0215],\n",
      "          [ 0.0555,  0.0441,  0.0788,  0.0702, -0.0613]],\n",
      "\n",
      "         [[-0.0034, -0.0729,  0.0615, -0.0270, -0.0409],\n",
      "          [ 0.0636, -0.0714, -0.0762,  0.0379,  0.0562],\n",
      "          [-0.0417,  0.0808,  0.0144,  0.0193,  0.0141],\n",
      "          [-0.0163,  0.0448,  0.0263, -0.0005,  0.0780],\n",
      "          [ 0.0265, -0.0809,  0.0482,  0.0149,  0.0232]],\n",
      "\n",
      "         [[-0.0643,  0.0005,  0.0795, -0.0016, -0.0174],\n",
      "          [ 0.0130,  0.0315, -0.0578, -0.0503, -0.0091],\n",
      "          [ 0.0001, -0.0350,  0.0484,  0.0029, -0.0007],\n",
      "          [ 0.0773, -0.0441, -0.0293,  0.0320, -0.0681],\n",
      "          [ 0.0634, -0.0134,  0.0553, -0.0042, -0.0018]],\n",
      "\n",
      "         [[ 0.0536,  0.0488,  0.0162,  0.0724, -0.0076],\n",
      "          [ 0.0429, -0.0428,  0.0211, -0.0325, -0.0535],\n",
      "          [-0.0754, -0.0403,  0.0785, -0.0651,  0.0789],\n",
      "          [ 0.0762, -0.0230, -0.0617, -0.0619, -0.0081],\n",
      "          [ 0.0722,  0.0301,  0.0488,  0.0552, -0.0674]],\n",
      "\n",
      "         [[ 0.0076,  0.0099,  0.0414, -0.0164,  0.0610],\n",
      "          [-0.0198,  0.0198, -0.0368,  0.0607,  0.0505],\n",
      "          [-0.0680,  0.0303, -0.0320,  0.0611, -0.0117],\n",
      "          [-0.0391,  0.0542,  0.0482, -0.0070, -0.0563],\n",
      "          [-0.0086, -0.0272,  0.0691,  0.0691,  0.0020]],\n",
      "\n",
      "         [[ 0.0435,  0.0803, -0.0723, -0.0447, -0.0657],\n",
      "          [-0.0702, -0.0099,  0.0724, -0.0805,  0.0076],\n",
      "          [-0.0286,  0.0201, -0.0596,  0.0116,  0.0575],\n",
      "          [ 0.0702,  0.0388,  0.0676, -0.0681,  0.0617],\n",
      "          [-0.0183,  0.0230, -0.0162, -0.0793, -0.0271]]]], requires_grad=True)), ('conv2.bias', Parameter containing:\n",
      "tensor([ 0.0538,  0.0034, -0.0184,  0.0422,  0.0204,  0.0733, -0.0330, -0.0123,\n",
      "         0.0777,  0.0518,  0.0503, -0.0173,  0.0287, -0.0385,  0.0345, -0.0748],\n",
      "       requires_grad=True)), ('fc1.weight', Parameter containing:\n",
      "tensor([[ 4.0261e-02,  3.0279e-02, -4.9983e-02,  ...,  6.2900e-03,\n",
      "         -5.3258e-04, -4.7123e-02],\n",
      "        [ 2.7571e-02, -1.3566e-02, -1.4466e-02,  ..., -4.5363e-02,\n",
      "         -1.8754e-02,  4.7895e-02],\n",
      "        [-8.7958e-03,  1.7781e-02, -3.7683e-02,  ..., -1.1211e-02,\n",
      "          4.9079e-02,  3.0938e-02],\n",
      "        ...,\n",
      "        [-4.9207e-02, -1.1341e-02,  3.1855e-02,  ...,  4.8097e-02,\n",
      "         -4.9991e-03, -1.0737e-02],\n",
      "        [-4.3429e-02, -1.8544e-02, -2.9933e-02,  ...,  1.0944e-02,\n",
      "         -4.4038e-02, -6.5051e-03],\n",
      "        [-1.1392e-05, -4.1996e-02,  1.9847e-04,  ..., -2.2988e-02,\n",
      "         -2.5931e-02, -4.3163e-02]], requires_grad=True)), ('fc1.bias', Parameter containing:\n",
      "tensor([ 3.8600e-02, -4.7601e-02,  3.3190e-02,  1.4191e-02, -5.7142e-05,\n",
      "        -4.1726e-02,  4.1131e-03,  1.5377e-02, -3.9810e-02,  4.0622e-02,\n",
      "         3.8793e-02, -3.6682e-02, -3.7070e-02, -1.0644e-02,  2.9345e-02,\n",
      "        -6.8233e-03,  4.1380e-02, -1.1982e-02,  1.4369e-02, -2.7734e-02,\n",
      "         3.2706e-02,  1.1742e-02, -2.2494e-02, -2.7515e-02, -3.3270e-02,\n",
      "        -1.0182e-03,  2.3305e-03, -2.2479e-02, -1.5514e-02, -4.4556e-02,\n",
      "        -2.9185e-02,  4.6921e-03, -3.7478e-02, -1.5931e-02, -4.9502e-03,\n",
      "        -2.3748e-02, -3.1538e-02,  3.2074e-02, -2.2146e-02, -4.9931e-02,\n",
      "         2.9066e-02,  4.2301e-02, -6.8529e-03, -3.9041e-02, -4.8320e-02,\n",
      "         2.9452e-02, -4.8347e-02,  1.0027e-02, -4.2366e-02, -2.0061e-02,\n",
      "         1.5945e-02,  2.6397e-02, -5.1250e-03,  3.6581e-02,  3.4633e-02,\n",
      "         3.9744e-02, -1.4745e-02, -4.1728e-02, -2.6492e-02,  4.9308e-02,\n",
      "         9.7995e-04, -3.7353e-02, -2.1192e-02,  3.3814e-02, -4.9681e-02,\n",
      "        -2.6476e-02, -2.7608e-02,  4.6299e-02,  4.8414e-02,  3.2442e-02,\n",
      "         2.2826e-02,  3.4370e-02, -3.4917e-02,  1.0902e-03,  2.2636e-02,\n",
      "        -2.6123e-02,  9.4941e-03, -2.7578e-02,  3.4369e-02,  4.7804e-02,\n",
      "         2.2847e-03,  9.9875e-03,  1.9339e-02,  2.7147e-02, -5.0043e-03,\n",
      "         1.5843e-02,  2.3974e-02,  1.6400e-02, -2.4980e-02, -3.2078e-02,\n",
      "        -2.6210e-02, -2.8415e-02, -4.2841e-03,  1.9123e-02,  2.6014e-02,\n",
      "        -2.0883e-03,  3.5396e-02, -1.5551e-02, -1.1809e-02,  1.3857e-02,\n",
      "        -4.2158e-02,  2.9782e-02, -4.7188e-02,  4.4319e-03,  3.1965e-02,\n",
      "         5.3941e-03,  7.2705e-03, -1.0142e-02, -3.0608e-02, -4.3706e-02,\n",
      "        -4.9386e-02, -2.1708e-02, -4.6707e-02,  6.0073e-03, -1.1556e-02,\n",
      "         2.5381e-02,  2.7697e-02, -3.5557e-02, -4.6896e-02, -3.2715e-02],\n",
      "       requires_grad=True)), ('fc2.weight', Parameter containing:\n",
      "tensor([[-0.0532, -0.0464, -0.0345,  ..., -0.0606, -0.0164, -0.0289],\n",
      "        [ 0.0455,  0.0245, -0.0418,  ...,  0.0220, -0.0277, -0.0017],\n",
      "        [-0.0113, -0.0169,  0.0171,  ...,  0.0488, -0.0357, -0.0616],\n",
      "        ...,\n",
      "        [ 0.0583,  0.0206,  0.0884,  ..., -0.0107, -0.0262, -0.0286],\n",
      "        [-0.0693,  0.0666,  0.0791,  ...,  0.0060, -0.0715,  0.0320],\n",
      "        [-0.0516,  0.0401, -0.0493,  ..., -0.0452, -0.0198, -0.0596]],\n",
      "       requires_grad=True)), ('fc2.bias', Parameter containing:\n",
      "tensor([-0.0526,  0.0222,  0.0586,  0.0657, -0.0374,  0.0223,  0.0874, -0.0596,\n",
      "        -0.0581,  0.0392,  0.0092,  0.0729, -0.0345, -0.0782,  0.0733, -0.0021,\n",
      "         0.0492,  0.0662,  0.0510, -0.0878, -0.0233,  0.0321, -0.0879,  0.0463,\n",
      "         0.0057, -0.0903, -0.0027, -0.0429,  0.0541,  0.0512, -0.0864, -0.0719,\n",
      "         0.0836, -0.0907,  0.0855,  0.0815, -0.0201, -0.0765,  0.0085,  0.0849,\n",
      "        -0.0827, -0.0632, -0.0187, -0.0383, -0.0049,  0.0291, -0.0572,  0.0088,\n",
      "         0.0289,  0.0313, -0.0504, -0.0811,  0.0483,  0.0403,  0.0063,  0.0353,\n",
      "        -0.0762,  0.0354, -0.0639,  0.0397, -0.0639,  0.0189, -0.0335,  0.0175,\n",
      "        -0.0411, -0.0589,  0.0644,  0.0397, -0.0717, -0.0560,  0.0087, -0.0393,\n",
      "        -0.0621,  0.0677,  0.0747,  0.0136,  0.0093,  0.0492,  0.0307,  0.0715,\n",
      "         0.0801,  0.0310,  0.0244, -0.0065], requires_grad=True)), ('fc3.weight', Parameter containing:\n",
      "tensor([[-7.5680e-02,  7.7950e-02,  6.1832e-02, -4.7377e-02,  4.8159e-02,\n",
      "          6.9454e-02, -3.2534e-02,  2.0379e-02, -7.5961e-02, -4.6476e-02,\n",
      "         -6.9640e-02, -4.6610e-02,  1.0568e-02, -4.2476e-02, -6.3887e-02,\n",
      "          8.7082e-02, -4.1206e-02,  1.0341e-01, -2.2951e-02, -2.3114e-02,\n",
      "          1.0661e-01, -1.0530e-01, -5.5595e-03,  5.2345e-02, -5.5332e-02,\n",
      "          3.6761e-02, -6.6831e-02, -2.5676e-02, -8.1244e-02,  2.0673e-02,\n",
      "         -8.4823e-02,  2.3408e-02,  9.0868e-02,  4.1004e-03, -5.0293e-02,\n",
      "         -5.6880e-02,  7.6352e-02,  7.0099e-02, -7.0275e-02, -1.7515e-02,\n",
      "         -3.5322e-02, -5.0922e-02,  7.6954e-02, -6.2586e-02, -5.8908e-02,\n",
      "         -2.2426e-02, -4.2690e-02,  1.3231e-02, -5.8378e-02, -6.8259e-03,\n",
      "          1.0612e-01,  4.5808e-02,  4.6438e-02,  9.8022e-03, -4.5039e-02,\n",
      "          8.9178e-03, -1.0425e-01,  9.5781e-02,  8.2206e-02, -5.7439e-03,\n",
      "          7.1006e-02,  2.3825e-02, -1.3501e-02,  4.0841e-02, -1.2726e-02,\n",
      "         -4.4742e-02,  8.4165e-03, -3.2607e-02,  5.8409e-02, -6.7420e-02,\n",
      "         -7.8976e-02, -1.2807e-02,  9.0191e-02,  9.0883e-03,  2.4910e-02,\n",
      "         -1.0448e-01,  7.6756e-02,  7.9997e-02,  5.8405e-02, -4.5897e-02,\n",
      "         -1.0026e-01,  7.5888e-02,  6.0256e-02,  3.7405e-02],\n",
      "        [-2.6420e-02,  3.0010e-03,  5.8777e-02, -3.9437e-02,  8.1442e-02,\n",
      "          6.1441e-02, -9.4589e-02, -9.2447e-02,  7.0287e-02, -5.2133e-02,\n",
      "          5.8463e-02,  4.7068e-02, -3.7567e-02,  8.3459e-02,  6.3138e-02,\n",
      "         -1.0669e-01, -2.8366e-02,  6.7364e-02,  4.0295e-02,  2.4356e-02,\n",
      "         -4.2145e-02,  9.6501e-03,  4.8393e-02, -3.4268e-02, -7.3768e-02,\n",
      "          3.5972e-02,  2.6663e-02,  6.7622e-02,  3.3602e-02, -1.0683e-01,\n",
      "          1.0203e-01,  5.2670e-02,  1.3068e-02,  6.6797e-04, -6.2188e-02,\n",
      "          6.5220e-02,  1.0009e-01, -4.2738e-02,  1.0471e-01, -8.6577e-02,\n",
      "          5.9418e-02,  6.7152e-02, -4.9220e-02, -1.0071e-01,  8.2864e-02,\n",
      "         -1.9370e-02, -7.7718e-02,  3.4097e-02, -5.5621e-03,  3.3517e-02,\n",
      "         -1.1199e-02, -1.0622e-01, -4.4453e-02, -3.8799e-02,  3.3086e-02,\n",
      "         -9.3499e-02, -4.8918e-02,  1.3851e-02, -9.7610e-02, -9.5463e-02,\n",
      "          4.3831e-02, -6.9399e-02,  6.1621e-02, -8.2042e-02, -1.1617e-02,\n",
      "          9.6757e-02,  1.2927e-03,  1.9989e-02, -5.8724e-02,  1.6697e-03,\n",
      "         -9.2437e-02,  7.7367e-02,  4.7858e-02,  9.3013e-02,  8.9807e-02,\n",
      "          4.4009e-02,  9.1107e-02, -9.4870e-02,  9.9420e-02, -3.0873e-02,\n",
      "          1.0806e-01,  9.4780e-02,  7.4735e-02, -8.3734e-02],\n",
      "        [ 6.6125e-02,  6.4899e-02, -2.6020e-02, -6.5754e-02,  5.9769e-03,\n",
      "         -4.5243e-02,  1.7221e-02,  2.7976e-02,  5.5749e-02, -7.9478e-02,\n",
      "         -7.7478e-02,  1.0030e-01,  3.5052e-02, -3.3734e-03,  3.8957e-02,\n",
      "         -2.8906e-02,  6.1057e-02,  4.1695e-02, -3.9734e-02,  4.8431e-03,\n",
      "         -2.4689e-02,  1.0291e-01, -7.0648e-03,  5.1570e-02, -4.2968e-02,\n",
      "          3.1326e-02,  9.1601e-02, -1.0085e-01,  6.6445e-02,  3.5713e-02,\n",
      "          2.5720e-02, -6.7606e-02, -6.7498e-02, -5.8067e-02,  1.0790e-01,\n",
      "         -2.2042e-02, -7.0737e-02,  4.5833e-02, -4.5918e-02,  9.0300e-02,\n",
      "          1.0500e-01, -8.0514e-02,  1.4164e-03,  7.4751e-02, -9.7005e-02,\n",
      "          9.6180e-02, -9.6637e-02, -7.6394e-02,  5.5633e-02,  4.1798e-02,\n",
      "          5.2362e-02, -8.8541e-02, -9.9301e-03, -3.6266e-02,  1.0004e-01,\n",
      "          6.9308e-02, -1.0075e-01, -3.9355e-02,  2.3449e-02, -9.3733e-02,\n",
      "         -6.2539e-03,  7.6598e-02, -5.1101e-02, -8.5936e-03,  5.9488e-02,\n",
      "         -5.4033e-02,  1.8271e-02, -9.0031e-04, -9.9203e-02,  6.2468e-02,\n",
      "         -7.1595e-02,  2.1153e-02, -5.4529e-02,  7.6410e-02, -9.0713e-02,\n",
      "          9.8866e-02,  9.6606e-04, -7.8371e-02,  8.3163e-03,  8.3119e-02,\n",
      "         -5.3826e-02,  1.0580e-01, -4.7641e-02,  1.0772e-01],\n",
      "        [-1.3292e-02,  2.6426e-02, -8.1468e-02,  5.6072e-02,  9.7312e-02,\n",
      "          2.5701e-02,  4.3695e-02,  8.1836e-02, -1.8652e-02, -8.5065e-02,\n",
      "         -6.4339e-02,  8.7754e-02, -1.0693e-01,  9.9442e-02,  8.5975e-02,\n",
      "          3.0053e-02, -8.7110e-02, -2.7401e-02, -6.6881e-04,  6.9305e-02,\n",
      "         -3.0214e-03, -4.8789e-02, -1.0572e-01,  7.3975e-02, -5.8193e-02,\n",
      "          6.4226e-02,  9.5028e-02, -9.6982e-02,  9.4574e-02,  9.7201e-02,\n",
      "         -7.7784e-02, -8.3620e-02,  8.8566e-02,  3.8449e-03, -3.8566e-02,\n",
      "          8.2425e-02,  2.7984e-02,  6.1169e-02, -7.4464e-03,  7.5237e-02,\n",
      "          3.7046e-02,  6.8083e-02, -5.3643e-02,  7.4714e-02,  7.8209e-02,\n",
      "         -3.0889e-03,  4.2293e-02,  5.2264e-02,  1.5695e-02, -5.4713e-02,\n",
      "          6.7125e-03,  1.6271e-04, -7.7745e-02, -4.5009e-02, -1.7917e-03,\n",
      "          6.5347e-02, -7.7137e-02, -7.5238e-02, -4.0181e-02, -2.6860e-02,\n",
      "          8.1763e-02, -1.0715e-01,  1.4739e-02, -9.5610e-02, -7.3551e-04,\n",
      "          5.8079e-02,  3.6460e-03, -1.6159e-02,  9.8024e-03,  1.0655e-01,\n",
      "          9.4181e-02,  7.8663e-02, -6.0305e-02,  9.1060e-02,  1.0228e-01,\n",
      "         -3.2053e-02, -7.1706e-02, -1.5853e-02, -2.5389e-02, -8.0776e-02,\n",
      "         -5.7169e-02, -8.5552e-02,  4.4548e-02, -9.4913e-02],\n",
      "        [-2.6768e-02, -6.6145e-02,  6.9563e-02,  6.9594e-02,  6.9546e-02,\n",
      "         -5.8148e-02, -9.6351e-02,  8.0548e-03,  6.7997e-02, -2.1068e-02,\n",
      "          8.2002e-02,  1.9977e-02, -9.8877e-02,  1.4916e-02, -8.7352e-03,\n",
      "          9.6152e-02,  1.0664e-01, -4.8735e-02, -9.7077e-02, -2.5608e-02,\n",
      "         -1.0664e-01, -7.6839e-02, -5.0200e-02, -8.1163e-02,  6.6864e-02,\n",
      "         -1.0810e-02,  9.5547e-02,  8.7503e-02, -4.9977e-02,  7.3483e-02,\n",
      "          8.4226e-02,  1.4283e-02,  2.1384e-02, -4.9501e-02, -1.3148e-02,\n",
      "         -1.0148e-01, -2.0824e-02,  7.0386e-02,  5.6739e-02, -1.4739e-02,\n",
      "          5.9431e-03, -8.3607e-02, -6.2088e-02, -9.5499e-02,  9.2293e-02,\n",
      "          4.1080e-02, -6.3452e-02, -8.6650e-02,  5.2435e-02, -9.2073e-02,\n",
      "          8.5454e-02, -1.6448e-02, -4.9789e-02,  8.6978e-02,  1.9182e-02,\n",
      "         -3.7104e-02, -5.6794e-02, -8.5740e-02, -1.0496e-02,  9.7445e-02,\n",
      "          8.1036e-03, -6.6503e-02,  6.2340e-02,  2.1068e-02, -6.2449e-02,\n",
      "         -2.8532e-02, -1.0023e-01, -8.9853e-02, -9.8429e-02, -8.2875e-02,\n",
      "         -4.5365e-02, -3.7889e-02, -9.1498e-02,  3.7407e-02, -9.0268e-02,\n",
      "          8.5713e-02,  1.6583e-02,  8.7785e-02,  3.6988e-02, -8.0147e-02,\n",
      "         -2.1009e-02,  5.5331e-04,  1.4988e-02,  1.0753e-01],\n",
      "        [ 4.4471e-02,  2.8763e-02, -5.7505e-02, -3.3376e-02,  9.3116e-02,\n",
      "         -9.5651e-02,  2.8656e-02,  1.0703e-01,  2.2562e-02, -4.7202e-02,\n",
      "          1.0625e-01,  7.6341e-02, -3.2980e-04, -1.0171e-01, -2.2612e-02,\n",
      "         -5.5512e-02,  3.6451e-02, -3.1965e-02,  7.9412e-02,  6.8681e-02,\n",
      "          3.3344e-02,  5.1678e-03,  7.5981e-02,  1.6359e-02, -3.0875e-02,\n",
      "         -4.7575e-02,  9.6292e-02, -5.1653e-02,  6.6436e-02,  2.1711e-02,\n",
      "          8.3161e-02, -1.2161e-02, -4.8596e-02,  7.1216e-02,  5.9689e-02,\n",
      "          6.2755e-02,  7.3663e-03,  5.2815e-02, -2.3731e-02,  2.5268e-02,\n",
      "         -2.1276e-02,  2.5064e-02,  4.8579e-02, -6.2810e-02, -1.0754e-01,\n",
      "          5.2213e-02, -4.9146e-02, -1.0460e-01, -1.5789e-02, -5.2551e-02,\n",
      "          1.6096e-02,  4.3418e-02, -1.0736e-01,  1.0755e-01, -4.1508e-02,\n",
      "         -3.6133e-02,  5.7755e-02, -9.1761e-02,  1.0885e-01, -5.5533e-02,\n",
      "         -5.8065e-02,  9.1194e-02, -1.0738e-01, -6.7086e-02, -7.6981e-02,\n",
      "          6.2592e-02,  9.0073e-02,  1.2368e-02,  8.7318e-02, -8.1853e-02,\n",
      "         -1.0221e-01, -3.9869e-02,  8.3773e-02,  6.0246e-02,  1.0043e-01,\n",
      "         -8.4603e-02,  5.1719e-02, -9.5992e-02, -6.2736e-03,  4.7983e-02,\n",
      "          5.8228e-02, -4.5345e-02, -7.9488e-02,  4.4839e-02],\n",
      "        [-7.3853e-02, -3.4892e-02, -3.5800e-02,  4.6828e-03, -1.0241e-01,\n",
      "         -1.4963e-02, -9.6512e-02, -1.0236e-01,  9.8191e-02, -3.3013e-02,\n",
      "         -5.2323e-03, -4.7687e-02, -3.2073e-02, -6.2945e-02,  9.4983e-02,\n",
      "          1.0639e-01,  9.7072e-02,  8.4011e-02,  3.1415e-02,  5.6933e-02,\n",
      "          8.1304e-02,  6.6381e-02,  1.0064e-01,  4.0738e-02, -5.1627e-02,\n",
      "          4.4981e-02,  4.6229e-02,  8.8056e-02, -7.5757e-02,  3.0856e-02,\n",
      "          5.1206e-02,  7.5833e-02,  3.4873e-02, -9.5738e-02,  4.3049e-02,\n",
      "          5.9941e-03, -4.9022e-02, -1.9231e-02, -5.4676e-03,  6.7876e-03,\n",
      "          6.5898e-02, -5.6637e-02, -1.4953e-02, -3.4528e-02, -9.9152e-02,\n",
      "          2.0527e-02,  6.2335e-02, -6.0046e-02,  1.0790e-01, -2.2173e-02,\n",
      "         -9.3810e-02,  2.9890e-02, -4.1710e-02, -5.5020e-02,  8.3970e-02,\n",
      "         -9.8980e-02, -7.8246e-02,  3.4843e-02, -5.4218e-02, -8.9560e-02,\n",
      "          1.7871e-02, -4.0596e-02,  8.8222e-02,  5.9901e-02,  3.2459e-02,\n",
      "         -9.1232e-02, -9.1206e-02, -8.6972e-02,  4.2675e-02,  8.4551e-02,\n",
      "         -2.9836e-02,  1.0625e-01, -9.9803e-02,  6.2346e-02,  9.2995e-02,\n",
      "          5.1763e-02, -4.3261e-02, -5.5978e-02,  6.2224e-03, -6.1143e-02,\n",
      "         -1.7760e-02, -3.3228e-02, -5.1892e-02, -1.6955e-02],\n",
      "        [ 4.9511e-04, -6.4138e-02, -5.8976e-02, -8.6404e-02, -1.2855e-03,\n",
      "          8.1571e-02,  6.9581e-03, -1.3424e-02, -4.0578e-02,  3.0023e-02,\n",
      "         -3.2692e-02, -9.6133e-02,  4.3883e-02, -3.9227e-02, -1.6885e-02,\n",
      "          2.6068e-02,  3.7826e-02, -8.8844e-03, -8.2971e-02,  3.5971e-04,\n",
      "          1.9521e-05,  5.2931e-02,  3.5109e-02,  9.8350e-02, -6.7430e-02,\n",
      "         -2.8136e-02, -9.5854e-02,  5.4210e-02,  9.1910e-02,  1.1932e-02,\n",
      "         -9.2036e-02, -2.8596e-02, -5.3971e-02, -1.0691e-01, -3.1526e-02,\n",
      "         -4.4541e-02,  2.6336e-02,  1.7007e-02,  7.5637e-02,  1.0326e-01,\n",
      "         -6.7358e-02, -8.9372e-02, -8.1230e-04,  2.9780e-02,  2.5738e-02,\n",
      "          3.9066e-03,  1.5186e-02,  4.3040e-02, -2.3802e-02,  7.1177e-02,\n",
      "          2.6827e-02,  6.3179e-02,  4.1378e-02, -7.3010e-02,  1.0801e-01,\n",
      "          1.3720e-02, -6.7249e-02, -1.2938e-02, -5.3518e-02, -2.3004e-02,\n",
      "          5.9699e-03, -5.1393e-02, -1.2030e-02,  1.5594e-03, -7.5780e-02,\n",
      "          8.9087e-02, -8.1250e-03, -1.0803e-01, -1.2547e-02, -4.2537e-02,\n",
      "          5.7974e-02,  1.5836e-02, -4.0241e-02,  3.9425e-02, -1.3435e-02,\n",
      "         -8.7248e-02, -9.6747e-02, -3.7835e-03,  1.0065e-02,  5.3425e-02,\n",
      "         -5.2505e-02,  9.1210e-02, -8.3617e-02,  1.6120e-02],\n",
      "        [-1.0042e-01,  7.8992e-02,  8.0003e-02,  1.0489e-02,  8.7006e-02,\n",
      "          6.6799e-02,  4.0471e-02,  7.1147e-02, -8.7103e-02,  1.3335e-02,\n",
      "         -4.7058e-02,  8.2687e-02,  2.1253e-02,  7.0475e-03,  9.2206e-02,\n",
      "         -7.2783e-03, -8.8977e-02,  5.7108e-02, -5.0992e-02, -5.0539e-02,\n",
      "         -9.5289e-02,  3.3719e-02, -6.8797e-02, -3.1385e-02,  1.7427e-02,\n",
      "         -8.2868e-02, -4.1872e-02,  8.8743e-02,  6.5684e-03,  7.1235e-02,\n",
      "          5.5353e-02, -6.4504e-02, -7.6318e-02, -6.7026e-02, -2.8050e-02,\n",
      "         -5.4396e-02, -9.5307e-02,  1.0517e-01,  1.0789e-01,  3.0331e-02,\n",
      "         -7.1174e-03,  8.9794e-02, -3.9727e-02, -7.5693e-02,  5.4150e-02,\n",
      "         -4.8708e-03,  7.2351e-03, -1.0462e-01,  1.5214e-02, -4.1508e-02,\n",
      "         -5.3162e-02,  4.7298e-02, -7.1344e-02, -2.8987e-03, -1.0906e-01,\n",
      "          6.8111e-02, -7.8697e-02, -2.5651e-03,  7.3082e-02, -5.6747e-02,\n",
      "         -9.7414e-02, -5.1587e-02,  1.0098e-01,  5.7649e-03, -6.2262e-02,\n",
      "         -6.3005e-02, -5.0343e-03,  1.7091e-02, -6.6832e-02, -5.4600e-02,\n",
      "          7.6759e-02, -4.5995e-02, -6.6187e-02,  6.1675e-02, -5.8854e-02,\n",
      "          7.5920e-02, -2.2563e-02, -8.8019e-02,  8.9795e-02, -7.7601e-02,\n",
      "         -2.5176e-02, -2.1308e-02, -5.6888e-02, -8.2031e-02],\n",
      "        [ 3.1648e-02, -8.4083e-02, -1.3289e-02,  5.2620e-02, -5.6643e-02,\n",
      "          9.2115e-02,  5.0736e-02,  6.6197e-02,  8.7611e-02, -7.9514e-02,\n",
      "          1.0200e-01,  9.7294e-02,  6.4449e-02, -1.1754e-02, -9.8091e-02,\n",
      "          9.8597e-02, -8.0468e-02,  3.4705e-03, -1.0516e-01, -9.7696e-02,\n",
      "         -2.8149e-02, -3.2860e-03,  1.0291e-01, -2.2624e-02,  6.6088e-02,\n",
      "          7.9274e-02,  1.0320e-01,  6.6124e-03, -8.3925e-02,  8.7071e-02,\n",
      "          6.1893e-02, -1.1810e-02,  8.1443e-02, -2.5009e-03, -1.0578e-01,\n",
      "         -5.1696e-02,  2.0637e-02,  1.1884e-02, -2.2481e-02, -4.2964e-02,\n",
      "          9.4556e-02, -8.9234e-02,  1.0509e-01, -9.3010e-02,  7.5346e-02,\n",
      "         -1.0276e-01, -1.0591e-01,  6.5713e-02,  3.0124e-02,  7.8457e-02,\n",
      "         -3.8223e-02, -9.1029e-02,  2.5907e-02, -6.8258e-02,  1.0254e-01,\n",
      "         -3.6898e-02,  3.0143e-02,  1.9462e-02,  1.7026e-02,  3.5352e-02,\n",
      "          1.0907e-01,  4.0426e-02, -3.6181e-02,  4.2785e-02, -9.2798e-02,\n",
      "         -1.3293e-02,  6.5869e-02,  7.7393e-02,  2.1738e-02,  2.7950e-02,\n",
      "         -2.0381e-03,  4.7503e-02,  9.3217e-02,  5.9419e-02,  2.2632e-02,\n",
      "          3.9036e-02,  1.0050e-01,  1.7821e-02,  8.1532e-02, -3.3050e-02,\n",
      "         -3.4297e-02,  4.0399e-02,  9.2748e-02, -9.0985e-02]],\n",
      "       requires_grad=True)), ('fc3.bias', Parameter containing:\n",
      "tensor([ 0.0783, -0.0899,  0.0726, -0.0203, -0.0106, -0.0748, -0.0710, -0.1004,\n",
      "        -0.0581,  0.0488], requires_grad=True))]\n",
      "[Parameter containing:\n",
      "tensor([[[[-0.0853,  0.0259, -0.1353, -0.1591,  0.1841],\n",
      "          [-0.0578, -0.1340,  0.1382, -0.0614, -0.1107],\n",
      "          [ 0.0715, -0.1830,  0.1895, -0.0060, -0.0329],\n",
      "          [ 0.0978,  0.1967,  0.1908,  0.1854,  0.1111],\n",
      "          [ 0.1166, -0.0787,  0.0061,  0.0560, -0.1067]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1763,  0.0658,  0.0384, -0.0494, -0.0980],\n",
      "          [-0.0579,  0.0167, -0.0629, -0.1563,  0.0857],\n",
      "          [ 0.1075, -0.1408,  0.1752, -0.1713,  0.1558],\n",
      "          [ 0.1045, -0.1933, -0.1849,  0.0725,  0.1523],\n",
      "          [ 0.1928,  0.1108, -0.0329,  0.1358,  0.0879]]],\n",
      "\n",
      "\n",
      "        [[[-0.0225,  0.0709,  0.1168,  0.0247, -0.1249],\n",
      "          [ 0.0682, -0.0889,  0.0442,  0.0234, -0.0219],\n",
      "          [-0.0119,  0.1589,  0.0332,  0.1510,  0.0026],\n",
      "          [ 0.1021,  0.1403, -0.1616,  0.1320,  0.1635],\n",
      "          [-0.1633, -0.0975, -0.1950, -0.1938,  0.0261]]],\n",
      "\n",
      "\n",
      "        [[[-0.1310, -0.0213, -0.1168,  0.1326,  0.0142],\n",
      "          [-0.1107, -0.1345,  0.0002,  0.0845, -0.1744],\n",
      "          [-0.1335, -0.1718, -0.1341, -0.1297,  0.1698],\n",
      "          [ 0.0946,  0.1710,  0.1697,  0.1459, -0.0926],\n",
      "          [ 0.1898,  0.1420, -0.0659,  0.0733,  0.0317]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0877,  0.0502, -0.0618, -0.0413,  0.1183],\n",
      "          [ 0.1802, -0.0496,  0.1836, -0.0242, -0.1479],\n",
      "          [-0.1537, -0.0741, -0.1641, -0.1254,  0.1039],\n",
      "          [-0.0693, -0.0144,  0.0324,  0.1287, -0.0938],\n",
      "          [ 0.1804, -0.1108, -0.0689, -0.1077,  0.0831]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0527,  0.1926,  0.0053,  0.0525, -0.0321],\n",
      "          [ 0.0120,  0.0013,  0.1825, -0.1574, -0.0424],\n",
      "          [ 0.1855,  0.1606, -0.0261,  0.0691, -0.1108],\n",
      "          [ 0.0399,  0.1221,  0.0359, -0.1821,  0.1601],\n",
      "          [-0.0959,  0.0081,  0.0045, -0.0336,  0.1305]]]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1937, -0.1225, -0.1504, -0.0871,  0.1506, -0.0757],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0090,  0.0094,  0.0321, -0.0535, -0.0211],\n",
      "          [-0.0119,  0.0376,  0.0023, -0.0109, -0.0380],\n",
      "          [ 0.0447, -0.0111,  0.0468, -0.0634,  0.0091],\n",
      "          [ 0.0751, -0.0473,  0.0771, -0.0045, -0.0759],\n",
      "          [-0.0030,  0.0057, -0.0722, -0.0526, -0.0638]],\n",
      "\n",
      "         [[-0.0732,  0.0040, -0.0166, -0.0497, -0.0650],\n",
      "          [-0.0788,  0.0410,  0.0163, -0.0450,  0.0045],\n",
      "          [-0.0666,  0.0610,  0.0043, -0.0351, -0.0639],\n",
      "          [ 0.0199,  0.0716, -0.0712,  0.0203,  0.0814],\n",
      "          [-0.0236, -0.0086,  0.0132, -0.0261,  0.0378]],\n",
      "\n",
      "         [[-0.0215, -0.0590,  0.0752, -0.0247,  0.0629],\n",
      "          [ 0.0430,  0.0502,  0.0309, -0.0733, -0.0169],\n",
      "          [ 0.0687,  0.0334,  0.0589,  0.0153, -0.0201],\n",
      "          [-0.0267, -0.0676,  0.0588, -0.0722, -0.0060],\n",
      "          [ 0.0150,  0.0041, -0.0288, -0.0540,  0.0544]],\n",
      "\n",
      "         [[-0.0772, -0.0166,  0.0071,  0.0329, -0.0154],\n",
      "          [-0.0192, -0.0382,  0.0627, -0.0657, -0.0265],\n",
      "          [ 0.0621,  0.0269,  0.0152,  0.0567,  0.0777],\n",
      "          [ 0.0778, -0.0285,  0.0054,  0.0181, -0.0720],\n",
      "          [ 0.0466, -0.0724, -0.0014,  0.0105,  0.0421]],\n",
      "\n",
      "         [[ 0.0762,  0.0217, -0.0092, -0.0297,  0.0147],\n",
      "          [-0.0477,  0.0573,  0.0455, -0.0633,  0.0801],\n",
      "          [-0.0022, -0.0544,  0.0435, -0.0183, -0.0438],\n",
      "          [-0.0621, -0.0297,  0.0089, -0.0689,  0.0784],\n",
      "          [-0.0332,  0.0291,  0.0511,  0.0190,  0.0549]],\n",
      "\n",
      "         [[ 0.0274, -0.0645,  0.0197,  0.0685, -0.0294],\n",
      "          [ 0.0088,  0.0526,  0.0424, -0.0525,  0.0232],\n",
      "          [ 0.0616, -0.0356,  0.0148,  0.0270,  0.0565],\n",
      "          [-0.0703,  0.0070, -0.0293, -0.0126, -0.0771],\n",
      "          [ 0.0344,  0.0343, -0.0663, -0.0516,  0.0282]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0668, -0.0255,  0.0786,  0.0252,  0.0587],\n",
      "          [-0.0616, -0.0516, -0.0480,  0.0397,  0.0432],\n",
      "          [-0.0578, -0.0576, -0.0170,  0.0068,  0.0815],\n",
      "          [-0.0751, -0.0318,  0.0754,  0.0058, -0.0341],\n",
      "          [-0.0011, -0.0688,  0.0013,  0.0667,  0.0047]],\n",
      "\n",
      "         [[ 0.0240,  0.0622, -0.0300,  0.0253, -0.0273],\n",
      "          [-0.0460,  0.0030,  0.0600,  0.0786, -0.0359],\n",
      "          [-0.0252, -0.0059,  0.0367,  0.0234, -0.0780],\n",
      "          [-0.0457,  0.0385,  0.0088,  0.0580,  0.0606],\n",
      "          [ 0.0706,  0.0808,  0.0651,  0.0216, -0.0286]],\n",
      "\n",
      "         [[ 0.0005, -0.0225,  0.0240,  0.0629, -0.0431],\n",
      "          [ 0.0600,  0.0523, -0.0310,  0.0224, -0.0411],\n",
      "          [-0.0466,  0.0670,  0.0060,  0.0245,  0.0602],\n",
      "          [-0.0351,  0.0409,  0.0288, -0.0302, -0.0265],\n",
      "          [ 0.0145, -0.0728, -0.0760,  0.0639, -0.0268]],\n",
      "\n",
      "         [[ 0.0373,  0.0596, -0.0642, -0.0146, -0.0745],\n",
      "          [-0.0401,  0.0567,  0.0074,  0.0042, -0.0382],\n",
      "          [-0.0610,  0.0360,  0.0352,  0.0206, -0.0586],\n",
      "          [ 0.0778, -0.0406,  0.0535, -0.0675, -0.0557],\n",
      "          [-0.0345, -0.0078, -0.0477, -0.0516,  0.0256]],\n",
      "\n",
      "         [[-0.0808,  0.0606,  0.0003,  0.0265, -0.0609],\n",
      "          [ 0.0136, -0.0178, -0.0701, -0.0175,  0.0527],\n",
      "          [-0.0458,  0.0406, -0.0531, -0.0562, -0.0755],\n",
      "          [ 0.0745, -0.0712,  0.0458,  0.0492,  0.0567],\n",
      "          [-0.0128,  0.0370,  0.0383, -0.0152, -0.0725]],\n",
      "\n",
      "         [[ 0.0542,  0.0657,  0.0414,  0.0663,  0.0007],\n",
      "          [-0.0448,  0.0573, -0.0587,  0.0674,  0.0023],\n",
      "          [-0.0754,  0.0091, -0.0146, -0.0767, -0.0706],\n",
      "          [-0.0342, -0.0124, -0.0372, -0.0614,  0.0685],\n",
      "          [ 0.0813,  0.0674,  0.0325,  0.0079,  0.0111]]],\n",
      "\n",
      "\n",
      "        [[[-0.0394, -0.0806, -0.0163, -0.0563, -0.0761],\n",
      "          [-0.0641, -0.0513,  0.0497, -0.0653,  0.0229],\n",
      "          [-0.0745, -0.0015, -0.0433, -0.0203, -0.0191],\n",
      "          [ 0.0769,  0.0411,  0.0037,  0.0808,  0.0025],\n",
      "          [ 0.0435, -0.0169, -0.0379,  0.0648, -0.0476]],\n",
      "\n",
      "         [[-0.0147,  0.0129,  0.0547, -0.0744, -0.0167],\n",
      "          [-0.0409,  0.0219, -0.0592,  0.0385, -0.0801],\n",
      "          [ 0.0135, -0.0721, -0.0277,  0.0097,  0.0417],\n",
      "          [ 0.0290,  0.0023,  0.0287, -0.0761, -0.0500],\n",
      "          [ 0.0802,  0.0513, -0.0707, -0.0646,  0.0119]],\n",
      "\n",
      "         [[-0.0749, -0.0015,  0.0013,  0.0281, -0.0366],\n",
      "          [ 0.0282,  0.0266,  0.0810, -0.0533, -0.0510],\n",
      "          [ 0.0251, -0.0765,  0.0033,  0.0616,  0.0784],\n",
      "          [-0.0300,  0.0078, -0.0357, -0.0716, -0.0411],\n",
      "          [-0.0108, -0.0433, -0.0408, -0.0364,  0.0439]],\n",
      "\n",
      "         [[ 0.0791,  0.0531, -0.0069, -0.0467, -0.0101],\n",
      "          [-0.0796, -0.0509, -0.0391, -0.0219,  0.0732],\n",
      "          [ 0.0258, -0.0101,  0.0770, -0.0252,  0.0102],\n",
      "          [-0.0485,  0.0191, -0.0074,  0.0351, -0.0256],\n",
      "          [-0.0735, -0.0628, -0.0288, -0.0520,  0.0192]],\n",
      "\n",
      "         [[ 0.0263, -0.0744,  0.0073,  0.0003,  0.0541],\n",
      "          [-0.0536, -0.0395,  0.0303,  0.0408, -0.0725],\n",
      "          [ 0.0073,  0.0284,  0.0751, -0.0192,  0.0493],\n",
      "          [ 0.0171,  0.0769, -0.0309,  0.0478, -0.0460],\n",
      "          [-0.0187, -0.0002, -0.0632, -0.0206,  0.0208]],\n",
      "\n",
      "         [[ 0.0319,  0.0645,  0.0600,  0.0816, -0.0533],\n",
      "          [-0.0598, -0.0682,  0.0379, -0.0563,  0.0077],\n",
      "          [ 0.0729, -0.0137,  0.0309,  0.0051,  0.0458],\n",
      "          [-0.0479,  0.0690,  0.0140, -0.0613,  0.0238],\n",
      "          [ 0.0799,  0.0360, -0.0056, -0.0616,  0.0355]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0173, -0.0430, -0.0669,  0.0281, -0.0182],\n",
      "          [ 0.0590,  0.0180, -0.0158, -0.0688, -0.0699],\n",
      "          [ 0.0101,  0.0691, -0.0784, -0.0715,  0.0429],\n",
      "          [ 0.0178, -0.0553, -0.0100, -0.0359, -0.0713],\n",
      "          [-0.0749, -0.0704,  0.0801, -0.0790,  0.0141]],\n",
      "\n",
      "         [[ 0.0623, -0.0670, -0.0743,  0.0744,  0.0464],\n",
      "          [-0.0043,  0.0644, -0.0166,  0.0776,  0.0197],\n",
      "          [-0.0109, -0.0572,  0.0009, -0.0440,  0.0679],\n",
      "          [ 0.0648, -0.0323, -0.0332,  0.0138,  0.0636],\n",
      "          [ 0.0537, -0.0374, -0.0117, -0.0499,  0.0398]],\n",
      "\n",
      "         [[-0.0467,  0.0111,  0.0164, -0.0735, -0.0424],\n",
      "          [-0.0387,  0.0307, -0.0385,  0.0701,  0.0281],\n",
      "          [-0.0375, -0.0624,  0.0547,  0.0358,  0.0814],\n",
      "          [ 0.0537, -0.0605,  0.0719, -0.0062, -0.0402],\n",
      "          [-0.0339,  0.0130,  0.0430,  0.0483, -0.0114]],\n",
      "\n",
      "         [[ 0.0796, -0.0338,  0.0626, -0.0505,  0.0657],\n",
      "          [-0.0212, -0.0638, -0.0552, -0.0300,  0.0589],\n",
      "          [-0.0628, -0.0573, -0.0730,  0.0100, -0.0399],\n",
      "          [-0.0395,  0.0261,  0.0334, -0.0109,  0.0807],\n",
      "          [-0.0385,  0.0754, -0.0580,  0.0310, -0.0469]],\n",
      "\n",
      "         [[-0.0056, -0.0460, -0.0660,  0.0049,  0.0002],\n",
      "          [ 0.0770,  0.0355,  0.0287, -0.0031, -0.0432],\n",
      "          [-0.0313, -0.0540,  0.0281,  0.0477,  0.0118],\n",
      "          [ 0.0226, -0.0420, -0.0809,  0.0335, -0.0714],\n",
      "          [ 0.0791,  0.0169, -0.0412, -0.0525,  0.0807]],\n",
      "\n",
      "         [[ 0.0157, -0.0636,  0.0494,  0.0612, -0.0130],\n",
      "          [ 0.0759,  0.0262, -0.0087, -0.0609,  0.0514],\n",
      "          [ 0.0535,  0.0317, -0.0543, -0.0223, -0.0182],\n",
      "          [-0.0572, -0.0333, -0.0779, -0.0348, -0.0265],\n",
      "          [-0.0750,  0.0242,  0.0024, -0.0003,  0.0155]]],\n",
      "\n",
      "\n",
      "        [[[-0.0215, -0.0746, -0.0340, -0.0475,  0.0649],\n",
      "          [ 0.0204,  0.0205,  0.0484, -0.0291,  0.0416],\n",
      "          [ 0.0007, -0.0735, -0.0609,  0.0753,  0.0690],\n",
      "          [-0.0256, -0.0223, -0.0540,  0.0339, -0.0740],\n",
      "          [ 0.0677,  0.0458,  0.0612,  0.0665, -0.0302]],\n",
      "\n",
      "         [[-0.0612, -0.0681,  0.0680, -0.0599,  0.0562],\n",
      "          [ 0.0217, -0.0638,  0.0501,  0.0589, -0.0524],\n",
      "          [-0.0232, -0.0471,  0.0352,  0.0708, -0.0580],\n",
      "          [-0.0716,  0.0111,  0.0598, -0.0212,  0.0139],\n",
      "          [ 0.0590, -0.0685,  0.0071, -0.0184, -0.0250]],\n",
      "\n",
      "         [[ 0.0594,  0.0464,  0.0135, -0.0081,  0.0077],\n",
      "          [ 0.0097, -0.0622,  0.0046, -0.0616, -0.0080],\n",
      "          [-0.0642,  0.0179, -0.0438,  0.0589, -0.0266],\n",
      "          [-0.0185,  0.0159,  0.0647,  0.0510,  0.0292],\n",
      "          [ 0.0801,  0.0267,  0.0442, -0.0118, -0.0472]],\n",
      "\n",
      "         [[-0.0062, -0.0021,  0.0752,  0.0301,  0.0358],\n",
      "          [-0.0729,  0.0472, -0.0707,  0.0748, -0.0002],\n",
      "          [ 0.0235,  0.0444, -0.0612, -0.0668,  0.0780],\n",
      "          [ 0.0029, -0.0055,  0.0658, -0.0552, -0.0529],\n",
      "          [ 0.0607, -0.0113,  0.0745,  0.0615, -0.0332]],\n",
      "\n",
      "         [[-0.0711,  0.0816, -0.0146,  0.0497, -0.0638],\n",
      "          [-0.0711, -0.0186, -0.0334,  0.0740, -0.0317],\n",
      "          [-0.0620,  0.0312, -0.0127, -0.0484, -0.0283],\n",
      "          [-0.0320,  0.0168, -0.0628, -0.0248, -0.0045],\n",
      "          [ 0.0180, -0.0003,  0.0769, -0.0736, -0.0798]],\n",
      "\n",
      "         [[ 0.0461, -0.0400,  0.0258,  0.0387, -0.0450],\n",
      "          [-0.0774,  0.0014, -0.0647,  0.0431, -0.0782],\n",
      "          [-0.0530,  0.0429, -0.0253,  0.0466,  0.0056],\n",
      "          [ 0.0041,  0.0383,  0.0331, -0.0661, -0.0598],\n",
      "          [ 0.0081,  0.0421, -0.0102,  0.0785,  0.0722]]],\n",
      "\n",
      "\n",
      "        [[[-0.0494,  0.0342, -0.0218,  0.0332,  0.0203],\n",
      "          [ 0.0585,  0.0462, -0.0250, -0.0095,  0.0743],\n",
      "          [ 0.0034, -0.0809,  0.0156,  0.0791,  0.0455],\n",
      "          [-0.0149, -0.0508,  0.0539,  0.0098, -0.0215],\n",
      "          [ 0.0555,  0.0441,  0.0788,  0.0702, -0.0613]],\n",
      "\n",
      "         [[-0.0034, -0.0729,  0.0615, -0.0270, -0.0409],\n",
      "          [ 0.0636, -0.0714, -0.0762,  0.0379,  0.0562],\n",
      "          [-0.0417,  0.0808,  0.0144,  0.0193,  0.0141],\n",
      "          [-0.0163,  0.0448,  0.0263, -0.0005,  0.0780],\n",
      "          [ 0.0265, -0.0809,  0.0482,  0.0149,  0.0232]],\n",
      "\n",
      "         [[-0.0643,  0.0005,  0.0795, -0.0016, -0.0174],\n",
      "          [ 0.0130,  0.0315, -0.0578, -0.0503, -0.0091],\n",
      "          [ 0.0001, -0.0350,  0.0484,  0.0029, -0.0007],\n",
      "          [ 0.0773, -0.0441, -0.0293,  0.0320, -0.0681],\n",
      "          [ 0.0634, -0.0134,  0.0553, -0.0042, -0.0018]],\n",
      "\n",
      "         [[ 0.0536,  0.0488,  0.0162,  0.0724, -0.0076],\n",
      "          [ 0.0429, -0.0428,  0.0211, -0.0325, -0.0535],\n",
      "          [-0.0754, -0.0403,  0.0785, -0.0651,  0.0789],\n",
      "          [ 0.0762, -0.0230, -0.0617, -0.0619, -0.0081],\n",
      "          [ 0.0722,  0.0301,  0.0488,  0.0552, -0.0674]],\n",
      "\n",
      "         [[ 0.0076,  0.0099,  0.0414, -0.0164,  0.0610],\n",
      "          [-0.0198,  0.0198, -0.0368,  0.0607,  0.0505],\n",
      "          [-0.0680,  0.0303, -0.0320,  0.0611, -0.0117],\n",
      "          [-0.0391,  0.0542,  0.0482, -0.0070, -0.0563],\n",
      "          [-0.0086, -0.0272,  0.0691,  0.0691,  0.0020]],\n",
      "\n",
      "         [[ 0.0435,  0.0803, -0.0723, -0.0447, -0.0657],\n",
      "          [-0.0702, -0.0099,  0.0724, -0.0805,  0.0076],\n",
      "          [-0.0286,  0.0201, -0.0596,  0.0116,  0.0575],\n",
      "          [ 0.0702,  0.0388,  0.0676, -0.0681,  0.0617],\n",
      "          [-0.0183,  0.0230, -0.0162, -0.0793, -0.0271]]]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0538,  0.0034, -0.0184,  0.0422,  0.0204,  0.0733, -0.0330, -0.0123,\n",
      "         0.0777,  0.0518,  0.0503, -0.0173,  0.0287, -0.0385,  0.0345, -0.0748],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 4.0261e-02,  3.0279e-02, -4.9983e-02,  ...,  6.2900e-03,\n",
      "         -5.3258e-04, -4.7123e-02],\n",
      "        [ 2.7571e-02, -1.3566e-02, -1.4466e-02,  ..., -4.5363e-02,\n",
      "         -1.8754e-02,  4.7895e-02],\n",
      "        [-8.7958e-03,  1.7781e-02, -3.7683e-02,  ..., -1.1211e-02,\n",
      "          4.9079e-02,  3.0938e-02],\n",
      "        ...,\n",
      "        [-4.9207e-02, -1.1341e-02,  3.1855e-02,  ...,  4.8097e-02,\n",
      "         -4.9991e-03, -1.0737e-02],\n",
      "        [-4.3429e-02, -1.8544e-02, -2.9933e-02,  ...,  1.0944e-02,\n",
      "         -4.4038e-02, -6.5051e-03],\n",
      "        [-1.1392e-05, -4.1996e-02,  1.9847e-04,  ..., -2.2988e-02,\n",
      "         -2.5931e-02, -4.3163e-02]], requires_grad=True), Parameter containing:\n",
      "tensor([ 3.8600e-02, -4.7601e-02,  3.3190e-02,  1.4191e-02, -5.7142e-05,\n",
      "        -4.1726e-02,  4.1131e-03,  1.5377e-02, -3.9810e-02,  4.0622e-02,\n",
      "         3.8793e-02, -3.6682e-02, -3.7070e-02, -1.0644e-02,  2.9345e-02,\n",
      "        -6.8233e-03,  4.1380e-02, -1.1982e-02,  1.4369e-02, -2.7734e-02,\n",
      "         3.2706e-02,  1.1742e-02, -2.2494e-02, -2.7515e-02, -3.3270e-02,\n",
      "        -1.0182e-03,  2.3305e-03, -2.2479e-02, -1.5514e-02, -4.4556e-02,\n",
      "        -2.9185e-02,  4.6921e-03, -3.7478e-02, -1.5931e-02, -4.9502e-03,\n",
      "        -2.3748e-02, -3.1538e-02,  3.2074e-02, -2.2146e-02, -4.9931e-02,\n",
      "         2.9066e-02,  4.2301e-02, -6.8529e-03, -3.9041e-02, -4.8320e-02,\n",
      "         2.9452e-02, -4.8347e-02,  1.0027e-02, -4.2366e-02, -2.0061e-02,\n",
      "         1.5945e-02,  2.6397e-02, -5.1250e-03,  3.6581e-02,  3.4633e-02,\n",
      "         3.9744e-02, -1.4745e-02, -4.1728e-02, -2.6492e-02,  4.9308e-02,\n",
      "         9.7995e-04, -3.7353e-02, -2.1192e-02,  3.3814e-02, -4.9681e-02,\n",
      "        -2.6476e-02, -2.7608e-02,  4.6299e-02,  4.8414e-02,  3.2442e-02,\n",
      "         2.2826e-02,  3.4370e-02, -3.4917e-02,  1.0902e-03,  2.2636e-02,\n",
      "        -2.6123e-02,  9.4941e-03, -2.7578e-02,  3.4369e-02,  4.7804e-02,\n",
      "         2.2847e-03,  9.9875e-03,  1.9339e-02,  2.7147e-02, -5.0043e-03,\n",
      "         1.5843e-02,  2.3974e-02,  1.6400e-02, -2.4980e-02, -3.2078e-02,\n",
      "        -2.6210e-02, -2.8415e-02, -4.2841e-03,  1.9123e-02,  2.6014e-02,\n",
      "        -2.0883e-03,  3.5396e-02, -1.5551e-02, -1.1809e-02,  1.3857e-02,\n",
      "        -4.2158e-02,  2.9782e-02, -4.7188e-02,  4.4319e-03,  3.1965e-02,\n",
      "         5.3941e-03,  7.2705e-03, -1.0142e-02, -3.0608e-02, -4.3706e-02,\n",
      "        -4.9386e-02, -2.1708e-02, -4.6707e-02,  6.0073e-03, -1.1556e-02,\n",
      "         2.5381e-02,  2.7697e-02, -3.5557e-02, -4.6896e-02, -3.2715e-02],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0532, -0.0464, -0.0345,  ..., -0.0606, -0.0164, -0.0289],\n",
      "        [ 0.0455,  0.0245, -0.0418,  ...,  0.0220, -0.0277, -0.0017],\n",
      "        [-0.0113, -0.0169,  0.0171,  ...,  0.0488, -0.0357, -0.0616],\n",
      "        ...,\n",
      "        [ 0.0583,  0.0206,  0.0884,  ..., -0.0107, -0.0262, -0.0286],\n",
      "        [-0.0693,  0.0666,  0.0791,  ...,  0.0060, -0.0715,  0.0320],\n",
      "        [-0.0516,  0.0401, -0.0493,  ..., -0.0452, -0.0198, -0.0596]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0526,  0.0222,  0.0586,  0.0657, -0.0374,  0.0223,  0.0874, -0.0596,\n",
      "        -0.0581,  0.0392,  0.0092,  0.0729, -0.0345, -0.0782,  0.0733, -0.0021,\n",
      "         0.0492,  0.0662,  0.0510, -0.0878, -0.0233,  0.0321, -0.0879,  0.0463,\n",
      "         0.0057, -0.0903, -0.0027, -0.0429,  0.0541,  0.0512, -0.0864, -0.0719,\n",
      "         0.0836, -0.0907,  0.0855,  0.0815, -0.0201, -0.0765,  0.0085,  0.0849,\n",
      "        -0.0827, -0.0632, -0.0187, -0.0383, -0.0049,  0.0291, -0.0572,  0.0088,\n",
      "         0.0289,  0.0313, -0.0504, -0.0811,  0.0483,  0.0403,  0.0063,  0.0353,\n",
      "        -0.0762,  0.0354, -0.0639,  0.0397, -0.0639,  0.0189, -0.0335,  0.0175,\n",
      "        -0.0411, -0.0589,  0.0644,  0.0397, -0.0717, -0.0560,  0.0087, -0.0393,\n",
      "        -0.0621,  0.0677,  0.0747,  0.0136,  0.0093,  0.0492,  0.0307,  0.0715,\n",
      "         0.0801,  0.0310,  0.0244, -0.0065], requires_grad=True), Parameter containing:\n",
      "tensor([[-7.5680e-02,  7.7950e-02,  6.1832e-02, -4.7377e-02,  4.8159e-02,\n",
      "          6.9454e-02, -3.2534e-02,  2.0379e-02, -7.5961e-02, -4.6476e-02,\n",
      "         -6.9640e-02, -4.6610e-02,  1.0568e-02, -4.2476e-02, -6.3887e-02,\n",
      "          8.7082e-02, -4.1206e-02,  1.0341e-01, -2.2951e-02, -2.3114e-02,\n",
      "          1.0661e-01, -1.0530e-01, -5.5595e-03,  5.2345e-02, -5.5332e-02,\n",
      "          3.6761e-02, -6.6831e-02, -2.5676e-02, -8.1244e-02,  2.0673e-02,\n",
      "         -8.4823e-02,  2.3408e-02,  9.0868e-02,  4.1004e-03, -5.0293e-02,\n",
      "         -5.6880e-02,  7.6352e-02,  7.0099e-02, -7.0275e-02, -1.7515e-02,\n",
      "         -3.5322e-02, -5.0922e-02,  7.6954e-02, -6.2586e-02, -5.8908e-02,\n",
      "         -2.2426e-02, -4.2690e-02,  1.3231e-02, -5.8378e-02, -6.8259e-03,\n",
      "          1.0612e-01,  4.5808e-02,  4.6438e-02,  9.8022e-03, -4.5039e-02,\n",
      "          8.9178e-03, -1.0425e-01,  9.5781e-02,  8.2206e-02, -5.7439e-03,\n",
      "          7.1006e-02,  2.3825e-02, -1.3501e-02,  4.0841e-02, -1.2726e-02,\n",
      "         -4.4742e-02,  8.4165e-03, -3.2607e-02,  5.8409e-02, -6.7420e-02,\n",
      "         -7.8976e-02, -1.2807e-02,  9.0191e-02,  9.0883e-03,  2.4910e-02,\n",
      "         -1.0448e-01,  7.6756e-02,  7.9997e-02,  5.8405e-02, -4.5897e-02,\n",
      "         -1.0026e-01,  7.5888e-02,  6.0256e-02,  3.7405e-02],\n",
      "        [-2.6420e-02,  3.0010e-03,  5.8777e-02, -3.9437e-02,  8.1442e-02,\n",
      "          6.1441e-02, -9.4589e-02, -9.2447e-02,  7.0287e-02, -5.2133e-02,\n",
      "          5.8463e-02,  4.7068e-02, -3.7567e-02,  8.3459e-02,  6.3138e-02,\n",
      "         -1.0669e-01, -2.8366e-02,  6.7364e-02,  4.0295e-02,  2.4356e-02,\n",
      "         -4.2145e-02,  9.6501e-03,  4.8393e-02, -3.4268e-02, -7.3768e-02,\n",
      "          3.5972e-02,  2.6663e-02,  6.7622e-02,  3.3602e-02, -1.0683e-01,\n",
      "          1.0203e-01,  5.2670e-02,  1.3068e-02,  6.6797e-04, -6.2188e-02,\n",
      "          6.5220e-02,  1.0009e-01, -4.2738e-02,  1.0471e-01, -8.6577e-02,\n",
      "          5.9418e-02,  6.7152e-02, -4.9220e-02, -1.0071e-01,  8.2864e-02,\n",
      "         -1.9370e-02, -7.7718e-02,  3.4097e-02, -5.5621e-03,  3.3517e-02,\n",
      "         -1.1199e-02, -1.0622e-01, -4.4453e-02, -3.8799e-02,  3.3086e-02,\n",
      "         -9.3499e-02, -4.8918e-02,  1.3851e-02, -9.7610e-02, -9.5463e-02,\n",
      "          4.3831e-02, -6.9399e-02,  6.1621e-02, -8.2042e-02, -1.1617e-02,\n",
      "          9.6757e-02,  1.2927e-03,  1.9989e-02, -5.8724e-02,  1.6697e-03,\n",
      "         -9.2437e-02,  7.7367e-02,  4.7858e-02,  9.3013e-02,  8.9807e-02,\n",
      "          4.4009e-02,  9.1107e-02, -9.4870e-02,  9.9420e-02, -3.0873e-02,\n",
      "          1.0806e-01,  9.4780e-02,  7.4735e-02, -8.3734e-02],\n",
      "        [ 6.6125e-02,  6.4899e-02, -2.6020e-02, -6.5754e-02,  5.9769e-03,\n",
      "         -4.5243e-02,  1.7221e-02,  2.7976e-02,  5.5749e-02, -7.9478e-02,\n",
      "         -7.7478e-02,  1.0030e-01,  3.5052e-02, -3.3734e-03,  3.8957e-02,\n",
      "         -2.8906e-02,  6.1057e-02,  4.1695e-02, -3.9734e-02,  4.8431e-03,\n",
      "         -2.4689e-02,  1.0291e-01, -7.0648e-03,  5.1570e-02, -4.2968e-02,\n",
      "          3.1326e-02,  9.1601e-02, -1.0085e-01,  6.6445e-02,  3.5713e-02,\n",
      "          2.5720e-02, -6.7606e-02, -6.7498e-02, -5.8067e-02,  1.0790e-01,\n",
      "         -2.2042e-02, -7.0737e-02,  4.5833e-02, -4.5918e-02,  9.0300e-02,\n",
      "          1.0500e-01, -8.0514e-02,  1.4164e-03,  7.4751e-02, -9.7005e-02,\n",
      "          9.6180e-02, -9.6637e-02, -7.6394e-02,  5.5633e-02,  4.1798e-02,\n",
      "          5.2362e-02, -8.8541e-02, -9.9301e-03, -3.6266e-02,  1.0004e-01,\n",
      "          6.9308e-02, -1.0075e-01, -3.9355e-02,  2.3449e-02, -9.3733e-02,\n",
      "         -6.2539e-03,  7.6598e-02, -5.1101e-02, -8.5936e-03,  5.9488e-02,\n",
      "         -5.4033e-02,  1.8271e-02, -9.0031e-04, -9.9203e-02,  6.2468e-02,\n",
      "         -7.1595e-02,  2.1153e-02, -5.4529e-02,  7.6410e-02, -9.0713e-02,\n",
      "          9.8866e-02,  9.6606e-04, -7.8371e-02,  8.3163e-03,  8.3119e-02,\n",
      "         -5.3826e-02,  1.0580e-01, -4.7641e-02,  1.0772e-01],\n",
      "        [-1.3292e-02,  2.6426e-02, -8.1468e-02,  5.6072e-02,  9.7312e-02,\n",
      "          2.5701e-02,  4.3695e-02,  8.1836e-02, -1.8652e-02, -8.5065e-02,\n",
      "         -6.4339e-02,  8.7754e-02, -1.0693e-01,  9.9442e-02,  8.5975e-02,\n",
      "          3.0053e-02, -8.7110e-02, -2.7401e-02, -6.6881e-04,  6.9305e-02,\n",
      "         -3.0214e-03, -4.8789e-02, -1.0572e-01,  7.3975e-02, -5.8193e-02,\n",
      "          6.4226e-02,  9.5028e-02, -9.6982e-02,  9.4574e-02,  9.7201e-02,\n",
      "         -7.7784e-02, -8.3620e-02,  8.8566e-02,  3.8449e-03, -3.8566e-02,\n",
      "          8.2425e-02,  2.7984e-02,  6.1169e-02, -7.4464e-03,  7.5237e-02,\n",
      "          3.7046e-02,  6.8083e-02, -5.3643e-02,  7.4714e-02,  7.8209e-02,\n",
      "         -3.0889e-03,  4.2293e-02,  5.2264e-02,  1.5695e-02, -5.4713e-02,\n",
      "          6.7125e-03,  1.6271e-04, -7.7745e-02, -4.5009e-02, -1.7917e-03,\n",
      "          6.5347e-02, -7.7137e-02, -7.5238e-02, -4.0181e-02, -2.6860e-02,\n",
      "          8.1763e-02, -1.0715e-01,  1.4739e-02, -9.5610e-02, -7.3551e-04,\n",
      "          5.8079e-02,  3.6460e-03, -1.6159e-02,  9.8024e-03,  1.0655e-01,\n",
      "          9.4181e-02,  7.8663e-02, -6.0305e-02,  9.1060e-02,  1.0228e-01,\n",
      "         -3.2053e-02, -7.1706e-02, -1.5853e-02, -2.5389e-02, -8.0776e-02,\n",
      "         -5.7169e-02, -8.5552e-02,  4.4548e-02, -9.4913e-02],\n",
      "        [-2.6768e-02, -6.6145e-02,  6.9563e-02,  6.9594e-02,  6.9546e-02,\n",
      "         -5.8148e-02, -9.6351e-02,  8.0548e-03,  6.7997e-02, -2.1068e-02,\n",
      "          8.2002e-02,  1.9977e-02, -9.8877e-02,  1.4916e-02, -8.7352e-03,\n",
      "          9.6152e-02,  1.0664e-01, -4.8735e-02, -9.7077e-02, -2.5608e-02,\n",
      "         -1.0664e-01, -7.6839e-02, -5.0200e-02, -8.1163e-02,  6.6864e-02,\n",
      "         -1.0810e-02,  9.5547e-02,  8.7503e-02, -4.9977e-02,  7.3483e-02,\n",
      "          8.4226e-02,  1.4283e-02,  2.1384e-02, -4.9501e-02, -1.3148e-02,\n",
      "         -1.0148e-01, -2.0824e-02,  7.0386e-02,  5.6739e-02, -1.4739e-02,\n",
      "          5.9431e-03, -8.3607e-02, -6.2088e-02, -9.5499e-02,  9.2293e-02,\n",
      "          4.1080e-02, -6.3452e-02, -8.6650e-02,  5.2435e-02, -9.2073e-02,\n",
      "          8.5454e-02, -1.6448e-02, -4.9789e-02,  8.6978e-02,  1.9182e-02,\n",
      "         -3.7104e-02, -5.6794e-02, -8.5740e-02, -1.0496e-02,  9.7445e-02,\n",
      "          8.1036e-03, -6.6503e-02,  6.2340e-02,  2.1068e-02, -6.2449e-02,\n",
      "         -2.8532e-02, -1.0023e-01, -8.9853e-02, -9.8429e-02, -8.2875e-02,\n",
      "         -4.5365e-02, -3.7889e-02, -9.1498e-02,  3.7407e-02, -9.0268e-02,\n",
      "          8.5713e-02,  1.6583e-02,  8.7785e-02,  3.6988e-02, -8.0147e-02,\n",
      "         -2.1009e-02,  5.5331e-04,  1.4988e-02,  1.0753e-01],\n",
      "        [ 4.4471e-02,  2.8763e-02, -5.7505e-02, -3.3376e-02,  9.3116e-02,\n",
      "         -9.5651e-02,  2.8656e-02,  1.0703e-01,  2.2562e-02, -4.7202e-02,\n",
      "          1.0625e-01,  7.6341e-02, -3.2980e-04, -1.0171e-01, -2.2612e-02,\n",
      "         -5.5512e-02,  3.6451e-02, -3.1965e-02,  7.9412e-02,  6.8681e-02,\n",
      "          3.3344e-02,  5.1678e-03,  7.5981e-02,  1.6359e-02, -3.0875e-02,\n",
      "         -4.7575e-02,  9.6292e-02, -5.1653e-02,  6.6436e-02,  2.1711e-02,\n",
      "          8.3161e-02, -1.2161e-02, -4.8596e-02,  7.1216e-02,  5.9689e-02,\n",
      "          6.2755e-02,  7.3663e-03,  5.2815e-02, -2.3731e-02,  2.5268e-02,\n",
      "         -2.1276e-02,  2.5064e-02,  4.8579e-02, -6.2810e-02, -1.0754e-01,\n",
      "          5.2213e-02, -4.9146e-02, -1.0460e-01, -1.5789e-02, -5.2551e-02,\n",
      "          1.6096e-02,  4.3418e-02, -1.0736e-01,  1.0755e-01, -4.1508e-02,\n",
      "         -3.6133e-02,  5.7755e-02, -9.1761e-02,  1.0885e-01, -5.5533e-02,\n",
      "         -5.8065e-02,  9.1194e-02, -1.0738e-01, -6.7086e-02, -7.6981e-02,\n",
      "          6.2592e-02,  9.0073e-02,  1.2368e-02,  8.7318e-02, -8.1853e-02,\n",
      "         -1.0221e-01, -3.9869e-02,  8.3773e-02,  6.0246e-02,  1.0043e-01,\n",
      "         -8.4603e-02,  5.1719e-02, -9.5992e-02, -6.2736e-03,  4.7983e-02,\n",
      "          5.8228e-02, -4.5345e-02, -7.9488e-02,  4.4839e-02],\n",
      "        [-7.3853e-02, -3.4892e-02, -3.5800e-02,  4.6828e-03, -1.0241e-01,\n",
      "         -1.4963e-02, -9.6512e-02, -1.0236e-01,  9.8191e-02, -3.3013e-02,\n",
      "         -5.2323e-03, -4.7687e-02, -3.2073e-02, -6.2945e-02,  9.4983e-02,\n",
      "          1.0639e-01,  9.7072e-02,  8.4011e-02,  3.1415e-02,  5.6933e-02,\n",
      "          8.1304e-02,  6.6381e-02,  1.0064e-01,  4.0738e-02, -5.1627e-02,\n",
      "          4.4981e-02,  4.6229e-02,  8.8056e-02, -7.5757e-02,  3.0856e-02,\n",
      "          5.1206e-02,  7.5833e-02,  3.4873e-02, -9.5738e-02,  4.3049e-02,\n",
      "          5.9941e-03, -4.9022e-02, -1.9231e-02, -5.4676e-03,  6.7876e-03,\n",
      "          6.5898e-02, -5.6637e-02, -1.4953e-02, -3.4528e-02, -9.9152e-02,\n",
      "          2.0527e-02,  6.2335e-02, -6.0046e-02,  1.0790e-01, -2.2173e-02,\n",
      "         -9.3810e-02,  2.9890e-02, -4.1710e-02, -5.5020e-02,  8.3970e-02,\n",
      "         -9.8980e-02, -7.8246e-02,  3.4843e-02, -5.4218e-02, -8.9560e-02,\n",
      "          1.7871e-02, -4.0596e-02,  8.8222e-02,  5.9901e-02,  3.2459e-02,\n",
      "         -9.1232e-02, -9.1206e-02, -8.6972e-02,  4.2675e-02,  8.4551e-02,\n",
      "         -2.9836e-02,  1.0625e-01, -9.9803e-02,  6.2346e-02,  9.2995e-02,\n",
      "          5.1763e-02, -4.3261e-02, -5.5978e-02,  6.2224e-03, -6.1143e-02,\n",
      "         -1.7760e-02, -3.3228e-02, -5.1892e-02, -1.6955e-02],\n",
      "        [ 4.9511e-04, -6.4138e-02, -5.8976e-02, -8.6404e-02, -1.2855e-03,\n",
      "          8.1571e-02,  6.9581e-03, -1.3424e-02, -4.0578e-02,  3.0023e-02,\n",
      "         -3.2692e-02, -9.6133e-02,  4.3883e-02, -3.9227e-02, -1.6885e-02,\n",
      "          2.6068e-02,  3.7826e-02, -8.8844e-03, -8.2971e-02,  3.5971e-04,\n",
      "          1.9521e-05,  5.2931e-02,  3.5109e-02,  9.8350e-02, -6.7430e-02,\n",
      "         -2.8136e-02, -9.5854e-02,  5.4210e-02,  9.1910e-02,  1.1932e-02,\n",
      "         -9.2036e-02, -2.8596e-02, -5.3971e-02, -1.0691e-01, -3.1526e-02,\n",
      "         -4.4541e-02,  2.6336e-02,  1.7007e-02,  7.5637e-02,  1.0326e-01,\n",
      "         -6.7358e-02, -8.9372e-02, -8.1230e-04,  2.9780e-02,  2.5738e-02,\n",
      "          3.9066e-03,  1.5186e-02,  4.3040e-02, -2.3802e-02,  7.1177e-02,\n",
      "          2.6827e-02,  6.3179e-02,  4.1378e-02, -7.3010e-02,  1.0801e-01,\n",
      "          1.3720e-02, -6.7249e-02, -1.2938e-02, -5.3518e-02, -2.3004e-02,\n",
      "          5.9699e-03, -5.1393e-02, -1.2030e-02,  1.5594e-03, -7.5780e-02,\n",
      "          8.9087e-02, -8.1250e-03, -1.0803e-01, -1.2547e-02, -4.2537e-02,\n",
      "          5.7974e-02,  1.5836e-02, -4.0241e-02,  3.9425e-02, -1.3435e-02,\n",
      "         -8.7248e-02, -9.6747e-02, -3.7835e-03,  1.0065e-02,  5.3425e-02,\n",
      "         -5.2505e-02,  9.1210e-02, -8.3617e-02,  1.6120e-02],\n",
      "        [-1.0042e-01,  7.8992e-02,  8.0003e-02,  1.0489e-02,  8.7006e-02,\n",
      "          6.6799e-02,  4.0471e-02,  7.1147e-02, -8.7103e-02,  1.3335e-02,\n",
      "         -4.7058e-02,  8.2687e-02,  2.1253e-02,  7.0475e-03,  9.2206e-02,\n",
      "         -7.2783e-03, -8.8977e-02,  5.7108e-02, -5.0992e-02, -5.0539e-02,\n",
      "         -9.5289e-02,  3.3719e-02, -6.8797e-02, -3.1385e-02,  1.7427e-02,\n",
      "         -8.2868e-02, -4.1872e-02,  8.8743e-02,  6.5684e-03,  7.1235e-02,\n",
      "          5.5353e-02, -6.4504e-02, -7.6318e-02, -6.7026e-02, -2.8050e-02,\n",
      "         -5.4396e-02, -9.5307e-02,  1.0517e-01,  1.0789e-01,  3.0331e-02,\n",
      "         -7.1174e-03,  8.9794e-02, -3.9727e-02, -7.5693e-02,  5.4150e-02,\n",
      "         -4.8708e-03,  7.2351e-03, -1.0462e-01,  1.5214e-02, -4.1508e-02,\n",
      "         -5.3162e-02,  4.7298e-02, -7.1344e-02, -2.8987e-03, -1.0906e-01,\n",
      "          6.8111e-02, -7.8697e-02, -2.5651e-03,  7.3082e-02, -5.6747e-02,\n",
      "         -9.7414e-02, -5.1587e-02,  1.0098e-01,  5.7649e-03, -6.2262e-02,\n",
      "         -6.3005e-02, -5.0343e-03,  1.7091e-02, -6.6832e-02, -5.4600e-02,\n",
      "          7.6759e-02, -4.5995e-02, -6.6187e-02,  6.1675e-02, -5.8854e-02,\n",
      "          7.5920e-02, -2.2563e-02, -8.8019e-02,  8.9795e-02, -7.7601e-02,\n",
      "         -2.5176e-02, -2.1308e-02, -5.6888e-02, -8.2031e-02],\n",
      "        [ 3.1648e-02, -8.4083e-02, -1.3289e-02,  5.2620e-02, -5.6643e-02,\n",
      "          9.2115e-02,  5.0736e-02,  6.6197e-02,  8.7611e-02, -7.9514e-02,\n",
      "          1.0200e-01,  9.7294e-02,  6.4449e-02, -1.1754e-02, -9.8091e-02,\n",
      "          9.8597e-02, -8.0468e-02,  3.4705e-03, -1.0516e-01, -9.7696e-02,\n",
      "         -2.8149e-02, -3.2860e-03,  1.0291e-01, -2.2624e-02,  6.6088e-02,\n",
      "          7.9274e-02,  1.0320e-01,  6.6124e-03, -8.3925e-02,  8.7071e-02,\n",
      "          6.1893e-02, -1.1810e-02,  8.1443e-02, -2.5009e-03, -1.0578e-01,\n",
      "         -5.1696e-02,  2.0637e-02,  1.1884e-02, -2.2481e-02, -4.2964e-02,\n",
      "          9.4556e-02, -8.9234e-02,  1.0509e-01, -9.3010e-02,  7.5346e-02,\n",
      "         -1.0276e-01, -1.0591e-01,  6.5713e-02,  3.0124e-02,  7.8457e-02,\n",
      "         -3.8223e-02, -9.1029e-02,  2.5907e-02, -6.8258e-02,  1.0254e-01,\n",
      "         -3.6898e-02,  3.0143e-02,  1.9462e-02,  1.7026e-02,  3.5352e-02,\n",
      "          1.0907e-01,  4.0426e-02, -3.6181e-02,  4.2785e-02, -9.2798e-02,\n",
      "         -1.3293e-02,  6.5869e-02,  7.7393e-02,  2.1738e-02,  2.7950e-02,\n",
      "         -2.0381e-03,  4.7503e-02,  9.3217e-02,  5.9419e-02,  2.2632e-02,\n",
      "          3.9036e-02,  1.0050e-01,  1.7821e-02,  8.1532e-02, -3.3050e-02,\n",
      "         -3.4297e-02,  4.0399e-02,  9.2748e-02, -9.0985e-02]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0783, -0.0899,  0.0726, -0.0203, -0.0106, -0.0748, -0.0710, -0.1004,\n",
      "        -0.0581,  0.0488], requires_grad=True)]\n",
      "[('', LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")), ('conv1', Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))), ('conv2', Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))), ('fc1', Linear(in_features=400, out_features=120, bias=True)), ('fc2', Linear(in_features=120, out_features=84, bias=True)), ('fc3', Linear(in_features=84, out_features=10, bias=True))]\n",
      "[LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "), Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)), Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)), Linear(in_features=400, out_features=120, bias=True), Linear(in_features=120, out_features=84, bias=True), Linear(in_features=84, out_features=10, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(list(model.named_parameters()))\n",
    "print(list(model.parameters()))\n",
    "print(list(model.named_modules()))\n",
    "print(list(model.modules()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b647e36c",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Create an instance of the class below and print its modules and parameters. What do we observe? Fix this issue by using the object `torch.nn.ModuleList`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c00fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SomeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SomeModel, self).__init__()\n",
    "\n",
    "        self.layers = torch.nn.ModuleList([torch.nn.Linear(6, 6), torch.nn.ReLU(), torch.nn.Linear(6, 1)]) #need of a list in terms of module, not jsut a typical array\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in self.layers : \n",
    "            x = i(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c55c0386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('layers.0.weight', Parameter containing:\n",
      "tensor([[ 0.2439, -0.0118, -0.2720,  0.4013, -0.1016,  0.1192],\n",
      "        [-0.0368, -0.2211, -0.2109,  0.2396,  0.0240, -0.3616],\n",
      "        [-0.3807,  0.3916, -0.3530, -0.3158,  0.0021, -0.1602],\n",
      "        [ 0.2817, -0.1004, -0.0932,  0.2453,  0.3340,  0.2400],\n",
      "        [-0.1019, -0.1456,  0.0620, -0.1090, -0.0927, -0.1850],\n",
      "        [ 0.1231,  0.0769, -0.0406,  0.3607, -0.0891, -0.2938]],\n",
      "       requires_grad=True)), ('layers.0.bias', Parameter containing:\n",
      "tensor([-0.3880, -0.3139, -0.3710,  0.2403,  0.1253, -0.2949],\n",
      "       requires_grad=True)), ('layers.2.weight', Parameter containing:\n",
      "tensor([[-0.1638, -0.1463,  0.3491, -0.2572, -0.1089, -0.0021]],\n",
      "       requires_grad=True)), ('layers.2.bias', Parameter containing:\n",
      "tensor([-0.2893], requires_grad=True))]\n",
      "[Parameter containing:\n",
      "tensor([[ 0.2439, -0.0118, -0.2720,  0.4013, -0.1016,  0.1192],\n",
      "        [-0.0368, -0.2211, -0.2109,  0.2396,  0.0240, -0.3616],\n",
      "        [-0.3807,  0.3916, -0.3530, -0.3158,  0.0021, -0.1602],\n",
      "        [ 0.2817, -0.1004, -0.0932,  0.2453,  0.3340,  0.2400],\n",
      "        [-0.1019, -0.1456,  0.0620, -0.1090, -0.0927, -0.1850],\n",
      "        [ 0.1231,  0.0769, -0.0406,  0.3607, -0.0891, -0.2938]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.3880, -0.3139, -0.3710,  0.2403,  0.1253, -0.2949],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.1638, -0.1463,  0.3491, -0.2572, -0.1089, -0.0021]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2893], requires_grad=True)]\n",
      "[SomeModel(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=6, out_features=1, bias=True)\n",
      "  )\n",
      "), ModuleList(\n",
      "  (0): Linear(in_features=6, out_features=6, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=6, out_features=1, bias=True)\n",
      "), Linear(in_features=6, out_features=6, bias=True), ReLU(), Linear(in_features=6, out_features=1, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "model1 = SomeModel()\n",
    "\n",
    "print(list(model1.named_parameters()))\n",
    "print(list(model1.parameters()))\n",
    "print(list(model1.modules()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bf2a1a",
   "metadata": {},
   "source": [
    "**Question 3 (optional)**\n",
    "\n",
    "Check out the method `register_buffer` of `Module`, explain why it can be useful and show a use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aafbcd",
   "metadata": {},
   "source": [
    "## Training a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bc5f40ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, nepochs):\n",
    "    #List to store loss to visualize\n",
    "    valid_loss_min = np.inf # track change in validation loss\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    acc_eval = []\n",
    "    #test_counter = [i*len(train_loader.dataset) for i in n_epochs]\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device = device)\n",
    "            print(data.size())\n",
    "            target = target.to(device = device)\n",
    "            break \n",
    "            \n",
    "            optimizer.zero_grad() #clear the gradients (previously computed)\n",
    "\n",
    "            out = model(data) #compute the output of the model\n",
    "\n",
    "            loss = criterion(out, target) #calculate the loss for this output\n",
    "\n",
    "            loss.backward() #compute the gradient of the loss to guide the optimisation\n",
    "\n",
    "            optimizer.step() #optimize\n",
    "\n",
    "            train_loss += loss.item()*data.size(0) #keep track of the loss\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            with torch.no_grad():\n",
    "                data = data.to(device = device)\n",
    "                target = target.to(device = device)\n",
    "\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(test_loader.dataset)\n",
    "        acc_eval.append(correct/len(test_loader.dataset)*100)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(valid_loss)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2dac6847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "Epoch: 0 \tTraining Loss: 0.000000 \tValidation Loss: 2.307807\n",
      "torch.Size([64, 1, 28, 28])\n",
      "Epoch: 1 \tTraining Loss: 0.000000 \tValidation Loss: 2.307807\n",
      "torch.Size([64, 1, 28, 28])\n",
      "Epoch: 2 \tTraining Loss: 0.000000 \tValidation Loss: 2.307807\n",
      "torch.Size([64, 1, 28, 28])\n",
      "Epoch: 3 \tTraining Loss: 0.000000 \tValidation Loss: 2.307807\n",
      "torch.Size([64, 1, 28, 28])\n",
      "Epoch: 4 \tTraining Loss: 0.000000 \tValidation Loss: 2.307807\n"
     ]
    }
   ],
   "source": [
    "model = LeNet().to(device = device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr =.01)\n",
    "\n",
    "nepochs = 5\n",
    "\n",
    "train_model(model, criterion, optimizer, nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f3b56",
   "metadata": {},
   "source": [
    "Loading a model with checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e2e39",
   "metadata": {},
   "source": [
    "## Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f6058",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "To make sure that the inputs of the neural network are within a controlled range, we usually transform the dataset to be sure that the data are centered with variance 1. It is not always necessary, but it is worth knowing it.\n",
    "\n",
    "Check the range of values on a sample of MNIST. Compute the mean and the standard deviation of the training dataset of MNIST and normalize the dataset accordingly by using `transforms.Normalize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "05a53a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.1476e-08)\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "mean = 0\n",
    "\n",
    "total_sum = 0\n",
    "for img, target in train_loader:\n",
    "    total_sum +=img.sum()\n",
    "\n",
    "mean = total_sum / (28**2 * len(train_loader.dataset))\n",
    "\n",
    "print(mean)\n",
    "\n",
    "std_dev = 0\n",
    "for img, target in train_loader:\n",
    "    std_dev += (img - mean).pow(2).sum()\n",
    "std_dev = (std_dev / (28**2 * len(train_loader.dataset))).sqrt()\n",
    "\n",
    "print(std_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d82e219f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Compose' object has no attribute 'Compose'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[140]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m batch_size = \u001b[32m64\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m transform = \u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompose\u001b[49m([\n\u001b[32m      4\u001b[39m     transforms.ToTensor(), \n\u001b[32m      5\u001b[39m     transforms.Normalize(mean, std_dev)\n\u001b[32m      6\u001b[39m     ])\n\u001b[32m      8\u001b[39m train_data = datasets.MNIST(datasets_path, train = \u001b[38;5;28;01mTrue\u001b[39;00m, transform = transform, download = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      9\u001b[39m test_data = datasets.MNIST(datasets_path, train = \u001b[38;5;28;01mFalse\u001b[39;00m, transform = transform, download = \u001b[38;5;28;01mTrue\u001b[39;00m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'Compose' object has no attribute 'Compose'"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean, std_dev)\n",
    "    ])\n",
    "\n",
    "train_data = datasets.MNIST(datasets_path, train = True, transform = transform, download = True)\n",
    "test_data = datasets.MNIST(datasets_path, train = False, transform = transform, download = True )\n",
    "\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle = False)\n",
    "\n",
    "classes = [f\"{i}\" for i in range(10)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f61498",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bfa454",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "It is very common to face overfitting when doing deep learning. So, several methods can be used to solve this problem. One of them is called \"data augmentation\". It consists in adding \"noise\" to data points of the training dataset in order to make the model resistant to small changes of the data. \n",
    "\n",
    "When training on images, it is common to perform \"random crops\", \"random flips\", and small \"random rotations\". With MNIST, it is meaningless to add random flips, because most digits are not supposed to be invariant by vertical or horizontal symmetries.\n",
    "\n",
    "Add random crops with `transforms.RandomCrop` with a reasonable number of pixels to the transforms to do on the dataset, visualize the resulting images and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "70d9955a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Compose' object has no attribute 'Compose'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[136]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m batch_size = \u001b[32m64\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m transform = \u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompose\u001b[49m([\n\u001b[32m      4\u001b[39m     transforms.ToTensor(),\n\u001b[32m      5\u001b[39m     transforms.Normalize(mean, std),\n\u001b[32m      6\u001b[39m     transforms.RandomCrop(\u001b[32m28\u001b[39m, padding = \u001b[32m3\u001b[39m)\n\u001b[32m      7\u001b[39m ]) \n",
      "\u001b[31mAttributeError\u001b[39m: 'Compose' object has no attribute 'Compose'"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "    transforms.RandomCrop(28, padding = 3)\n",
    "]) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9fc21d",
   "metadata": {},
   "source": [
    "## Influence of the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891776ef",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "Build a Multilayer Perceptron with ReLU activation functions, which takes 3 arguments: \n",
    " * `layers`: the list of layer sizes;\n",
    " * `sigma_w`: the standard deviation chosen for initializing the weights;\n",
    " * `with_scaling`: if True, we multiply the generated weights by $1/\\sqrt{\\# \\text{inputs}}$.\n",
    "\n",
    "Write the `reset_parameters` method, which initialize the weights according to a Gaussian distribution, either with variance $\\sigma_w^2$, or $\\sigma_w^2/\\# \\text{inputs}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2401188",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, layers, sigma_w, scaling = False):\n",
    "        super(Perceptron, self).__init__()\n",
    "        \n",
    "        self.act_function = torch.relu\n",
    "        self.scaling = scaling\n",
    "        self.sigma_w = sigma_w\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for l_in, l_out in zip(layers[:-1], layers[1:]):\n",
    "            self.layers.append(nn.Linear(l_in, l_out))\n",
    "        self.nb_layers = len(self.layers)\n",
    "\n",
    "        self.reset_parameters()\n",
    "         \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        with torch.no_grad():\n",
    "            for i, l in enumerate(self.layers):\n",
    "                l.weight.data.normal_()\n",
    "                l.weight.data.mul(self.sigma_w)\n",
    "\n",
    "                if self.scaling : \n",
    "                    l.weight.data.div_(np.sqrt(l.weight.size(1)))\n",
    "\n",
    "                l.biais.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for l in self.layers[:-1]:\n",
    "            x = l(x)\n",
    "            x = self.act_function(x)\n",
    "                                       \n",
    "        x = self.layers[-1](x)\n",
    "        x = torch.nn.functional.log_softmax(x, dim = 1)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
